{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6cc0d71-c16f-42f1-8ebe-a8d758efc9c0",
   "metadata": {},
   "source": [
    "**Introduzione a LangChain e agli agenti RAG**\n",
    "\n",
    "LangChain è una libreria Python progettata per facilitare l'integrazione dei Large Language Model (LLM) con dati esterni, strumenti e ambienti complessi. Il suo obiettivo principale è permettere lo sviluppo di applicazioni avanzate che non si limitano a generare testo, ma che possono interagire dinamicamente con fonti di conoscenza, API, database e sistemi esterni.\n",
    "\n",
    "Tra le applicazioni più interessanti realizzabili con LangChain ci sono gli **agenti**, ovvero sistemi intelligenti che utilizzano un LLM per prendere decisioni, selezionare strumenti, pianificare azioni e risolvere compiti. Gli agenti sono fondamentali per automatizzare processi multi-step, dove serve non solo completare testi ma anche eseguire azioni guidate dal ragionamento.\n",
    "\n",
    "Una delle architetture più efficaci per rispondere a domande specifiche su fonti informative complesse è quella degli **agenti RAG** (Retrieval-Augmented Generation). In questa struttura, l’agente combina due fasi principali:\n",
    "\n",
    "1. **Retrieval** – Recupera documenti rilevanti da una base di conoscenza tramite tecniche di ricerca semantica o keyword-based.\n",
    "2. **Generation** – Genera una risposta sintetica e coerente basandosi sui documenti recuperati.\n",
    "\n",
    "Questo approccio permette di superare i limiti di memoria degli LLM, mantenendo la risposta aderente a una fonte verificabile. RAG è utilizzato in molte applicazioni reali, ad esempio per costruire chatbot aziendali, assistenti legali, motori di ricerca semantici e sistemi di supporto decisionale.\n",
    "\n",
    "LangChain fornisce strumenti modulari per implementare agenti RAG, integrando moduli di embedding, vector store, chain di prompt, strumenti personalizzati e controllo del flusso logico.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6e080-5427-4657-99d7-ba86fa0e2c45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Struttura di una tipica applicazione RAG**\n",
    "\n",
    "Una tipica applicazione **Retrieval-Augmented Generation (RAG)** è composta da due componenti principali:\n",
    "\n",
    "1. **Indicizzazione (Indexing)**\n",
    "   Un processo che serve a caricare, suddividere e indicizzare i dati provenienti da una fonte. Questa fase viene eseguita **offline**, prima dell’interazione con l’utente.\n",
    "\n",
    "2. **Recupero e generazione (Retrieval and Generation)**\n",
    "   La fase eseguita **a runtime**, quando l’utente pone una domanda. Il sistema recupera i dati rilevanti dall’indice e li fornisce al modello, che genera la risposta.\n",
    "\n",
    "> **Nota**: la fase di indicizzazione segue spesso lo stesso schema di un sistema di ricerca semantica.\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline completa: dal dato grezzo alla risposta**\n",
    "\n",
    "Il flusso completo più comune, dalla fonte di dati grezza alla risposta generata, prevede i seguenti passaggi:\n",
    "\n",
    "#### **1. Indexing**\n",
    "\n",
    "* **Load** – Caricamento dei dati\n",
    "  I dati vengono caricati tramite **Document Loaders**, connettori che leggono file, pagine web, PDF, database, ecc.\n",
    "\n",
    "* **Split** – Suddivisione in chunk\n",
    "  I **Text Splitters** dividono i documenti in porzioni più piccole. Questo è utile sia per ottimizzare la ricerca, sia per assicurarsi che il contenuto rientri nel contesto gestibile del modello LLM (limite di token).\n",
    "\n",
    "* **Store** – Salvataggio e indicizzazione\n",
    "  Le porzioni di testo (chunk) vengono memorizzate e indicizzate all'interno di un **Vector Store**, dove ogni chunk è trasformato in un **embedding vettoriale**. Questo consente di effettuare ricerche semantiche rapide e precise.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc27b7-88f4-4bb7-9c25-53d58329155e",
   "metadata": {},
   "source": [
    "![Alt text](rag_indexing-8160f90a90a33253d0154659cf7d453f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03515f-2709-4a5d-8bca-a2ad648d3034",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **2. Retrieval and Generation**\n",
    "\n",
    "Una volta che i dati sono stati indicizzati, entra in gioco la seconda fase: **recupero e generazione**, eseguita ogni volta che l’utente pone una domanda.\n",
    "\n",
    "#### **Retrieve – Recupero dei dati rilevanti**\n",
    "\n",
    "A partire dall’input dell’utente (la **query**), il sistema recupera i chunk più pertinenti dalla memoria vettoriale.\n",
    "Questo avviene tramite un componente chiamato **Retriever**, che esegue una ricerca semantica confrontando l’embedding della query con quelli memorizzati.\n",
    "\n",
    "#### **Generate – Generazione della risposta**\n",
    "\n",
    "I chunk recuperati, insieme alla domanda dell’utente, vengono inseriti in un **prompt** che viene poi fornito a un **ChatModel** o **LLM** (Large Language Model).\n",
    "Il modello genera una risposta sfruttando il contesto fornito dal materiale recuperato, evitando allucinazioni e mantenendo l’aderenza ai dati reali.\n",
    "\n",
    "---\n",
    "\n",
    "Questa architettura consente di ottenere risposte più accurate e affidabili rispetto a un LLM standalone, specialmente quando è necessario rispondere su dati proprietari, documentazione interna o fonti aggiornabili.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae00617-af54-493b-b0b2-8c3d84146b19",
   "metadata": {},
   "source": [
    "![Alt text](rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617214a4-175e-410e-93d8-bf129b35dd3a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Installazione**\n",
    "\n",
    "Per seguire questo tutorial è necessario installare alcune dipendenze di **LangChain**.\n",
    "Puoi farlo tramite **pip**.\n",
    "\n",
    "#### **Installazione via pip**\n",
    "\n",
    "```bash\n",
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
    "```\n",
    "\n",
    "Questo comando installa:\n",
    "\n",
    "* `langchain-text-splitters`: per suddividere i documenti in chunk\n",
    "* `langchain-community`: insieme di integrazioni e loader della community\n",
    "* `langgraph`: framework per costruire agenti complessi tramite grafi\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c2c61-e5fe-412a-8983-471f52162ebc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Componenti necessari**\n",
    "\n",
    "Per costruire la nostra applicazione RAG, dobbiamo selezionare tre componenti fondamentali dalla suite di integrazioni offerte da LangChain:\n",
    "\n",
    "1. **Modello di chat (LLM)**\n",
    "2. **Modello di embeddings**\n",
    "3. **Vector store (memoria vettoriale)**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1358f17d-a8c1-4bcd-ab53-23ad53f01849",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **1. Selezione del modello di chat (LLM)**\n",
    "\n",
    "LangChain consente di utilizzare diversi provider per accedere a modelli di linguaggio come GPT-4, GPT-3.5, Claude, ecc.\n",
    "In questo esempio, ci concentreremo su **modelli OpenAI compatibili**, scegliendo tra:\n",
    "\n",
    "* **OpenAI** (API ufficiale)\n",
    "* **Azure OpenAI**\n",
    "* **LM Studio** (server locale compatibile con OpenAI)\n",
    "\n",
    "Per tutti i casi, il modulo da utilizzare è `init_chat_model` di `langchain.chat_models`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Installazione**\n",
    "\n",
    "```bash\n",
    "pip install -qU \"langchain[openai]\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Opzione 1 – Connessione via OpenAI API (ufficiale)**\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Imposta la chiave API se non già presente\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Inserisci la tua API Key OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Inizializza un modello GPT-4o-mini usando OpenAI come provider\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Opzione 2 – Connessione via Azure OpenAI**\n",
    "\n",
    "> Azure richiede di specificare **API key**, **endpoint** e **nome del deployment**.\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Imposta le variabili per Azure OpenAI\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Inserisci la Azure API Key: \")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = input(\"Inserisci l'endpoint Azure (es. https://nome.cognitiveservices.azure.com): \")\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = input(\"Inserisci il nome del deployment (es. gpt-4o-deployment): \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\n",
    "    \"gpt-4o\", \n",
    "    model_provider=\"azure\",\n",
    ")\n",
    "```\n",
    "\n",
    "LangChain rileva automaticamente le variabili `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` e `AZURE_OPENAI_CHAT_DEPLOYMENT_NAME`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Opzione 3 – Connessione via LM Studio (server locale compatibile OpenAI)**\n",
    "\n",
    "> LM Studio espone un endpoint locale compatibile con OpenAI (tipicamente su `http://localhost:1234/v1`)\n",
    "\n",
    "```python\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Configura l'accesso al server locale LM Studio\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"not-needed\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"http://localhost:1234/v1\"\n",
    "\n",
    "llm = init_chat_model(\n",
    "    \"lmstudio-model\",           # Es: \"mistral\", \"llama3\", ecc.\n",
    "    model_provider=\"openai\"     # LM Studio è compatibile OpenAI, quindi usiamo questo\n",
    ")\n",
    "```\n",
    "\n",
    "Puoi ottenere il nome esatto del modello aperto su LM Studio tramite l’interfaccia oppure controllando i log nel terminale.\n",
    "\n",
    "---\n",
    "\n",
    "### **Nota finale**\n",
    "\n",
    "Il metodo `init_chat_model()` astrae la complessità del provider. Cambiare tra OpenAI, Azure o LM Studio è questione di cambiare:\n",
    "\n",
    "* Il valore di `model_provider`\n",
    "* Le variabili d’ambiente richieste\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c4e4c-4100-43fa-8ea4-e59770e5c543",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **2. Selezione del modello di Embedding**\n",
    "\n",
    "Gli **embedding** sono rappresentazioni numeriche (vettori) di frasi o documenti. Servono per misurare la **similarità semantica** tra testi e sono fondamentali per il recupero nella pipeline RAG.\n",
    "\n",
    "LangChain consente l’uso di diversi modelli di embedding. In questa sezione vedremo:\n",
    "\n",
    "* `text-embedding-3-large` (OpenAI – più recente e accurato)\n",
    "* `text-embedding-ada-002` (OpenAI – più economico, legacy)\n",
    "* `MiniLM` (open-source – via Hugging Face)\n",
    "\n",
    "Ti mostrerò:\n",
    "\n",
    "1. Come configurarli\n",
    "2. Quando e perché usare ciascuno\n",
    "3. Differenze chiave tra prestazioni, costi e compatibilità\n",
    "\n",
    "---\n",
    "\n",
    "###  **Installazione necessaria**\n",
    "\n",
    "Per i modelli OpenAI:\n",
    "\n",
    "```bash\n",
    "pip install -qU langchain-openai\n",
    "```\n",
    "\n",
    "Per MiniLM (Hugging Face):\n",
    "\n",
    "```bash\n",
    "pip install -qU sentence-transformers langchain-community\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **A. OpenAI – `text-embedding-3-large`**\n",
    "\n",
    "> Ultimo modello embedding rilasciato da OpenAI. Supporta testo multilingua, alta accuratezza e compressione.\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Imposta la chiave OpenAI se non presente\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Inserisci la tua OpenAI API Key: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Modello embedding più accurato\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "```\n",
    "\n",
    "**Vantaggi**:\n",
    "\n",
    "* Alta precisione semantica\n",
    "* Supporto multilingua\n",
    "* Comprimibile a 1536 dimensioni\n",
    "* Ottimo per applicazioni critiche (RAG, ricerca documentale)\n",
    "\n",
    "**Svantaggi**:\n",
    "\n",
    "* Costo più alto per token\n",
    "* Richiede connessione alle API di OpenAI\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **B. OpenAI – `text-embedding-ada-002`**\n",
    "\n",
    "> Modello precedente, molto utilizzato per il buon rapporto qualità/prezzo.\n",
    "\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Modello embedding economico e ancora valido\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "```\n",
    "\n",
    "**Vantaggi**:\n",
    "\n",
    "* Più economico del `3-large`\n",
    "* Buona accuratezza per testi in inglese\n",
    "* Bassa latenza\n",
    "\n",
    "**Svantaggi**:\n",
    "\n",
    "* Peggiore su testi multilingua\n",
    "* Meno efficace su concetti complessi\n",
    "\n",
    "**Quando usarlo**:\n",
    "\n",
    "* Quando si lavora con grandi volumi di dati\n",
    "* Quando il budget è limitato\n",
    "* Quando si ha bisogno di embedding rapidi ed economici\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **C. Hugging Face – `MiniLM` (open-source)**\n",
    "\n",
    "> Alternativa gratuita, locale, utile per prototipazione e ambienti privati.\n",
    "\n",
    "```python\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "```\n",
    "\n",
    "**Vantaggi**:\n",
    "\n",
    "* Gratuito\n",
    "* Esecuzione locale (nessuna API esterna)\n",
    "* Adatto a molti task generici di similarità semantica\n",
    "* Più veloce dei modelli di grandi dimensioni\n",
    "\n",
    "**Svantaggi**:\n",
    "\n",
    "* Meno accurato di OpenAI 3-large\n",
    "* Limitato su testi complessi o lunghi\n",
    "* Solo supporto parziale multilingua\n",
    "\n",
    "**Quando usarlo**:\n",
    "\n",
    "* In progetti on-premise o air-gapped\n",
    "* Per test e prototipi\n",
    "* Quando non è disponibile una connessione a internet o si vuole evitare l’uso di servizi esterni\n",
    "\n",
    "---\n",
    "\n",
    "##  **Confronto sintetico**\n",
    "\n",
    "| Modello                  | Precisione | Multilingua  | Latenza     | Costo    | Note                           |\n",
    "| ------------------------ | ---------- | ------------ | ----------- | -------- | ------------------------------ |\n",
    "| `text-embedding-3-large` | ⭐⭐⭐⭐⭐      | ✅            | Media       | Alta     | Miglior qualità generale       |\n",
    "| `text-embedding-ada-002` | ⭐⭐⭐⭐       | ❌ (parziale) | Bassa       | Bassa    | Ottimo rapporto qualità/prezzo |\n",
    "| `MiniLM` (`L6-v2`)       | ⭐⭐⭐        | ❌ (parziale) | Molto bassa | Gratuito | Ottimo per prototipi locali    |\n",
    "\n",
    "---\n",
    "\n",
    "##  **Conclusioni**\n",
    "\n",
    "* Se vuoi **la massima qualità** per un'applicazione RAG in produzione → usa `text-embedding-3-large`.\n",
    "* Se devi **ottimizzare i costi** su grandi dataset → considera `ada-002`.\n",
    "* Se vuoi **lavorare offline o open-source** → MiniLM è la scelta giusta.\n",
    "\n",
    "LangChain rende semplice cambiare embedding: basta modificare la riga di inizializzazione, mantenendo tutto il resto del sistema invariato.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c406372-be23-414c-88cc-1c296a56327d",
   "metadata": {},
   "source": [
    "### **Approfondimento: Cos'è un Embedding e perché è fondamentale nei sistemi RAG**\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Definizione di Embedding**\n",
    "\n",
    "Un **embedding** è una rappresentazione numerica densa di un'informazione testuale (come una parola, una frase o un documento).\n",
    "In pratica, trasforma il linguaggio naturale in **vettori di numeri** che catturano la **semantica** del testo, rendendo possibile confrontare, cercare e analizzare frasi con criteri matematici.\n",
    "\n",
    "Esempio intuitivo:\n",
    "La frase *\"Come stai?\"* e *\"Tutto bene?\"* avranno embedding simili, perché esprimono un significato vicino.\n",
    "Al contrario, *\"Apri la porta\"* e *\"Mangia una mela\"* avranno embedding distanti.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Perché gli Embedding sono centrali nei sistemi RAG**\n",
    "\n",
    "RAG significa **Retrieval-Augmented Generation**. È un'architettura che combina:\n",
    "\n",
    "* **retrieval** (recupero di contenuti rilevanti)\n",
    "* **generation** (generazione di risposte da parte di un LLM)\n",
    "\n",
    "Il recupero dei contenuti si basa **quasi sempre su embedding**, ed è qui che il loro ruolo diventa cruciale.\n",
    "\n",
    "**Come funziona:**\n",
    "\n",
    "1. Tutti i documenti vengono suddivisi in \"chunk\" (pezzi di testo) → *text splitting*\n",
    "2. Ogni chunk viene trasformato in un **vettore embedding**\n",
    "3. Quando l’utente fa una domanda:\n",
    "\n",
    "   * La domanda viene **anch’essa convertita in embedding**\n",
    "   * Viene calcolata la **distanza vettoriale** tra l’embedding della domanda e quelli dei documenti\n",
    "   * I chunk più “vicini” vengono recuperati e forniti al modello generativo\n",
    "\n",
    "> Senza embedding, non potremmo confrontare in modo efficiente frasi in linguaggio naturale per similarità di significato.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Proprietà di un buon embedding**\n",
    "\n",
    "Un buon modello di embedding deve:\n",
    "\n",
    "* Catturare **relazioni semantiche**, non solo sintattiche\n",
    "* Supportare **testi multilingua** se necessario\n",
    "* Essere **compatto ma informativo** (vettori densi, es. 384–1536 dimensioni)\n",
    "* Essere **veloce da calcolare** anche su grandi quantità di testo\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Tipologie di modelli di embedding**\n",
    "\n",
    "| Tipo                           | Esempi                                                    | Caratteristiche principali                                |\n",
    "| ------------------------------ | --------------------------------------------------------- | --------------------------------------------------------- |\n",
    "| **Pre-addestrati commerciali** | OpenAI `text-embedding-ada-002`, `text-embedding-3-large` | Alta qualità, multilingua, ma richiedono API a pagamento  |\n",
    "| **Modelli open-source**        | MiniLM, BGE, Instructor, GTE                              | Gratuiti, eseguibili in locale, meno accurati in generale |\n",
    "| **Custom**                     | Addestrati su dati aziendali                              | Ottimizzati per il dominio specifico                      |\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Distanza vettoriale e similarità**\n",
    "\n",
    "Gli embedding non servono da soli: vanno confrontati con una **metrica di similarità**. Le più comuni sono:\n",
    "\n",
    "* **Cosine similarity** – misura l’angolo tra vettori (indipendente dalla lunghezza)\n",
    "* **Distanza L2 (euclidea)** – usata ad esempio da FAISS `IndexFlatL2`\n",
    "* **Dot product** – usata spesso in modelli neurali\n",
    "\n",
    "Queste metriche permettono di recuperare i chunk “più simili” alla query in modo efficiente, anche su milioni di documenti.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Prestazioni: qualità vs velocità**\n",
    "\n",
    "Scegliere il giusto embedding dipende dal contesto:\n",
    "\n",
    "| Contesto                                       | Modello consigliato      |\n",
    "| ---------------------------------------------- | ------------------------ |\n",
    "| RAG aziendale di qualità su dati in più lingue | `text-embedding-3-large` |\n",
    "| Ricerca veloce su documenti in inglese         | `text-embedding-ada-002` |\n",
    "| Sistema locale, senza costi o cloud            | `MiniLM`, `BGE`, `GTE`   |\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Visualizzazione (opzionale)**\n",
    "\n",
    "È possibile proiettare gli embedding in 2D o 3D (es. con PCA o UMAP) per visualizzare la distribuzione dei significati nel testo.\n",
    "Questa analisi è utile per verificare se frasi simili vengono effettivamente mappate vicino tra loro.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Rischi e best practice**\n",
    "\n",
    "* **Chunk troppo lunghi** portano a embedding poco precisi → usa text splitter\n",
    "* **Contenuto rumoroso** o ripetitivo può alterare i risultati → filtra o pulisci prima\n",
    "* **Usa batching** per calcolare più embedding in parallelo e risparmiare tempo\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "Gli embedding sono la base invisibile ma fondamentale di qualsiasi sistema di retrieval semantico.\n",
    "Che si tratti di un motore RAG, un motore di ricerca AI, o un sistema di classificazione testi, il **modo in cui trasformi il testo in numeri** determina tutta la qualità della tua pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d0f222-834b-4967-9b33-9870f11cdeb2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **3. Selezione del Vector Store**\n",
    "\n",
    "Il **vector store** è il componente responsabile dell’indicizzazione, memorizzazione e ricerca dei **vettori embedding**.\n",
    "È parte fondamentale in ogni sistema **RAG**, perché permette di **recuperare documenti simili semanticamente** a partire dalla query dell’utente.\n",
    "\n",
    "LangChain supporta numerosi vector store, tra cui:\n",
    "\n",
    "* **FAISS** – locale, veloce, open-source\n",
    "* **Qdrant** – scalabile, open-source, API-based o locale\n",
    "* **Pinecone** – cloud-native, ottimizzato per applicazioni su larga scala\n",
    "\n",
    "---\n",
    "\n",
    "###  **Installazione base**\n",
    "\n",
    "```bash\n",
    "pip install -qU langchain-community\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **Esempio 1 – FAISS (locale, veloce, open-source)**\n",
    "\n",
    "**FAISS** è una libreria sviluppata da Facebook AI Research. Ottima per test locali, prototipi e ambienti controllati.\n",
    "\n",
    "### Codice:\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Calcola la dimensione degli embedding\n",
    "embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
    "\n",
    "# Crea un indice FAISS con distanza L2\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Inizializza il vector store\n",
    "vs = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings\n",
    ")\n",
    "```\n",
    "\n",
    "### Quando usarlo:\n",
    "\n",
    "* Ambienti locali\n",
    "* Nessuna dipendenza da servizi esterni\n",
    "* Dataset medio-piccoli\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **Esempio 2 – Qdrant (cloud o locale, open-source)**\n",
    "\n",
    "**Qdrant** è un motore vettoriale moderno scritto in Rust, molto efficiente, scalabile e con supporto per **payload metadata**, **filtri**, **search ibrida**.\n",
    "\n",
    "### Installazione:\n",
    "\n",
    "```bash\n",
    "pip install qdrant-client langchain-community\n",
    "```\n",
    "\n",
    "### Codice (con server Qdrant locale su `http://localhost:6333`):\n",
    "\n",
    "```python\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Connessione a Qdrant\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Inizializzazione dello store vettoriale\n",
    "vector_store = Qdrant.from_documents(\n",
    "    documents=[Document(page_content=\"Test\", metadata={\"source\": \"example\"})],\n",
    "    embedding=embeddings,\n",
    "    location=\"http://localhost:6333\",\n",
    "    collection_name=\"rag_example\",\n",
    "    client=client,\n",
    ")\n",
    "```\n",
    "\n",
    "### Quando usarlo:\n",
    "\n",
    "* Hai bisogno di metadati avanzati e filtri\n",
    "* Deployment on-premise o su cloud\n",
    "* Performance e scalabilità superiori a FAISS\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 **Esempio 3 – Pinecone (cloud-native, altamente scalabile)**\n",
    "\n",
    "**Pinecone** è un vector database professionale gestito, ottimo per applicazioni in produzione con milioni di documenti. Offre **alta disponibilità, replica, filtraggio, gestione di namespace**, ecc.\n",
    "\n",
    "### Installazione:\n",
    "\n",
    "```bash\n",
    "pip install pinecone-client langchain-community\n",
    "```\n",
    "\n",
    "### Codice:\n",
    "\n",
    "```python\n",
    "import pinecone\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Inizializza Pinecone con API key e ambiente\n",
    "pinecone.init(\n",
    "    api_key=\"YOUR_PINECONE_API_KEY\",\n",
    "    environment=\"us-east1-gcp\"  # esempio, dipende dal tuo account\n",
    ")\n",
    "\n",
    "# Crea l'indice se non esiste\n",
    "index_name = \"rag-demo\"\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(index_name, dimension=len(embeddings.embed_query(\"test\")))\n",
    "\n",
    "# Connessione all’indice\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# Crea il vector store\n",
    "vector_store = Pinecone.from_documents(\n",
    "    documents=[Document(page_content=\"Questo è un documento\", metadata={\"source\": \"A\"})],\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name,\n",
    ")\n",
    "```\n",
    "\n",
    "### Quando usarlo:\n",
    "\n",
    "* Applicazioni enterprise\n",
    "* Dataset molto grandi\n",
    "* Hai bisogno di disponibilità e replicazione automatica\n",
    "\n",
    "---\n",
    "\n",
    "##  **Confronto tra FAISS, Qdrant e Pinecone**\n",
    "\n",
    "| Caratteristica    | FAISS             | Qdrant                  | Pinecone              |\n",
    "| ----------------- | ----------------- | ----------------------- | --------------------- |\n",
    "| Tipo              | Locale            | Cloud/Locale            | Solo Cloud            |\n",
    "| Performance       | Alta (locale)     | Alta (cloud & locale)   | Altissima (cloud)     |\n",
    "| Filtri avanzati   | ❌                 | ✅                       | ✅                     |\n",
    "| Supporto metadati | ❌ (limitato)      | ✅                       | ✅                     |\n",
    "| Persistenza       | Manuale           | Integrata               | Integrata             |\n",
    "| Costo             | Gratuito (locale) | Gratuito / self-hosting | A pagamento           |\n",
    "| Casistica ideale  | Prototipi         | Produzione flessibile   | Produzione enterprise |\n",
    "\n",
    "---\n",
    "\n",
    "###  **Conclusione**\n",
    "\n",
    "La scelta del vector store dipende dal contesto d’uso:\n",
    "\n",
    "* **FAISS**: perfetto per test, prototipi, sviluppo locale\n",
    "* **Qdrant**: ideale se vuoi scalabilità, open-source e controllo fine\n",
    "* **Pinecone**: soluzione cloud altamente affidabile per produzione su larga scala\n",
    "\n",
    "Tutti i vector store funzionano allo stesso modo: ricevono i vettori dagli **embedding** e li rendono ricercabili per similarità.\n",
    "LangChain rende il passaggio da uno all’altro molto semplice, mantenendo la stessa interfaccia di utilizzo per l’intero flusso RAG.\n",
    "\n",
    "## ATTENZIONE\n",
    "Faiss nella sua versione più recente non ha bisogno dell'indice esterni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d0fab-9702-4f42-9160-511ba9d1642b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Approfondimento: utilizzo di FAISS come Vector Store in LangChain\n",
    "\n",
    "### Introduzione\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) è una libreria open-source sviluppata da Meta per effettuare ricerche vettoriali efficienti su larga scala. Viene utilizzata per confrontare rappresentazioni numeriche (embedding) di documenti al fine di recuperarne quelli semanticamente più simili a una query.\n",
    "\n",
    "In LangChain, FAISS è uno dei vector store supportati per costruire pipeline di retrieval all'interno di applicazioni RAG (Retrieval-Augmented Generation). Di seguito viene mostrato un esempio pratico completo e una spiegazione di tutti i parametri rilevanti.\n",
    "\n",
    "---\n",
    "\n",
    "### Installazione\n",
    "\n",
    "Per utilizzare FAISS con LangChain e modelli Hugging Face:\n",
    "\n",
    "```bash\n",
    "pip install faiss-cpu langchain-community sentence-transformers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Inizializzazione: embedding Hugging Face e documenti di esempio\n",
    "\n",
    "```python\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Inizializza un modello di embedding gratuito e locale\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Documenti simulati in inglese\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain is a powerful framework for building LLM applications.\"),\n",
    "    Document(page_content=\"FAISS enables fast semantic search over text data.\"),\n",
    "    Document(page_content=\"Python is widely used in AI and machine learning projects.\"),\n",
    "    Document(page_content=\"Vector databases store dense representations of text.\"),\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Costruzione dell’indice FAISS\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# Calcola la dimensione dei vettori di embedding\n",
    "embedding_dim = len(embeddings.embed_query(\"test\"))\n",
    "\n",
    "def build_faiss_vectorstore(chunks: List[Document], embeddings: HuggingFaceEmbeddings, persist_dir: str) -> FAISS:\n",
    "    \"\"\"\n",
    "    Costruisce da zero un FAISS index (IndexFlatL2) e lo salva su disco.\n",
    "    \"\"\"\n",
    "    # Determina la dimensione dell'embedding\n",
    "    vs = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "\n",
    "    Path(persist_dir).mkdir(parents=True, exist_ok=True)\n",
    "    vs.save_local(persist_dir)\n",
    "    return vs\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Ricerca semantica\n",
    "\n",
    "È possibile effettuare una ricerca semantica a partire da una query in linguaggio naturale:\n",
    "\n",
    "```python\n",
    "results = vector_store.similarity_search(\"How can I build AI applications?\", k=2)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "```\n",
    "\n",
    "LangChain calcola l'embedding della query, esegue la ricerca nel vector store e restituisce i documenti più vicini semanticamente.\n",
    "\n",
    "---\n",
    "\n",
    "### Salvataggio dell’indice FAISS su disco\n",
    "\n",
    "Per evitare di ricostruire l’indice ogni volta, è possibile salvarlo localmente:\n",
    "\n",
    "```python\n",
    "vector_store.save_local(\"faiss_index_example\")\n",
    "```\n",
    "\n",
    "Questo salverà due file nella directory `faiss_index_example/`:\n",
    "\n",
    "* `index.faiss`: rappresentazione binaria dell’indice vettoriale\n",
    "* `index.pkl`: metadati, mappature e documenti\n",
    "\n",
    "---\n",
    "\n",
    "### Caricamento dell’indice FAISS già costruito\n",
    "\n",
    "In un secondo momento, è possibile ricaricare l’indice e riprendere l’uso del vector store:\n",
    "\n",
    "```python\n",
    "vector_store = FAISS.load_local(\"faiss_index_example\", embeddings)\n",
    "```\n",
    "\n",
    "In questo modo si evita il costo computazionale della rigenerazione degli embedding e dell’indice.\n",
    "\n",
    "---\n",
    "\n",
    "### Uso come retriever in una pipeline RAG\n",
    "\n",
    "LangChain consente di convertire il vector store in un retriever standard compatibile con le catene di domanda-risposta:\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "```\n",
    "\n",
    "Questo retriever può essere usato come componente all’interno di una catena `RetrievalQA` o `ConversationalRetrievalChain`.\n",
    "\n",
    "---\n",
    "\n",
    "### Tipologie di indice FAISS\n",
    "\n",
    "| Tipo FAISS      | Descrizione                                                              |\n",
    "| --------------- | ------------------------------------------------------------------------ |\n",
    "| `IndexFlatL2`   | Ricerca esatta con distanza euclidea                                     |\n",
    "| `IndexFlatIP`   | Ricerca esatta con prodotto scalare (dot product)                        |\n",
    "| `IndexIVFFlat`  | Ricerca approssimata basata su clustering (richiede addizionale `train`) |\n",
    "| `IndexHNSWFlat` | Ricerca approssimata tramite grafo navigabile                            |\n",
    "\n",
    "Per progetti semplici o prototipi, `IndexFlatL2` è sufficiente e non richiede training. Per dataset molto grandi, è possibile utilizzare indici approssimati come `IndexIVFFlat` o `HNSW`.\n",
    "\n",
    "---\n",
    "\n",
    "### Considerazioni finali\n",
    "\n",
    "FAISS è una soluzione locale ad alte prestazioni per la ricerca semantica su documenti testuali. In combinazione con LangChain e modelli di embedding gratuiti come MiniLM, consente di costruire pipeline RAG complete senza dover dipendere da API esterne.\n",
    "\n",
    "Per applicazioni scalabili e in produzione, può essere utile valutare alternative cloud come Qdrant o Pinecone, ma FAISS resta un’ottima scelta per prototipi, ambienti controllati, e applicazioni offline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6620ca1b-0123-4620-8611-ece99a13bef7",
   "metadata": {},
   "source": [
    "## Embedding Dim\n",
    "```python\n",
    "embedding_dim = len(embeddings.embed_query(\"test\"))\n",
    "```\n",
    "\n",
    "Nel contesto di LangChain e degli embedding, questa istruzione ha una funzione fondamentale: **determina la dimensione (numero di componenti) del vettore embedding prodotto dal modello**. Vediamo perché è importante e come funziona sotto il cofano.\n",
    "\n",
    "---\n",
    "\n",
    "### Cosa fa questa istruzione?\n",
    "\n",
    "* `embeddings.embed_query(\"test\")` restituisce un vettore embedding (una lista di numeri in virgola mobile) per la stringa `\"test\"`.\n",
    "* `len(...)` calcola la **lunghezza di quella lista**, ovvero il numero di dimensioni (componenti) del vettore embedding.\n",
    "\n",
    "Questa dimensione è essenziale per inizializzare un indice FAISS correttamente, perché FAISS richiede di sapere quanti elementi conterrà ogni vettore per costruire l’indice.\n",
    "\n",
    "---\n",
    "\n",
    "### Perché non esiste un metodo esplicito per ottenerla?\n",
    "\n",
    "LangChain non espone direttamente la dimensione degli embedding generati. In particolare, per l’implementazione `HuggingFaceEmbeddings`, non c’è un attributo come `embeddings.dimension`, motivo per cui molti sviluppatori utilizzano la soluzione più pratica:\n",
    "\n",
    "> Chiedere direttamente al modello: generare un embedding su una query di prova e misurarne la lunghezza.\n",
    "> ([Stack Overflow][1])\n",
    "\n",
    "---\n",
    "\n",
    "### Dimensioni comuni di embedding Hugging Face\n",
    "\n",
    "Un esempio frequente è il modello `all-MiniLM-L6-v2` di Sentence Transformers. Questo modello mappa ogni frase in uno spazio vettoriale di **384 dimensioni**.\n",
    "([Hugging Face][2])\n",
    "\n",
    "Ciò significa che l’istruzione\n",
    "\n",
    "```python\n",
    "len(embeddings.embed_query(\"test\"))\n",
    "```\n",
    "\n",
    "restituirà 384 quando `embeddings` è basato su `all-MiniLM-L6-v2`.\n",
    "\n",
    "---\n",
    "\n",
    "### Scenario pratico con FAISS\n",
    "\n",
    "Ecco il flusso completo per utilizzare questa informazione nel creare un indice FAISS:\n",
    "\n",
    "```python\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 1. Embedding open-source gratuito\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Documenti di esempio\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain is great for building intelligent systems.\"),\n",
    "    Document(page_content=\"FAISS enables quick semantic search.\"),\n",
    "]\n",
    "\n",
    "# 3. Calcola la dimensione del vettore embedding\n",
    "embedding_dim = len(embeddings.embed_query(\"test\"))\n",
    "\n",
    "# 4. Costruisci un indice FAISS basato su distanza euclidea (L2)\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# 5. Costruisci il vector store\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "```\n",
    "\n",
    "Così il vettore fornito da `embed_query(\"test\")` garantisce che `IndexFlatL2(embedding_dim)` sia inizializzato con la dimensione corretta, evitando errori e garantendo integrità nel salvataggio e ricerca dell’indice.\n",
    "\n",
    "---\n",
    "\n",
    "### In sintesi\n",
    "\n",
    "* **Scopo**: Determinare il numero di dimensioni del vettore embedding generato dal modello.\n",
    "* **Motivo**: LangChain attualmente non esprime questa informazione direttamente, quindi si utilizza un embedding di prova come workaround. \n",
    "* **Esempio pratico**: `all-MiniLM-L6-v2` restituisce embedding da 384 componenti. \n",
    "* **Applicazione**: Questa dimensione è fondamentale per inizializzare correttamente un indice Faiss, garantendo coerenza nel confronto vettoriale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ceaf70-06b4-4e6c-af59-68a5400e01b5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Similarity Metrics: Cosine, Dot Product, L2\n",
    "\n",
    "Quando si confrontano vettori di embedding, è necessario misurare **quanto sono simili due vettori nello spazio vettoriale**. Le metriche di similarità sono funzioni matematiche che restituiscono una misura numerica di quanto due vettori siano “vicini” tra loro.\n",
    "\n",
    "Le metriche più usate nei sistemi di ricerca semantica sono:\n",
    "\n",
    "* **Cosine Similarity**\n",
    "* **Dot Product (Inner Product)**\n",
    "* **L2 Distance (Euclidean Distance)**\n",
    "\n",
    "Ognuna ha caratteristiche, vantaggi e casi d'uso specifici.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Cosine Similarity\n",
    "\n",
    "#### Definizione\n",
    "\n",
    "La **cosine similarity** misura il **coseno dell’angolo** tra due vettori. Più piccolo è l’angolo, più simili sono i vettori.\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\cdot \\|B\\|}\n",
    "$$\n",
    "\n",
    "Dove:\n",
    "\n",
    "* $A \\cdot B$ è il prodotto scalare (dot product)\n",
    "* $\\|A\\|$ e $\\|B\\|$ sono le norme (lunghezze) dei vettori\n",
    "\n",
    "#### Valori\n",
    "\n",
    "* $1$: perfettamente simili (stesso verso)\n",
    "* $0$: ortogonali (nessuna similarità)\n",
    "* $-1$: direzioni opposte\n",
    "\n",
    "#### Pro e contro\n",
    "\n",
    "| Vantaggi                           | Svantaggi                |\n",
    "| ---------------------------------- | ------------------------ |\n",
    "| Invariante rispetto alla lunghezza | Richiede normalizzazione |\n",
    "| Ottimo per misurare orientamento   | Non cattura magnitudine  |\n",
    "\n",
    "#### Quando usarla\n",
    "\n",
    "* Quando ti interessa **la direzione del significato** piuttosto che la sua intensità\n",
    "* Per testi di **lunghezza variabile** e **embedding normalizzati**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Dot Product (Inner Product)\n",
    "\n",
    "#### Definizione\n",
    "\n",
    "Il **dot product** è una versione semplificata del cosine similarity **senza normalizzazione**. Misura l’allineamento e la magnitudine dei vettori:\n",
    "\n",
    "$$\n",
    "\\text{dot\\_product}(A, B) = \\sum_i A_i \\cdot B_i\n",
    "$$\n",
    "\n",
    "#### Valori\n",
    "\n",
    "* Più alto è il valore, più simili sono i vettori\n",
    "\n",
    "#### Pro e contro\n",
    "\n",
    "| Vantaggi                                        | Svantaggi                            |\n",
    "| ----------------------------------------------- | ------------------------------------ |\n",
    "| Veloce da calcolare                             | Sensibile alla lunghezza dei vettori |\n",
    "| Supportato nativamente da FAISS (`IndexFlatIP`) | Può penalizzare vettori corti        |\n",
    "\n",
    "#### Quando usarla\n",
    "\n",
    "* Quando gli **embedding non sono normalizzati**\n",
    "* Quando usi modelli dove la magnitudine del vettore ha significato (es. classificazione o ranking)\n",
    "* In contesti **dove vuoi priorità su \"importanza\" oltre alla direzione**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. L2 Distance (Euclidean Distance)\n",
    "\n",
    "#### Definizione\n",
    "\n",
    "La **L2 distance** è la distanza euclidea standard tra due vettori:\n",
    "\n",
    "$$\n",
    "\\text{L2}(A, B) = \\sqrt{\\sum_i (A_i - B_i)^2}\n",
    "$$\n",
    "\n",
    "Più piccolo è il valore, **più simili** sono i vettori.\n",
    "\n",
    "#### Pro e contro\n",
    "\n",
    "| Vantaggi                                        | Svantaggi                    |\n",
    "| ----------------------------------------------- | ---------------------------- |\n",
    "| Intuitiva e diretta                             | Sensibile alla scala         |\n",
    "| Supportata nativamente in FAISS (`IndexFlatL2`) | Necessita embedding uniformi |\n",
    "\n",
    "#### Quando usarla\n",
    "\n",
    "* Quando vuoi una metrica geometrica diretta\n",
    "* In applicazioni con embedding generati in ambienti controllati e normalizzati\n",
    "* Quando **non normalizzi** i vettori e vuoi valutare distanza “fisica”\n",
    "\n",
    "---\n",
    "\n",
    "## Riepilogo comparativo\n",
    "\n",
    "| Metrica           | Range tipico          | Richiede normalizzazione | Tipico uso in FAISS      | Adatta per                |\n",
    "| ----------------- | --------------------- | ------------------------ | ------------------------ | ------------------------- |\n",
    "| Cosine Similarity | -1 a 1                | Sì                       | Con `IndexFlatIP + norm` | Similarità semantica pura |\n",
    "| Dot Product       | $-\\infty$ a $+\\infty$ | No                       | `IndexFlatIP`            | Sistemi di ranking        |\n",
    "| L2 Distance       | $[0, +\\infty)$        | No                       | `IndexFlatL2`            | Misura geometrica         |\n",
    "\n",
    "---\n",
    "\n",
    "## Quale metrica scegliere?\n",
    "\n",
    "### Usa Cosine Similarity se:\n",
    "\n",
    "* Gli embedding sono normalizzati\n",
    "* Vuoi solo misurare **orientamento semantico**\n",
    "* Usi modelli come `sentence-transformers` con `cosine similarity` come obiettivo\n",
    "\n",
    "### Usa Dot Product se:\n",
    "\n",
    "* L’ampiezza del vettore embedding **ha significato**\n",
    "* Usi FAISS con `IndexFlatIP` e non vuoi normalizzare\n",
    "\n",
    "### Usa L2 Distance se:\n",
    "\n",
    "* Vuoi una **metrica geometrica diretta**\n",
    "* Usi `IndexFlatL2` in FAISS\n",
    "* Gli embedding non sono normalizzati ma comparabili\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio pratico con FAISS in LangChain\n",
    "\n",
    "### Cosine similarity con vettori normalizzati\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Normalizza i vettori\n",
    "def normalize(vectors):\n",
    "    return vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "\n",
    "# Usiamo IndexFlatIP ma normalizzando prima\n",
    "vectors = np.array([[1.0, 2.0], [2.0, 3.0]], dtype=\"float32\")\n",
    "vectors = normalize(vectors)\n",
    "\n",
    "index = faiss.IndexFlatIP(2)\n",
    "index.add(vectors)\n",
    "\n",
    "# Anche la query va normalizzata\n",
    "query = normalize(np.array([[1.0, 1.5]], dtype=\"float32\"))\n",
    "D, I = index.search(query, k=1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### L2 distance (ricerca esatta, non normalizzata)\n",
    "\n",
    "```python\n",
    "index = faiss.IndexFlatL2(2)\n",
    "index.add(np.array([[1.0, 2.0], [2.0, 3.0]], dtype=\"float32\"))\n",
    "query = np.array([[1.0, 1.5]], dtype=\"float32\")\n",
    "D, I = index.search(query, k=1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusione\n",
    "\n",
    "La metrica di similarità è un **elemento critico in un sistema RAG o di ricerca semantica**, perché influenza direttamente i risultati restituiti. Scegliere la metrica giusta dipende:\n",
    "\n",
    "* dal tipo di modello di embedding utilizzato\n",
    "* da come gli embedding sono normalizzati o strutturati\n",
    "* dal comportamento desiderato (priorità alla direzione semantica, magnitudine, o distanza)\n",
    "\n",
    "LangChain e FAISS forniscono gli strumenti per lavorare con tutte queste metriche in modo flessibile e integrato.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a4d36-1d3e-4528-a682-feee0920381b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Hybrid Search: BM25 + Vector Re-ranking**\n",
    "\n",
    "### **Cos'è la Hybrid Search?**\n",
    "\n",
    "La **Hybrid Search** (ricerca ibrida) è una tecnica che **combina due approcci diversi** per migliorare la qualità del recupero dei documenti in risposta a una query:\n",
    "\n",
    "1. **BM25** (ricerca testuale classica, keyword-based)\n",
    "2. **Vector Search** (ricerca semantica tramite embedding)\n",
    "\n",
    "L’obiettivo è **sfruttare i punti di forza di entrambi**:\n",
    "\n",
    "| Approccio         | Vantaggi principali                           | Limiti principali                                             |\n",
    "| ----------------- | --------------------------------------------- | ------------------------------------------------------------- |\n",
    "| **BM25**          | Ottimo per match di parole esatte, efficiente | Non comprende il significato semantico                        |\n",
    "| **Vector Search** | Capisce sinonimi e concetti simili            | Può restituire risultati semanticamente vicini ma irrilevanti |\n",
    "\n",
    "La ricerca ibrida consente di **aumentare precisione e recall**, specialmente in contesti dove è importante sia la **pertinenza semantica**, sia il **match testuale**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Come funziona una pipeline BM25 + Vector Re-ranking**\n",
    "\n",
    "1. **BM25** recupera un set iniziale di documenti **basato su corrispondenze di parole chiave**.\n",
    "2. Questi documenti vengono **embeddingizzati**.\n",
    "3. La query viene trasformata in un embedding vettoriale.\n",
    "4. Si calcola la **similarità vettoriale** tra la query e ciascun documento del set iniziale.\n",
    "5. I risultati vengono **riordinati (re-ranked)** in base alla similarità semantica.\n",
    "\n",
    "In alternativa:\n",
    "\n",
    "* si può fare **merge score-based**: combinando punteggi BM25 e vettoriali con pesi ($\\alpha \\cdot \\text{bm25\\_score} + (1 - \\alpha) \\cdot \\text{vector\\_similarity}$)\n",
    "\n",
    "---\n",
    "\n",
    "## **Cos'è Qdrant e perché è adatto alla ricerca ibrida**\n",
    "\n",
    "**Qdrant** è un **motore di ricerca vettoriale open-source**, moderno, scritto in Rust, pensato per la ricerca scalabile e semantica.\n",
    "A differenza di FAISS, Qdrant supporta **filtri**, **payload JSON**, **metadati** e **ricerca ibrida** nativamente.\n",
    "\n",
    "### Caratteristiche principali di Qdrant:\n",
    "\n",
    "* Supporto nativo per **BM25**\n",
    "* Supporto nativo per **Vector search**\n",
    "* Supporto per **filtri combinati**, **payload personalizzati**, **tag**\n",
    "* API REST o gRPC + supporto per client Python (`qdrant-client`)\n",
    "* Persistenza automatica e clustering\n",
    "\n",
    "---\n",
    "\n",
    "## **Installazione**\n",
    "\n",
    "### Server Qdrant (opzioni):\n",
    "\n",
    "* Via Docker:\n",
    "\n",
    "```bash\n",
    "docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant\n",
    "```\n",
    "\n",
    "### Python client:\n",
    "\n",
    "```bash\n",
    "pip install qdrant-client langchain-community\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Esempio pratico: Hybrid Search con Qdrant**\n",
    "\n",
    "### 1. Inizializzazione embedding e client\n",
    "\n",
    "```python\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Embedding model open-source\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Connessione al server Qdrant locale\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Creazione della collection (con supporto hybrid)\n",
    "\n",
    "```python\n",
    "# Dimensione dell'embedding\n",
    "dim = len(embedding_model.embed_query(\"example\"))\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=\"hybrid_demo\",\n",
    "    vectors_config={\"default\": {\"size\": dim, \"distance\": \"Cosine\"}},\n",
    "    optimizers_config={\"default_segment_number\": 1},\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Inserimento dei documenti\n",
    "\n",
    "```python\n",
    "from langchain.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain is a powerful framework for building LLM applications.\", metadata={\"id\": \"doc1\"}),\n",
    "    Document(page_content=\"FAISS enables fast semantic search over text data.\", metadata={\"id\": \"doc2\"}),\n",
    "    Document(page_content=\"Python is widely used in AI projects.\", metadata={\"id\": \"doc3\"}),\n",
    "]\n",
    "\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "vector_store = Qdrant.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"hybrid_demo\",\n",
    "    client=client\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Esecuzione di una Hybrid Search\n",
    "\n",
    "```python\n",
    "query = \"How can I build AI applications?\"\n",
    "\n",
    "# Metodo 1: solo vector search\n",
    "results_vector = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "# Metodo 2: hybrid search con BM25 + vector\n",
    "results_hybrid = client.search(\n",
    "    collection_name=\"hybrid_demo\",\n",
    "    query_vector=embedding_model.embed_query(query),\n",
    "    with_payload=True,\n",
    "    limit=3,\n",
    "    with_vectors=False,\n",
    "    score_threshold=None,\n",
    "    params={\n",
    "        \"exact\": False,\n",
    "        \"hybrid\": {\n",
    "            \"alpha\": 0.5,       # bilanciamento: 0 = solo BM25, 1 = solo vettore\n",
    "            \"vector\": True,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "for hit in results_hybrid:\n",
    "    print(hit.payload)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Vantaggi della ricerca ibrida con Qdrant**\n",
    "\n",
    "1. **Precisione migliorata** – Recupera documenti con match testuale *e* significato simile.\n",
    "2. **Supporto nativo** – Nessuna logica custom: BM25 + embedding sono integrati.\n",
    "3. **Filtri avanzati** – Puoi filtrare per metadati (es. categoria, data, autore).\n",
    "4. **Persistenza automatica** – Non c'è bisogno di gestire manualmente salvataggi.\n",
    "\n",
    "---\n",
    "\n",
    "## **Quando usare la Hybrid Search**\n",
    "\n",
    "* Quando i documenti contengono **termini tecnici precisi** ma anche sinonimi e varianti linguistiche.\n",
    "* Quando vuoi **evitare risultati semanticamente simili ma fuori tema**.\n",
    "* In domini come **ambiti legali, medici, documentazione tecnica**, dove sia il significato che la terminologia contano.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusione**\n",
    "\n",
    "La Hybrid Search combina il meglio dei due mondi: la precisione delle keyword (BM25) e la flessibilità semantica degli embedding.\n",
    "**Qdrant** è uno dei pochi vector store a supportare questa modalità **nativamente**, rendendolo una scelta eccellente per applicazioni RAG moderne, scalabili e sensibili al contesto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211d4a1-efa4-4af2-a8c4-0721114fe57f",
   "metadata": {},
   "source": [
    "## progetto RAG completo\n",
    "\n",
    "* **Documenti simulati** (in inglese)\n",
    "* **FAISS** come vector store (senza hybrid search)\n",
    "* **Embedding open-source gratuito** di Hugging Face (`all-MiniLM-L6-v2`)\n",
    "* **LM Studio** come LLM per rispondere alle domande (server OpenAI-compatible)\n",
    "\n",
    "Il codice include le ottimizzazioni: text splitting accurato, FAISS persistente (save/load per evitare rebuild), retriever con **MMR** per diversificazione dei risultati, prompt con **citazioni** alle fonti e struttura modulare “production-ready”.\n",
    "\n",
    "---\n",
    "\n",
    "## Requisiti\n",
    "\n",
    "```bash\n",
    "pip install -qU faiss-cpu langchain langchain-community sentence-transformers\n",
    "```\n",
    "\n",
    "> Assicurati che **LM Studio** sia in esecuzione e stia esponendo un endpoint OpenAI-compatible (default: `http://localhost:1234/v1`).\n",
    "> Carica un modello in LM Studio (es. Mistral, Llama) e annotane il nome visualizzato.\n",
    "\n",
    "---\n",
    "\n",
    "## Variabili d’ambiente (LM Studio)\n",
    "Crea un file .env\n",
    "\n",
    "```powershell\n",
    "OPENAI_BASE_URL=\"http://localhost:1234/v1\"\n",
    "OPENAI_API_KEY=\"not-needed\"\n",
    "LMSTUDIO_MODEL=\"mistral\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Codice completo\n",
    "\n",
    "> Salvalo come `rag_faiss_lmstudio.py` e avvialo.\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import faiss\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings \n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LangChain Core (prompt/chain)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Chat model init (provider-agnostic, qui puntiamo a LM Studio via OpenAI-compatible)\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Configurazione\n",
    "# =========================\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@dataclass\n",
    "class Settings:\n",
    "    # Persistenza FAISS\n",
    "    persist_dir: str = \"faiss_index_example\"\n",
    "    # Text splitting\n",
    "    chunk_size: int = 700\n",
    "    chunk_overlap: int = 100\n",
    "    # Retriever (MMR)\n",
    "    search_type: str = \"mmr\"        # \"mmr\" o \"similarity\"\n",
    "    k: int = 4                      # risultati finali\n",
    "    fetch_k: int = 20               # candidati iniziali (per MMR)\n",
    "    mmr_lambda: float = 0.3         # 0 = diversificazione massima, 1 = pertinenza massima\n",
    "    # Embedding\n",
    "    hf_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    # LM Studio (OpenAI-compatible)\n",
    "    lmstudio_model_env: str = \"LMSTUDIO_MODEL\"  # nome del modello in LM Studio, via env var\n",
    "\n",
    "\n",
    "\n",
    "SETTINGS = Settings()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Componenti di base\n",
    "# =========================\n",
    "\n",
    "def get_embeddings(settings: Settings) -> HuggingFaceEmbeddings:\n",
    "    \"\"\"\n",
    "    Restituisce un modello di embedding locale e gratuito (Hugging Face).\n",
    "    \"\"\"\n",
    "    return HuggingFaceEmbeddings(model_name=settings.hf_model_name)\n",
    "\n",
    "\n",
    "def generate_embeddings(settings: Settings) -> AzureOpenAIEmbeddings:\n",
    "    return AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=os.getenv(\"AZURE_EMBEDDING_ENDPOINT\"),\n",
    "        azure_deployment=os.getenv(\"AZURE_EMBEDDING_MODEL\"),\n",
    "        openai_api_version=config.api_version\n",
    "    )\n",
    "\n",
    "\n",
    "def get_llm_from_lmstudio(settings: Settings):\n",
    "    \"\"\"\n",
    "    Inizializza un ChatModel puntando a LM Studio (OpenAI-compatible).\n",
    "    Richiede:\n",
    "      - OPENAI_BASE_URL (es. http://localhost:1234/v1)\n",
    "      - OPENAI_API_KEY (placeholder qualsiasi, es. \"not-needed\")\n",
    "      - LMSTUDIO_MODEL (nome del modello caricato in LM Studio)\n",
    "    \"\"\"\n",
    "    base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    model_name = os.getenv(settings.lmstudio_model_env)\n",
    "\n",
    "    if not base_url or not api_key:\n",
    "        raise RuntimeError(\n",
    "            \"OPENAI_BASE_URL e OPENAI_API_KEY devono essere impostate per LM Studio.\"\n",
    "        )\n",
    "    if not model_name:\n",
    "        raise RuntimeError(\n",
    "            f\"Imposta la variabile {settings.lmstudio_model_env} con il nome del modello caricato in LM Studio.\"\n",
    "        )\n",
    "\n",
    "    # model_provider=\"openai\" perché l'endpoint è OpenAI-compatible\n",
    "    return init_chat_model(model_name, model_provider=\"openai\")\n",
    "\n",
    "\n",
    "def simulate_corpus() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Crea un piccolo corpus di documenti in inglese con metadati e 'source' per citazioni.\n",
    "    \"\"\"\n",
    "    docs = [\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"LangChain is a framework that helps developers build applications \"\n",
    "                \"powered by Large Language Models (LLMs). It provides chains, agents, \"\n",
    "                \"prompt templates, memory, and integrations with vector stores.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc1\", \"source\": \"intro-langchain.md\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"FAISS is a library for efficient similarity search and clustering of dense vectors. \"\n",
    "                \"It supports exact and approximate nearest neighbor search and scales to millions of vectors.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc2\", \"source\": \"faiss-overview.md\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"Sentence-transformers like all-MiniLM-L6-v2 produce sentence embeddings suitable \"\n",
    "                \"for semantic search, clustering, and information retrieval. The embedding size is 384.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc3\", \"source\": \"embeddings-minilm.md\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"A typical RAG pipeline includes indexing (load, split, embed, store) and \"\n",
    "                \"retrieval+generation. Retrieval selects the most relevant chunks, and the LLM produces \"\n",
    "                \"an answer grounded in those chunks.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc4\", \"source\": \"rag-pipeline.md\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"Maximal Marginal Relevance (MMR) balances relevance and diversity during retrieval. \"\n",
    "                \"It helps avoid redundant chunks and improves coverage of different aspects.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc5\", \"source\": \"retrieval-mmr.md\"}\n",
    "        ),\n",
    "    ]\n",
    "    return docs\n",
    "\n",
    "\n",
    "def split_documents(docs: List[Document], settings: Settings) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Applica uno splitting robusto ai documenti per ottimizzare il retrieval.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=settings.chunk_size,\n",
    "        chunk_overlap=settings.chunk_overlap,\n",
    "        separators=[\n",
    "            \"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \"; \", \": \",\n",
    "            \", \", \" \", \"\"  # fallback aggressivo\n",
    "        ],\n",
    "    )\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "def build_faiss_vectorstore(chunks: List[Document], embeddings: HuggingFaceEmbeddings, persist_dir: str) -> FAISS:\n",
    "    \"\"\"\n",
    "    Costruisce da zero un FAISS index (IndexFlatL2) e lo salva su disco.\n",
    "    \"\"\"\n",
    "    # Determina la dimensione dell'embedding\n",
    "    vs = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "\n",
    "    Path(persist_dir).mkdir(parents=True, exist_ok=True)\n",
    "    vs.save_local(persist_dir)\n",
    "    return vs\n",
    "\n",
    "\n",
    "def load_or_build_vectorstore(settings: Settings, embeddings: HuggingFaceEmbeddings, docs: List[Document]) -> FAISS:\n",
    "    \"\"\"\n",
    "    Tenta il load di un indice FAISS persistente; se non esiste, lo costruisce e lo salva.\n",
    "    \"\"\"\n",
    "    persist_path = Path(settings.persist_dir)\n",
    "    index_file = persist_path / \"index.faiss\"\n",
    "    meta_file = persist_path / \"index.pkl\"\n",
    "\n",
    "    if index_file.exists() and meta_file.exists():\n",
    "        # Dal 2024/2025 molte build richiedono il flag 'allow_dangerous_deserialization' per caricare pkl locali\n",
    "        return FAISS.load_local(\n",
    "            settings.persist_dir,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "    chunks = split_documents(docs, settings)\n",
    "    return build_faiss_vectorstore(chunks, embeddings, settings.persist_dir)\n",
    "\n",
    "\n",
    "def make_retriever(vector_store: FAISS, settings: Settings):\n",
    "    \"\"\"\n",
    "    Configura il retriever. Con 'mmr' otteniamo risultati meno ridondanti e più coprenti.\n",
    "    \"\"\"\n",
    "    if settings.search_type == \"mmr\":\n",
    "        return vector_store.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": settings.k, \"fetch_k\": settings.fetch_k, \"lambda_mult\": settings.mmr_lambda},\n",
    "        )\n",
    "    else:\n",
    "        return vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": settings.k},\n",
    "        )\n",
    "\n",
    "\n",
    "def format_docs_for_prompt(docs: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Prepara il contesto per il prompt, includendo citazioni [source].\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        src = d.metadata.get(\"source\", f\"doc{i}\")\n",
    "        lines.append(f\"[source:{src}] {d.page_content}\")\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "\n",
    "def build_rag_chain(llm, retriever):\n",
    "    \"\"\"\n",
    "    Costruisce la catena RAG (retrieval -> prompt -> LLM) con citazioni e regole anti-hallucination.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"Sei un assistente esperto. Rispondi in italiano. \"\n",
    "        \"Usa esclusivamente il CONTENUTO fornito nel contesto. \"\n",
    "        \"Se l'informazione non è presente, dichiara che non è disponibile. \"\n",
    "        \"Includi citazioni tra parentesi quadre nel formato [source:...]. \"\n",
    "        \"Sii conciso, accurato e tecnicamente corretto.\"\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\",\n",
    "         \"Domanda:\\n{question}\\n\\n\"\n",
    "         \"Contesto (estratti selezionati):\\n{context}\\n\\n\"\n",
    "         \"Istruzioni:\\n\"\n",
    "         \"1) Rispondi solo con informazioni contenute nel contesto.\\n\"\n",
    "         \"2) Cita sempre le fonti pertinenti nel formato [source:FILE].\\n\"\n",
    "         \"3) Se la risposta non è nel contesto, scrivi: 'Non è presente nel contesto fornito.'\")\n",
    "    ])\n",
    "\n",
    "    # LCEL: dict -> prompt -> llm -> parser\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs_for_prompt,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "def rag_answer(question: str, chain) -> str:\n",
    "    \"\"\"\n",
    "    Esegue la catena RAG per una singola domanda.\n",
    "    \"\"\"\n",
    "    return chain.invoke(question)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Esecuzione dimostrativa\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    settings = SETTINGS\n",
    "\n",
    "    # 1) Componenti\n",
    "    embeddings = get_embeddings(settings)\n",
    "    llm = get_llm_from_lmstudio(settings)\n",
    "\n",
    "    # 2) Dati simulati e indicizzazione (load or build)\n",
    "    docs = simulate_corpus()\n",
    "    vector_store = load_or_build_vectorstore(settings, embeddings, docs)\n",
    "\n",
    "    # 3) Retriever ottimizzato\n",
    "    retriever = make_retriever(vector_store, settings)\n",
    "\n",
    "    # 4) Catena RAG\n",
    "    chain = build_rag_chain(llm, retriever)\n",
    "\n",
    "    # 5) Esempi di domande\n",
    "    questions = [\n",
    "        \"Che cos'è una pipeline RAG e quali sono le sue fasi principali?\",\n",
    "        \"A cosa serve FAISS e quali capacità offre?\",\n",
    "        \"Cos'è MMR e perché è utile durante il retrieval?\",\n",
    "        \"Quale dimensione hanno gli embedding prodotti da all-MiniLM-L6-v2?\"\n",
    "    ]\n",
    "\n",
    "    for q in questions:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Q:\", q)\n",
    "        print(\"-\" * 80)\n",
    "        ans = rag_answer(q, chain)\n",
    "        print(ans)\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Note progettuali e scelte tecniche\n",
    "\n",
    "1. **Embedding locale e gratuito**\n",
    "   Usa `sentence-transformers/all-MiniLM-L6-v2` (dimensione 384). È veloce, senza costi e adatto a prototipi e piccoli sistemi RAG.\n",
    "\n",
    "2. **Text splitting**\n",
    "   `RecursiveCharacterTextSplitter` con `chunk_size=700` e `chunk_overlap=100` bilancia copertura e coerenza. Puoi regolare questi parametri se i tuoi documenti sono molto densi o molto eterogenei.\n",
    "\n",
    "3. **FAISS persistente**\n",
    "   `save_local()` e `load_local()` evitano di ricalcolare gli embedding e ricostruire l’indice ad ogni esecuzione. È importante per tempi di avvio rapidi in ambienti reali.\n",
    "\n",
    "4. **Retriever MMR**\n",
    "   `search_type=\"mmr\"` con `fetch_k` e `lambda_mult` aiuta a ridurre la ridondanza e a coprire aspetti diversi dei documenti. Se preferisci i “top-k” più densi, passa a `search_type=\"similarity\"`.\n",
    "\n",
    "5. **LM Studio via endpoint OpenAI-compatible**\n",
    "   `init_chat_model(..., model_provider=\"openai\")` punta a `OPENAI_BASE_URL` con `OPENAI_API_KEY` di comodo. Imposta `LMSTUDIO_MODEL` al nome del modello caricato in LM Studio.\n",
    "\n",
    "6. **Citations**\n",
    "   Le fonti sono propagate nei metadati `source` dei `Document`. La funzione `format_docs_for_prompt` costruisce un contesto con citazioni `[source:<file>]`.\n",
    "\n",
    "7. **Estensioni possibili**\n",
    "\n",
    "   * Contextual compression (LLM chain extractor) per ridurre i chunk prima del prompt.\n",
    "   * Filtri per metadati in fase di retrieval.\n",
    "   * Valutazioni offline con LangSmith (tracing, debugging) se sposti il backend a un provider remoto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7adfe-ce30-4473-95ad-1abf4a1274e1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1. **Search Type** (tipi di ricerca nel retriever)\n",
    "\n",
    "Quando usi un retriever (ad esempio con FAISS o Qdrant) hai diversi modi di recuperare i documenti in risposta a una query. I più comuni in LangChain sono:\n",
    "\n",
    "### a) **Similarity Search (default)**\n",
    "\n",
    "* Recupera i documenti **più vicini alla query** nello spazio vettoriale.\n",
    "* Funziona ordinando i risultati per distanza (o similarità) e prendendo i primi *k*.\n",
    "* È semplice, veloce e di solito sufficiente.\n",
    "* Problema: può restituire chunk molto simili tra loro (ridondanza).\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # prendi i 4 più simili\n",
    ")\n",
    "```\n",
    "\n",
    "### b) **MMR (Maximal Marginal Relevance)**\n",
    "\n",
    "* Non prende solo i *più simili*, ma **bilancia somiglianza e diversità**.\n",
    "* Funziona così:\n",
    "\n",
    "  1. Trova un insieme più ampio di candidati (*fetch\\_k*, es. 20)\n",
    "  2. Seleziona i primi risultati bilanciando:\n",
    "\n",
    "     * **Pertinenza** rispetto alla query\n",
    "     * **Diversità** rispetto ai documenti già scelti\n",
    "* Parametro chiave: **λ (lambda\\_mult)**\n",
    "\n",
    "  * 0 = privilegia solo la diversità\n",
    "  * 1 = privilegia solo la pertinenza\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 4, \"fetch_k\": 20, \"lambda_mult\": 0.3}\n",
    ")\n",
    "```\n",
    "\n",
    "* Utile quando:\n",
    "\n",
    "  * Vuoi **evitare ridondanza**\n",
    "  * Hai documenti che trattano aspetti diversi della query\n",
    "  * Vuoi aumentare **copertura informativa**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Chunking e Overlap**\n",
    "\n",
    "Quando hai documenti grandi (manuali, articoli, report), non puoi mandarli interamente all’LLM (per limiti di contesto). Si usano i **Text Splitter** per dividerli in porzioni più piccole (**chunk**).\n",
    "\n",
    "### a) **Chunk Size**\n",
    "\n",
    "* È la lunghezza massima di un pezzo di documento (in caratteri o token).\n",
    "* Scelta critica:\n",
    "\n",
    "  * Troppo piccolo → perdita di contesto, pezzi non informativi\n",
    "  * Troppo grande → rischi di superare il limite del modello e di avere embedding poco precisi\n",
    "\n",
    "Esempio:\n",
    "\n",
    "```python\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,   # ~700 caratteri per chunk\n",
    "    chunk_overlap=100\n",
    ")\n",
    "```\n",
    "\n",
    "Qui ogni chunk avrà circa **700 caratteri** (con punteggiatura come separatori preferiti).\n",
    "\n",
    "### b) **Chunk Overlap**\n",
    "\n",
    "* È la **sovrapposizione** tra un chunk e il successivo (in caratteri).\n",
    "* Serve per **non perdere il contesto alle frontiere dei chunk**.\n",
    "* Esempio: se un paragrafo è lungo 710 caratteri e il chunk size è 700 senza overlap, rischi di “tagliare” frasi a metà.\n",
    "* Con overlap=100:\n",
    "\n",
    "  * Il chunk1 contiene 0–700\n",
    "  * Il chunk2 contiene 600–1300\n",
    "  * Le frasi tra 600–700 appaiono in entrambi, preservando il contesto.\n",
    "\n",
    "### c) Linee guida pratiche\n",
    "\n",
    "* **Chunk size**:\n",
    "\n",
    "  * 500–1000 caratteri se usi modelli piccoli (es. MiniLM)\n",
    "  * 1000–2000 caratteri per GPT-4/Claude con contesti ampi\n",
    "\n",
    "* **Overlap**:\n",
    "\n",
    "  * 10–20% della lunghezza del chunk\n",
    "  * Tipicamente 100–200 caratteri\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio pratico combinato\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Splitting robusto con chunking e overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \"; \", \": \", \", \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Retriever con MMR per ridurre ridondanza\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.3}\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusione\n",
    "\n",
    "* **Search Type**:\n",
    "\n",
    "  * `similarity`: prende i più simili → veloce, ma rischia ridondanza\n",
    "  * `mmr`: bilancia pertinenza e diversità → più copertura, meno ridondanza\n",
    "\n",
    "* **Chunking & Overlap**:\n",
    "\n",
    "  * Chunk size determina la granularità delle unità di ricerca\n",
    "  * Overlap evita di perdere il contesto tra due chunk adiacenti\n",
    "\n",
    "Entrambi sono strumenti cruciali per garantire che un sistema RAG sia **accurato, completo e robusto**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd0b17-c88d-46e8-a206-2f84e40e3f4a",
   "metadata": {},
   "source": [
    "## Documenti reali\n",
    "\n",
    "Per utilizzare **documenti reali** invece di testi simulati nella tua pipeline RAG, devi costruire una funzione che **carichi file da disco**, li legga, e li converta in oggetti `Document` compatibili con LangChain.\n",
    "\n",
    "Vediamo come fare in modo semplice, ordinato e professionale.\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivo della funzione\n",
    "\n",
    "Sostituire questa funzione:\n",
    "\n",
    "```python\n",
    "def simulate_corpus() -> List[Document]:\n",
    "    ...\n",
    "```\n",
    "\n",
    "con una versione reale che carichi i contenuti da una cartella locale contenente file `.txt`, `.md`, `.pdf`, o altri formati supportati da LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "## Passaggi da seguire\n",
    "\n",
    "1. Leggere i file reali da una directory locale.\n",
    "2. Usare i **Document Loaders** di LangChain per convertirli in `Document`.\n",
    "3. Impostare metadati utili, come il nome del file (per le citazioni).\n",
    "4. Restituire una `List[Document]` identica come formato a quella della funzione simulata.\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio: caricamento da file `.txt` e `.md`\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def load_real_documents_from_folder(folder_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Carica documenti reali da file di testo (es. .txt, .md) all'interno di una cartella.\n",
    "    Ogni file viene letto e convertito in un oggetto Document con metadato 'source'.\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    documents: List[Document] = []\n",
    "\n",
    "    if not folder.exists() or not folder.is_dir():\n",
    "        raise ValueError(f\"La cartella '{folder_path}' non esiste o non è una directory.\")\n",
    "\n",
    "    for file_path in folder.glob(\"**/*\"):\n",
    "        if file_path.suffix.lower() not in [\".txt\", \".md\"]:\n",
    "            continue  # ignora file non supportati\n",
    "\n",
    "        loader = TextLoader(str(file_path), encoding=\"utf-8\")\n",
    "        docs = loader.load()\n",
    "\n",
    "        # Aggiunge il metadato 'source' per citazioni (es. nome del file)\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = file_path.name\n",
    "\n",
    "        documents.extend(docs)\n",
    "\n",
    "    return documents\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Utilizzo\n",
    "\n",
    "Nel tuo codice principale, sostituisci:\n",
    "\n",
    "```python\n",
    "docs = simulate_corpus()\n",
    "```\n",
    "\n",
    "con:\n",
    "\n",
    "```python\n",
    "docs = load_real_documents_from_folder(\"path/alla/cartella/documenti\")\n",
    "```\n",
    "\n",
    "Assicurati che la cartella contenga file `.txt` o `.md` leggibili, come:\n",
    "\n",
    "```\n",
    "/documenti/\n",
    "├── langchain_overview.md\n",
    "├── faiss_notes.txt\n",
    "├── embeddings_info.md\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Supporto per altri formati (PDF, HTML, CSV, ecc.)\n",
    "\n",
    "LangChain supporta altri loader:\n",
    "\n",
    "* `PyPDFLoader` per `.pdf`\n",
    "* `UnstructuredFileLoader` per `.docx`, `.pptx`, `.eml`\n",
    "* `BSHTMLLoader` per file HTML\n",
    "* `CSVLoader` per `.csv`\n",
    "\n",
    "Puoi combinarli usando controlli come:\n",
    "\n",
    "```python\n",
    "if file_path.suffix == \".pdf\":\n",
    "    loader = PyPDFLoader(str(file_path))\n",
    "elif file_path.suffix == \".html\":\n",
    "    loader = BSHTMLLoader(str(file_path))\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Considerazioni aggiuntive\n",
    "\n",
    "* I file troppo lunghi verranno poi **spezzati in chunk** tramite lo `TextSplitter`, come già previsto nel tuo codice.\n",
    "* Il metadato `\"source\"` è importante per le **citazioni automatiche** nei prompt RAG.\n",
    "* È buona norma assicurarsi che tutti i documenti siano in UTF-8 ed evitare file binari o malformati.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adff0f4-ce0b-4216-bb08-1290b1ea9238",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Come funziona la `build_rag_chain`\n",
    "\n",
    "```python\n",
    "def build_rag_chain(llm, retriever):\n",
    "```\n",
    "\n",
    "Questa funzione crea una **catena LCEL (LangChain Expression Language)** composta da 4 step:\n",
    "\n",
    "1. **Retriever + formatter** → recupera i documenti e li trasforma in stringa.\n",
    "2. **PromptTemplate** → costruisce il prompt per l’LLM con una struttura coerente.\n",
    "3. **LLM** → genera la risposta.\n",
    "4. **OutputParser** → estrae solo il testo generato.\n",
    "\n",
    "---\n",
    "\n",
    "##  Dettaglio: `system_prompt` e `ChatPromptTemplate`\n",
    "\n",
    "### 1. `system_prompt`\n",
    "\n",
    "Il system prompt serve a dare istruzioni **costanti** al modello, come se fosse la sua \"personalità\":\n",
    "\n",
    "```python\n",
    "system_prompt = (\n",
    "    \"Sei un assistente esperto. Rispondi in italiano. \"\n",
    "    \"Usa esclusivamente il CONTENUTO fornito nel contesto. \"\n",
    "    \"Se l'informazione non è presente, dichiara che non è disponibile. \"\n",
    "    \"Includi citazioni tra parentesi quadre nel formato [source:...]. \"\n",
    "    \"Sii conciso, accurato e tecnicamente corretto.\"\n",
    ")\n",
    "```\n",
    "\n",
    "Questo è **statico**, non cambia a ogni input.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `ChatPromptTemplate` con variabili\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\",\n",
    "     \"Domanda:\\n{question}\\n\\n\"\n",
    "     \"Contesto (estratti selezionati):\\n{context}\\n\\n\"\n",
    "     \"Istruzioni:\\n\"\n",
    "     \"1) Rispondi solo con informazioni contenute nel contesto.\\n\"\n",
    "     \"2) Cita sempre le fonti pertinenti nel formato [source:FILE].\\n\"\n",
    "     \"3) Se la risposta non è nel contesto, scrivi: 'Non è presente nel contesto fornito.'\")\n",
    "])\n",
    "```\n",
    "\n",
    "Qui usi **due messaggi**:\n",
    "\n",
    "* `(\"system\", ...)`: per il comportamento fisso del modello.\n",
    "* `(\"human\", ...)`: per l’input dell’utente, con **variabili** `{question}` e `{context}` che verranno riempite a runtime.\n",
    "\n",
    "---\n",
    "\n",
    "##  Cos’è la `chain` e il passaggio dei dati\n",
    "\n",
    "```python\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs_for_prompt,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "```\n",
    "\n",
    "### Step-by-step:\n",
    "\n",
    "1. `question`: passa direttamente il testo con `RunnablePassthrough()`.\n",
    "2. `context`: passa per il retriever e una funzione che formatta i documenti.\n",
    "3. `prompt`: usa `ChatPromptTemplate` per combinare domanda e contesto.\n",
    "4. `llm`: genera la risposta.\n",
    "5. `StrOutputParser`: prende solo il testo, escludendo metadati.\n",
    "\n",
    "---\n",
    "\n",
    "##  Come si esegue con `.invoke()`\n",
    "\n",
    "```python\n",
    "def rag_answer(question: str, chain) -> str:\n",
    "    return chain.invoke(question)\n",
    "```\n",
    "\n",
    "### `chain.invoke(...)`:\n",
    "\n",
    "* Prende in input una stringa (`question`).\n",
    "* La passa lungo la catena.\n",
    "* Recupera il contesto, costruisce il prompt, genera risposta.\n",
    "* Restituisce solo la stringa finale (grazie a `StrOutputParser()`).\n",
    "\n",
    "---\n",
    "\n",
    "##  Riassunto visivo\n",
    "\n",
    "```\n",
    "[User Question] \n",
    "   ↓\n",
    "[Retriever]\n",
    "   ↓\n",
    "[Contesto] + [Question]\n",
    "   ↓\n",
    "[Prompt Template]\n",
    "   ↓\n",
    "[LLM]\n",
    "   ↓\n",
    "[Output Parser]\n",
    "   ↓\n",
    "Risposta con citazioni accurate \n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ebaaf-e5a4-48f8-a482-b115d4dfa2df",
   "metadata": {},
   "source": [
    "Quando si sviluppa un'applicazione RAG (Retrieval-Augmented Generation) destinata a diventare un *tool* in un sistema più ampio (ad esempio come parte di un agente o flusso multi-step), è importante progettare la fase di *retrieval* in modo che i dati passati al modello rientrino nei limiti della **finestra di contesto (context window)** e risultino il più possibile **informativi**, **diversificati** e **non ridondanti**.\n",
    "\n",
    "Di seguito è riportato un approccio ragionato alla scelta di tre parametri fondamentali: **chunk size**, **k**, e **MMR**, in relazione alla finestra di contesto del modello.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Chunking: dimensione e sovrapposizione\n",
    "\n",
    "### Obiettivo:\n",
    "\n",
    "Suddividere i documenti in frammenti (chunk) abbastanza piccoli da poter essere gestiti dal modello, ma abbastanza grandi da mantenere la coerenza semantica del contenuto.\n",
    "\n",
    "### Linee guida:\n",
    "\n",
    "* Scegli chunk con lunghezza in token tra **300 e 700**, con **overlap del 10–20%**.\n",
    "* Se i documenti sono tecnici, tabellari o contenenti codice, prediligi chunk più brevi (150–300 token).\n",
    "* Utilizza splitter gerarchici, come `RecursiveCharacterTextSplitter`, per preservare la struttura (paragrafi, frasi, ecc.).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Numero di documenti recuperati (k)\n",
    "\n",
    "### Obiettivo:\n",
    "\n",
    "Selezionare un numero `k` di chunk rilevanti da fornire al modello senza superare la finestra di contesto.\n",
    "\n",
    "### Considerazioni:\n",
    "\n",
    "* Il totale dei token occupati dal **sistema + istruzioni + domanda + contesto + output atteso** deve rientrare nella finestra di contesto del modello.\n",
    "* Se la finestra è di 8k token, riserva indicativamente:\n",
    "\n",
    "  * 500–1000 token per il prompt (istruzioni + domanda)\n",
    "  * 300–600 token per la risposta attesa\n",
    "  * il resto per i chunk recuperati\n",
    "\n",
    "### Esempio:\n",
    "\n",
    "Con un modello a 8k token:\n",
    "\n",
    "* Prompt + domanda = 1000 token\n",
    "* Output massimo = 500 token\n",
    "* Rimanenti ≈ 6500 token → puoi includere circa 10–15 chunk da 400 token ciascuno\n",
    "\n",
    "Tuttavia, **non è necessario utilizzare tutto lo spazio disponibile**: a parità di rilevanza, meno contenuto ridondante consente al modello di concentrarsi meglio.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. MMR (Maximal Marginal Relevance)\n",
    "\n",
    "### Obiettivo:\n",
    "\n",
    "Bilanciare **rilevanza** e **diversità** tra i chunk selezionati, evitando ripetizioni e migliorando la copertura del tema.\n",
    "\n",
    "### Parametri:\n",
    "\n",
    "* `lambda_mult = 1.0`: solo rilevanza\n",
    "* `lambda_mult = 0.0`: solo diversità\n",
    "* **Valori consigliati**: tra **0.2 e 0.4**\n",
    "\n",
    "### Quando usarlo:\n",
    "\n",
    "* Se il corpus contiene contenuti simili tra loro (es. documentazione tecnica, legale, codice)\n",
    "* Se i documenti sono lunghi e la probabilità di selezionare passaggi ridondanti è alta\n",
    "\n",
    "---\n",
    "\n",
    "## Approccio consigliato alla progettazione\n",
    "\n",
    "1. **Conosci la finestra del modello**\n",
    "   Verifica il numero massimo di token che il modello supporta. I modelli OpenAI compatibili offrono 4k, 8k, 16k o 32k a seconda della versione.\n",
    "\n",
    "2. **Assegna budget espliciti**\n",
    "\n",
    "   * Prompt e istruzioni: 500–1000 token\n",
    "   * Domanda utente: 100–300 token\n",
    "   * Risposta desiderata: 300–800 token\n",
    "   * Contesto: tutto il resto\n",
    "\n",
    "3. **Scegli chunking adatto**\n",
    "\n",
    "   * 300–500 token per chunk sono una scelta solida\n",
    "   * Overlap di 50–100 token\n",
    "\n",
    "4. **Configura il retriever**\n",
    "\n",
    "   * Usa `search_type=\"mmr\"`\n",
    "   * Imposta `k = 4–8`\n",
    "   * Imposta `fetch_k = 15–30` per lasciare margine a MMR\n",
    "\n",
    "5. **Se vuoi automatizzare**, calcola a runtime il budget per il contesto e riduci dinamicamente `k` se il totale stimato supera i limiti\n",
    "\n",
    "---\n",
    "\n",
    "## Considerazioni aggiuntive nel caso di tool\n",
    "\n",
    "Quando il risultato del RAG viene **rielaborato da un altro LLM**, ad esempio come parte di una catena di ragionamento o un sistema agentico, allora:\n",
    "\n",
    "* Il prompt generato dal RAG deve essere **compresso e citabile**, non ridondante\n",
    "* È importante garantire che il contenuto **non saturi la nuova finestra di contesto**\n",
    "* Valuta l’uso di **contextual compression** (riassunto o filtro) se i documenti sono lunghi\n",
    "\n",
    "In questi casi, è ancora più importante che `k` sia modulato in base alla **lunghezza della risposta generata**: il risultato del RAG non è fine a se stesso, ma diventa input per un’altra generazione.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusione\n",
    "\n",
    "La scelta di `chunk_size`, `k` e `MMR` non può prescindere dalla context window del modello e dall’utilizzo successivo del risultato. In un contesto production-grade, è preferibile:\n",
    "\n",
    "* rendere questi parametri configurabili\n",
    "* misurare la lunghezza reale in token (o stimarla)\n",
    "* introdurre salvaguardie per evitare overflow\n",
    "\n",
    "Nel caso di tool, è ancora più importante che l'output sia breve, focalizzato e facilmente incapsulabile all'interno di un nuovo prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbba5c0-8c5f-4510-b7d8-7e25e432f250",
   "metadata": {},
   "source": [
    "# Ragas e Valutazione\n",
    "\n",
    "---\n",
    "\n",
    "# 0) Prerequisiti\n",
    "\n",
    "```bash\n",
    "pip install ragas\n",
    "```\n",
    "\n",
    "Ragas può usare il tuo LLM come **giudice**; passiamo l’istanza che già crei con `init_chat_model` e le tue HuggingFaceEmbeddings. \n",
    "\n",
    "---\n",
    "\n",
    "# 1) Import minimi\n",
    "\n",
    "Aggiungi questi import vicino agli altri:\n",
    "\n",
    "```python\n",
    "# --- RAGAS ---\n",
    "from ragas import evaluate, EvaluationDataset\n",
    "from ragas.metrics import (\n",
    "    context_precision,   # \"precision@k\" sui chunk recuperati\n",
    "    context_recall,      # copertura dei chunk rilevanti\n",
    "    faithfulness,        # ancoraggio della risposta al contesto\n",
    "    answer_relevancy,    # pertinenza della risposta vs domanda\n",
    "    answer_correctness,  # usa questa solo se hai ground_truth\n",
    ")\n",
    "```\n",
    "\n",
    "(Le metriche di Ragas nascono proprio per RAG: precision/recall del contesto e qualità della risposta). \n",
    "\n",
    "---\n",
    "\n",
    "# 2) Helper: raccogli i dati per la valutazione\n",
    "\n",
    "Aggiungi **queste due funzioni** sotto le tue definizioni (riutilizzano il tuo retriever e la tua chain):\n",
    "\n",
    "```python\n",
    "def get_contexts_for_question(retriever, question: str, k: int) -> List[str]:\n",
    "    \"\"\"Ritorna i testi dei top-k documenti (chunk) usati come contesto.\"\"\"\n",
    "    docs = docs = retriever.invoke(question)[:k]\n",
    "    return [d.page_content for d in docs]\n",
    "\n",
    "def build_ragas_dataset(\n",
    "    questions: List[str],\n",
    "    retriever,\n",
    "    chain,\n",
    "    k: int,\n",
    "    ground_truth: dict[str, str] | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Esegue la pipeline RAG per ogni domanda e costruisce il dataset per Ragas.\n",
    "    Ogni riga contiene: question, contexts, answer, (opzionale) ground_truth.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for q in questions:\n",
    "        contexts = get_contexts_for_question(retriever, q, k)\n",
    "        answer = chain.invoke(q)\n",
    "\n",
    "        row = {\n",
    "            # chiavi richieste da molte metriche Ragas\n",
    "            \"user_input\": q,\n",
    "            \"retrieved_contexts\": contexts,\n",
    "            \"response\": answer,\n",
    "        }\n",
    "        if ground_truth and q in ground_truth:\n",
    "            row[\"reference\"] = ground_truth[q]\n",
    "\n",
    "        dataset.append(row)\n",
    "    return dataset\n",
    "```\n",
    "\n",
    "> Nota: Ragas si aspetta chiavi **`question`**, **`contexts`**, **`answer`** e (se la hai) **`ground_truth`**. In questo modo puoi usare `answer_correctness`; altrimenti lasciala fuori. \n",
    "\n",
    "---\n",
    "\n",
    "# 3) Aggiungi la fase di valutazione in `main()`\n",
    "\n",
    "Sostituisci il loop che stampava solo Q/A con questo blocco **compatto**:\n",
    "\n",
    "```python\n",
    "    # 5) Esempi di domande\n",
    "    questions = [\n",
    "        \"Che cos'è una pipeline RAG e quali sono le sue fasi principali?\",\n",
    "        \"A cosa serve FAISS e quali capacità offre?\",\n",
    "        \"Cos'è MMR e perché è utile durante il retrieval?\",\n",
    "        \"Quale dimensione hanno gli embedding prodotti da all-MiniLM-L6-v2?\"\n",
    "    ]\n",
    "\n",
    "    # (opzionale) ground truth sintetica per correctness\n",
    "    ground_truth = {\n",
    "        questions[0]: \"Indicizzazione (caricamento, splitting, embedding, storage) e retrieval + generazione.\",\n",
    "        questions[1]: \"Libreria per ricerca di similarità e clustering di vettori densi (ANN/NNN) scalabile.\",\n",
    "        questions[2]: \"Bilancia pertinenza e diversità per ridurre ridondanza e coprire aspetti differenti.\",\n",
    "        questions[3]: \"384\",\n",
    "    }\n",
    "\n",
    "    # 6) Costruisci dataset per Ragas (stessi top-k del tuo retriever)\n",
    "    dataset = build_ragas_dataset(\n",
    "        questions=questions,\n",
    "        retriever=retriever,\n",
    "        chain=chain,\n",
    "        k=settings.k,\n",
    "        ground_truth=ground_truth,  # rimuovi se non vuoi correctness\n",
    "    )\n",
    "\n",
    "    evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
    "\n",
    "    # 7) Scegli le metriche\n",
    "    metrics = [context_precision, context_recall, faithfulness, answer_relevancy]\n",
    "    # Aggiungi correctness solo se tutte le righe hanno ground_truth\n",
    "    if all(\"ground_truth\" in row for row in dataset):\n",
    "        metrics.append(answer_correctness)\n",
    "\n",
    "    # 8) Esegui la valutazione con il TUO LLM e le TUE embeddings\n",
    "    ragas_result = evaluate(\n",
    "        dataset=evaluation_dataset,\n",
    "        metrics=metrics,\n",
    "        llm=llm,                 # passa l'istanza LangChain del tuo LLM (LM Studio)\n",
    "        embeddings=get_embeddings(settings),  # o riusa 'embeddings' creato sopra\n",
    "    )\n",
    "\n",
    "    df = ragas_result.to_pandas()\n",
    "    cols = [\"user_input\", \"response\", \"context_precision\", \"context_recall\", \"faithfulness\", \"answer_relevancy\"]\n",
    "    print(\"\\n=== DETTAGLIO PER ESEMPIO ===\")\n",
    "    print(df[cols].round(4).to_string(index=False))\n",
    "\n",
    "    # (facoltativo) salva per revisione umana\n",
    "    df.to_csv(\"ragas_results.csv\", index=False)\n",
    "    print(\"Salvato: ragas_results.csv\")\n",
    "```\n",
    "\n",
    "Perché funziona così “plug-and-play”? Perché Ragas, quando gli passi LLM/Embeddings di **LangChain**, li incapsula con i suoi wrapper automaticamente (non devi fare altro). \n",
    "\n",
    "---\n",
    "\n",
    "# 4) Cosa leggere nei risultati\n",
    "\n",
    "* **context\\_precision** ↑ = i primi k chunk sono pertinenti alla domanda (retrieval “pulito”).\n",
    "* **context\\_recall** ↑ = copri i pezzi necessari.\n",
    "* **faithfulness** ↑ = la risposta sta **dentro** il contesto (meno allucinazioni).\n",
    "* **answer\\_relevancy** ↑ = risposta on-topic.\n",
    "* **answer\\_correctness** ↑ = risposta corretta vs ground truth (se fornita).\n",
    "\n",
    "(Queste sono proprio le metriche cardine per RAG “end-to-end”). \n",
    "\n",
    "---\n",
    "\n",
    "## FAQ veloci\n",
    "\n",
    "* **Serve per forza OpenAI?** No: stai già usando LM Studio con endpoint OpenAI-compatible; passiamo la tua istanza `llm` a Ragas e va bene. \n",
    "* **Posso usare solo alcune metriche?** Certo: passane anche una sola (es. `faithfulness`) e aggiungi le altre più avanti. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e277a1b-e05f-4ded-a620-2659934a39bd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 1) Cosa serve a Ragas (schema dei dati)\n",
    "\n",
    "Ragas valuta il tuo RAG se, per ogni domanda, fornisci un “record” con queste colonne:\n",
    "\n",
    "* **user\\_input** → la domanda dell’utente\n",
    "* **retrieved\\_contexts** → lista dei passaggi/chunk passati al modello\n",
    "* **response** → la risposta generata\n",
    "* **reference** *(opzionale)* → risposta “gold”/attesa\n",
    "\n",
    "Puoi costruire questi record con `EvaluationDataset.from_list([...])`. Ragas poi calcola le metriche e, con `to_pandas()`, ti restituisce una tabella per-esempio. \n",
    "\n",
    "---\n",
    "\n",
    "# 2) Le 4 metriche “core” (le più usate)\n",
    "\n",
    "## a) **Faithfulness** (fedeltà/groundedness)\n",
    "\n",
    "* **Che cosa misura:** quanto la risposta è **supportata** dai passaggi recuperati (niente allucinazioni).\n",
    "* **Range:** 0–1 (più alto è meglio).\n",
    "* **Come viene stimata:** il giudice LLM estrae le affermazioni dalla risposta e verifica se si **inferiscono** dai contesti forniti. \n",
    "* **Quando guardarla:** sempre. È la prima linea di difesa contro le allucinazioni.\n",
    "\n",
    "## b) **Answer relevancy** (pertinenza risposta↔domanda)\n",
    "\n",
    "* **Che cosa misura:** quanto la risposta è **aderente** alla domanda (non vaga, non fuori tema).\n",
    "* **Range:** 0–1 (alto = più on-topic).\n",
    "* **Come viene stimata:** giudice LLM confronta domanda, contesto e risposta premiando completezza e penalizzando ridondanza/fuori tema. \n",
    "\n",
    "## c) **Context precision** (precisione del retrieval)\n",
    "\n",
    "* **Che cosa misura:** tra i **top-k** passaggi recuperati, **quant’è la quota di passaggi davvero utili/rilevanti** per rispondere alla domanda (di fatto una **precision\\@k**).\n",
    "* **Range:** 0–1 (alto = i primi k sono pertinenti).\n",
    "* **Come viene stimata:** giudice LLM valuta la pertinenza dei contesti rispetto alla domanda/risposta; in alcune versioni può usare anche la reference. \n",
    "* **Quando guardarla:** per capire se il retriever porta **subito** contenuti buoni (ordinamento e qualità dei top-k).\n",
    "\n",
    "## d) **Context recall** (copertura del retrieval)\n",
    "\n",
    "* **Che cosa misura:** se i passaggi recuperati **coprono** ciò che serve per rispondere (recall dei contenuti necessari).\n",
    "* **Range:** 0–1 (alto = hai recuperato abbastanza evidenza).\n",
    "* **Come viene stimata:** giudice LLM (e/o confronto con reference) per stimare pezzi “mancanti”.\n",
    "* **Quando guardarla:** se la risposta è incompleta: può essere un problema di **recall** (servono k più alti, MMR, rerank).\n",
    "\n",
    "> In pratica: **precision** ti dice “quanto sono buoni i **primi** k”, **recall** ti dice “se nel recupero c’era **tutto il necessario**”. Le due insieme fotografano bene il retriever. \n",
    "\n",
    "---\n",
    "\n",
    "# 3) Metriche utili aggiuntive (quando servono)\n",
    "\n",
    "* **Context utilization** → misura **quanto** del contesto recuperato è **effettivamente usato** nella risposta (aiuta a diagnosticare risposte “di pancia” dell’LLM). \n",
    "* **Context entity recall** → per use-case “a entità” (nomi, ID, ecc.): verifica se le **entità** chiave presenti nella reference sono coperte dal contesto recuperato. \n",
    "* **Noise sensitivity** → come degrada la qualità se **inietti rumore** nel contesto (stress-test del prompt/LLM). \n",
    "* **Answer correctness** → correttezza **vs** reference (serve la colonna `reference`); utile quando hai risposte gold. \n",
    "\n",
    "> Nota: Ragas evolve e i nomi/parametri possono variare leggermente tra versioni; controlla sempre la pagina “Metrics” della tua release. \n",
    "\n",
    "---\n",
    "\n",
    "# 4) Quali colonne servono per ciascuna metrica (in breve)\n",
    "\n",
    "| Metrica              | Colonne minime tipiche                                                               |\n",
    "| -------------------- | ------------------------------------------------------------------------------------ |\n",
    "| faithfulness         | `user_input`, `retrieved_contexts`, `response`                                       |\n",
    "| answer\\_relevancy    | `user_input`, `retrieved_contexts`, `response`                                       |\n",
    "| context\\_precision   | `user_input`, `retrieved_contexts` *(talvolta anche `reference` a seconda versione)* |\n",
    "| context\\_recall      | `user_input`, `retrieved_contexts` *(spesso usa `reference` per stimare copertura)*  |\n",
    "| answer\\_correctness  | `user_input`, `response`, **`reference`**                                            |\n",
    "| context\\_utilization | `user_input`, `retrieved_contexts`, `response`                                       |\n",
    "\n",
    "(Se i tuoi campi hanno nomi diversi, usa `column_map` in `evaluate()` **oppure** genera già i record con quei nomi.) \n",
    "\n",
    "---\n",
    "\n",
    "# 5) Come leggere i numeri (diagnostica rapida)\n",
    "\n",
    "* **Faithfulness ↑** ma **answer\\_relevancy ↓** → la risposta è fedele ma **non risponde bene**: migliora **prompt di stile/formato** e “instruction following”.\n",
    "* **Precision ↑** e **Recall ↓** → i primi k sono buoni ma **manca copertura**: alza **k**, usa **MMR**/rerank.\n",
    "* **Precision ↓** e **Recall ↑** → c’è il contenuto giusto ma **è in basso**: serve **reranking**.\n",
    "* **Tutto alto tranne correctness** → hai una **reference diversa** (formulazione) o un **mismatch semantico**: valuta **exact match/F1** in parallelo o cura la reference.\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Consigli pratici per risultati stabili\n",
    "\n",
    "* Usa lo **stesso LLM** (lingua/temperatura bassa) come giudice, e **stessa lingua** di domanda/risposta/contesto.\n",
    "* Valuta **più k** (1/3/5) e confronta **MMR vs similarità pura** per capire se ti serve **reranking**.\n",
    "* Se hai **gold answer**, aggiungi `answer_correctness` per una vista “fatta-vs-atteso”; se non l’hai, affidati a **faithfulness + relevancy**.\n",
    "* Esporta la tabella e **controlla a campione** i casi peggiori (ordina per metrica ascendente).\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
