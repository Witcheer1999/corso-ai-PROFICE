{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6cc0d71-c16f-42f1-8ebe-a8d758efc9c0",
   "metadata": {},
   "source": [
    "**Introduzione a LangChain e agli agenti RAG**\n",
    "\n",
    "LangChain √® una libreria Python progettata per facilitare l'integrazione dei Large Language Model (LLM) con dati esterni, strumenti e ambienti complessi. Il suo obiettivo principale √® permettere lo sviluppo di applicazioni avanzate che non si limitano a generare testo, ma che possono interagire dinamicamente con fonti di conoscenza, API, database e sistemi esterni.\n",
    "\n",
    "Tra le applicazioni pi√π interessanti realizzabili con LangChain ci sono gli **agenti**, ovvero sistemi intelligenti che utilizzano un LLM per prendere decisioni, selezionare strumenti, pianificare azioni e risolvere compiti. Gli agenti sono fondamentali per automatizzare processi multi-step, dove serve non solo completare testi ma anche eseguire azioni guidate dal ragionamento.\n",
    "\n",
    "Una delle architetture pi√π efficaci per rispondere a domande specifiche su fonti informative complesse √® quella degli **agenti RAG** (Retrieval-Augmented Generation). In questa struttura, l‚Äôagente combina due fasi principali:\n",
    "\n",
    "1. **Retrieval** ‚Äì Recupera documenti rilevanti da una base di conoscenza tramite tecniche di ricerca semantica o keyword-based.\n",
    "2. **Generation** ‚Äì Genera una risposta sintetica e coerente basandosi sui documenti recuperati.\n",
    "\n",
    "Questo approccio permette di superare i limiti di memoria degli LLM, mantenendo la risposta aderente a una fonte verificabile. RAG √® utilizzato in molte applicazioni reali, ad esempio per costruire chatbot aziendali, assistenti legali, motori di ricerca semantici e sistemi di supporto decisionale.\n",
    "\n",
    "LangChain fornisce strumenti modulari per implementare agenti RAG, integrando moduli di embedding, vector store, chain di prompt, strumenti personalizzati e controllo del flusso logico.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6e080-5427-4657-99d7-ba86fa0e2c45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Struttura di una tipica applicazione RAG**\n",
    "\n",
    "Una tipica applicazione **Retrieval-Augmented Generation (RAG)** √® composta da due componenti principali:\n",
    "\n",
    "1. **Indicizzazione (Indexing)**\n",
    "   Un processo che serve a caricare, suddividere e indicizzare i dati provenienti da una fonte. Questa fase viene eseguita **offline**, prima dell‚Äôinterazione con l‚Äôutente.\n",
    "\n",
    "2. **Recupero e generazione (Retrieval and Generation)**\n",
    "   La fase eseguita **a runtime**, quando l‚Äôutente pone una domanda. Il sistema recupera i dati rilevanti dall‚Äôindice e li fornisce al modello, che genera la risposta.\n",
    "\n",
    "> **Nota**: la fase di indicizzazione segue spesso lo stesso schema di un sistema di ricerca semantica.\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline completa: dal dato grezzo alla risposta**\n",
    "\n",
    "Il flusso completo pi√π comune, dalla fonte di dati grezza alla risposta generata, prevede i seguenti passaggi:\n",
    "\n",
    "#### **1. Indexing**\n",
    "\n",
    "* **Load** ‚Äì Caricamento dei dati\n",
    "  I dati vengono caricati tramite **Document Loaders**, connettori che leggono file, pagine web, PDF, database, ecc.\n",
    "\n",
    "* **Split** ‚Äì Suddivisione in chunk\n",
    "  I **Text Splitters** dividono i documenti in porzioni pi√π piccole. Questo √® utile sia per ottimizzare la ricerca, sia per assicurarsi che il contenuto rientri nel contesto gestibile del modello LLM (limite di token).\n",
    "\n",
    "* **Store** ‚Äì Salvataggio e indicizzazione\n",
    "  Le porzioni di testo (chunk) vengono memorizzate e indicizzate all'interno di un **Vector Store**, dove ogni chunk √® trasformato in un **embedding vettoriale**. Questo consente di effettuare ricerche semantiche rapide e precise.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc27b7-88f4-4bb7-9c25-53d58329155e",
   "metadata": {},
   "source": [
    "![Alt text](rag_indexing-8160f90a90a33253d0154659cf7d453f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03515f-2709-4a5d-8bca-a2ad648d3034",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **2. Retrieval and Generation**\n",
    "\n",
    "Una volta che i dati sono stati indicizzati, entra in gioco la seconda fase: **recupero e generazione**, eseguita ogni volta che l‚Äôutente pone una domanda.\n",
    "\n",
    "#### **Retrieve ‚Äì Recupero dei dati rilevanti**\n",
    "\n",
    "A partire dall‚Äôinput dell‚Äôutente (la **query**), il sistema recupera i chunk pi√π pertinenti dalla memoria vettoriale.\n",
    "Questo avviene tramite un componente chiamato **Retriever**, che esegue una ricerca semantica confrontando l‚Äôembedding della query con quelli memorizzati.\n",
    "\n",
    "#### **Generate ‚Äì Generazione della risposta**\n",
    "\n",
    "I chunk recuperati, insieme alla domanda dell‚Äôutente, vengono inseriti in un **prompt** che viene poi fornito a un **ChatModel** o **LLM** (Large Language Model).\n",
    "Il modello genera una risposta sfruttando il contesto fornito dal materiale recuperato, evitando allucinazioni e mantenendo l‚Äôaderenza ai dati reali.\n",
    "\n",
    "---\n",
    "\n",
    "Questa architettura consente di ottenere risposte pi√π accurate e affidabili rispetto a un LLM standalone, specialmente quando √® necessario rispondere su dati proprietari, documentazione interna o fonti aggiornabili.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae00617-af54-493b-b0b2-8c3d84146b19",
   "metadata": {},
   "source": [
    "![Alt text](rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617214a4-175e-410e-93d8-bf129b35dd3a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Installazione**\n",
    "\n",
    "Per seguire questo tutorial √® necessario installare alcune dipendenze di **LangChain**.\n",
    "Puoi farlo tramite **pip**.\n",
    "\n",
    "#### **Installazione via pip**\n",
    "\n",
    "```bash\n",
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
    "```\n",
    "\n",
    "Questo comando installa:\n",
    "\n",
    "* `langchain-text-splitters`: per suddividere i documenti in chunk\n",
    "* `langchain-community`: insieme di integrazioni e loader della community\n",
    "* `langgraph`: framework per costruire agenti complessi tramite grafi\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c2c61-e5fe-412a-8983-471f52162ebc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Componenti necessari**\n",
    "\n",
    "Per costruire la nostra applicazione RAG, dobbiamo selezionare tre componenti fondamentali dalla suite di integrazioni offerte da LangChain:\n",
    "\n",
    "1. **Modello di chat (LLM)**\n",
    "2. **Modello di embeddings**\n",
    "3. **Vector store (memoria vettoriale)**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1358f17d-a8c1-4bcd-ab53-23ad53f01849",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **1. Selezione del modello di chat (LLM)**\n",
    "\n",
    "LangChain consente di utilizzare diversi provider per accedere a modelli di linguaggio come GPT-4, GPT-3.5, Claude, ecc.\n",
    "In questo esempio, ci concentreremo su **modelli OpenAI compatibili**, scegliendo tra:\n",
    "\n",
    "* **OpenAI** (API ufficiale)\n",
    "* **Azure OpenAI**\n",
    "* **LM Studio** (server locale compatibile con OpenAI)\n",
    "\n",
    "Per tutti i casi, il modulo da utilizzare √® `init_chat_model` di `langchain.chat_models`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Installazione**\n",
    "\n",
    "```bash\n",
    "pip install -qU \"langchain[openai]\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Opzione 1 ‚Äì Connessione via OpenAI API (ufficiale)**\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Imposta la chiave API se non gi√† presente\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Inserisci la tua API Key OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Inizializza un modello GPT-4o-mini usando OpenAI come provider\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Opzione 2 ‚Äì Connessione via Azure OpenAI**\n",
    "\n",
    "> Azure richiede di specificare **API key**, **endpoint** e **nome del deployment**.\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Imposta le variabili per Azure OpenAI\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Inserisci la Azure API Key: \")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = input(\"Inserisci l'endpoint Azure (es. https://nome.cognitiveservices.azure.com): \")\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = input(\"Inserisci il nome del deployment (es. gpt-4o-deployment): \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\n",
    "    \"gpt-4o\", \n",
    "    model_provider=\"azure\",\n",
    ")\n",
    "```\n",
    "\n",
    "LangChain rileva automaticamente le variabili `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` e `AZURE_OPENAI_CHAT_DEPLOYMENT_NAME`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Opzione 3 ‚Äì Connessione via LM Studio (server locale compatibile OpenAI)**\n",
    "\n",
    "> LM Studio espone un endpoint locale compatibile con OpenAI (tipicamente su `http://localhost:1234/v1`)\n",
    "\n",
    "```python\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Configura l'accesso al server locale LM Studio\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"not-needed\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"http://localhost:1234/v1\"\n",
    "\n",
    "llm = init_chat_model(\n",
    "    \"lmstudio-model\",           # Es: \"mistral\", \"llama3\", ecc.\n",
    "    model_provider=\"openai\"     # LM Studio √® compatibile OpenAI, quindi usiamo questo\n",
    ")\n",
    "```\n",
    "\n",
    "Puoi ottenere il nome esatto del modello aperto su LM Studio tramite l‚Äôinterfaccia oppure controllando i log nel terminale.\n",
    "\n",
    "---\n",
    "\n",
    "### **Nota finale**\n",
    "\n",
    "Il metodo `init_chat_model()` astrae la complessit√† del provider. Cambiare tra OpenAI, Azure o LM Studio √® questione di cambiare:\n",
    "\n",
    "* Il valore di `model_provider`\n",
    "* Le variabili d‚Äôambiente richieste\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c4e4c-4100-43fa-8ea4-e59770e5c543",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **2. Selezione del modello di Embedding**\n",
    "\n",
    "Gli **embedding** sono rappresentazioni numeriche (vettori) di frasi o documenti. Servono per misurare la **similarit√† semantica** tra testi e sono fondamentali per il recupero nella pipeline RAG.\n",
    "\n",
    "LangChain consente l‚Äôuso di diversi modelli di embedding. In questa sezione vedremo:\n",
    "\n",
    "* `text-embedding-3-large` (OpenAI ‚Äì pi√π recente e accurato)\n",
    "* `text-embedding-ada-002` (OpenAI ‚Äì pi√π economico, legacy)\n",
    "* `MiniLM` (open-source ‚Äì via Hugging Face)\n",
    "\n",
    "Ti mostrer√≤:\n",
    "\n",
    "1. Come configurarli\n",
    "2. Quando e perch√© usare ciascuno\n",
    "3. Differenze chiave tra prestazioni, costi e compatibilit√†\n",
    "\n",
    "---\n",
    "\n",
    "###  **Installazione necessaria**\n",
    "\n",
    "Per i modelli OpenAI:\n",
    "\n",
    "```bash\n",
    "pip install -qU langchain-openai\n",
    "```\n",
    "\n",
    "Per MiniLM (Hugging Face):\n",
    "\n",
    "```bash\n",
    "pip install -qU sentence-transformers langchain-community\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **A. OpenAI ‚Äì `text-embedding-3-large`**\n",
    "\n",
    "> Ultimo modello embedding rilasciato da OpenAI. Supporta testo multilingua, alta accuratezza e compressione.\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Imposta la chiave OpenAI se non presente\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Inserisci la tua OpenAI API Key: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Modello embedding pi√π accurato\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "```\n",
    "\n",
    "**Vantaggi**:\n",
    "\n",
    "* Alta precisione semantica\n",
    "* Supporto multilingua\n",
    "* Comprimibile a 1536 dimensioni\n",
    "* Ottimo per applicazioni critiche (RAG, ricerca documentale)\n",
    "\n",
    "**Svantaggi**:\n",
    "\n",
    "* Costo pi√π alto per token\n",
    "* Richiede connessione alle API di OpenAI\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **B. OpenAI ‚Äì `text-embedding-ada-002`**\n",
    "\n",
    "> Modello precedente, molto utilizzato per il buon rapporto qualit√†/prezzo.\n",
    "\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Modello embedding economico e ancora valido\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "```\n",
    "\n",
    "**Vantaggi**:\n",
    "\n",
    "* Pi√π economico del `3-large`\n",
    "* Buona accuratezza per testi in inglese\n",
    "* Bassa latenza\n",
    "\n",
    "**Svantaggi**:\n",
    "\n",
    "* Peggiore su testi multilingua\n",
    "* Meno efficace su concetti complessi\n",
    "\n",
    "**Quando usarlo**:\n",
    "\n",
    "* Quando si lavora con grandi volumi di dati\n",
    "* Quando il budget √® limitato\n",
    "* Quando si ha bisogno di embedding rapidi ed economici\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **C. Hugging Face ‚Äì `MiniLM` (open-source)**\n",
    "\n",
    "> Alternativa gratuita, locale, utile per prototipazione e ambienti privati.\n",
    "\n",
    "```python\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "```\n",
    "\n",
    "**Vantaggi**:\n",
    "\n",
    "* Gratuito\n",
    "* Esecuzione locale (nessuna API esterna)\n",
    "* Adatto a molti task generici di similarit√† semantica\n",
    "* Pi√π veloce dei modelli di grandi dimensioni\n",
    "\n",
    "**Svantaggi**:\n",
    "\n",
    "* Meno accurato di OpenAI 3-large\n",
    "* Limitato su testi complessi o lunghi\n",
    "* Solo supporto parziale multilingua\n",
    "\n",
    "**Quando usarlo**:\n",
    "\n",
    "* In progetti on-premise o air-gapped\n",
    "* Per test e prototipi\n",
    "* Quando non √® disponibile una connessione a internet o si vuole evitare l‚Äôuso di servizi esterni\n",
    "\n",
    "---\n",
    "\n",
    "##  **Confronto sintetico**\n",
    "\n",
    "| Modello                  | Precisione | Multilingua  | Latenza     | Costo    | Note                           |\n",
    "| ------------------------ | ---------- | ------------ | ----------- | -------- | ------------------------------ |\n",
    "| `text-embedding-3-large` | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | ‚úÖ            | Media       | Alta     | Miglior qualit√† generale       |\n",
    "| `text-embedding-ada-002` | ‚≠ê‚≠ê‚≠ê‚≠ê       | ‚ùå (parziale) | Bassa       | Bassa    | Ottimo rapporto qualit√†/prezzo |\n",
    "| `MiniLM` (`L6-v2`)       | ‚≠ê‚≠ê‚≠ê        | ‚ùå (parziale) | Molto bassa | Gratuito | Ottimo per prototipi locali    |\n",
    "\n",
    "---\n",
    "\n",
    "##  **Conclusioni**\n",
    "\n",
    "* Se vuoi **la massima qualit√†** per un'applicazione RAG in produzione ‚Üí usa `text-embedding-3-large`.\n",
    "* Se devi **ottimizzare i costi** su grandi dataset ‚Üí considera `ada-002`.\n",
    "* Se vuoi **lavorare offline o open-source** ‚Üí MiniLM √® la scelta giusta.\n",
    "\n",
    "LangChain rende semplice cambiare embedding: basta modificare la riga di inizializzazione, mantenendo tutto il resto del sistema invariato.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c406372-be23-414c-88cc-1c296a56327d",
   "metadata": {},
   "source": [
    "### **Approfondimento: Cos'√® un Embedding e perch√© √® fondamentale nei sistemi RAG**\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Definizione di Embedding**\n",
    "\n",
    "Un **embedding** √® una rappresentazione numerica densa di un'informazione testuale (come una parola, una frase o un documento).\n",
    "In pratica, trasforma il linguaggio naturale in **vettori di numeri** che catturano la **semantica** del testo, rendendo possibile confrontare, cercare e analizzare frasi con criteri matematici.\n",
    "\n",
    "Esempio intuitivo:\n",
    "La frase *\"Come stai?\"* e *\"Tutto bene?\"* avranno embedding simili, perch√© esprimono un significato vicino.\n",
    "Al contrario, *\"Apri la porta\"* e *\"Mangia una mela\"* avranno embedding distanti.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Perch√© gli Embedding sono centrali nei sistemi RAG**\n",
    "\n",
    "RAG significa **Retrieval-Augmented Generation**. √à un'architettura che combina:\n",
    "\n",
    "* **retrieval** (recupero di contenuti rilevanti)\n",
    "* **generation** (generazione di risposte da parte di un LLM)\n",
    "\n",
    "Il recupero dei contenuti si basa **quasi sempre su embedding**, ed √® qui che il loro ruolo diventa cruciale.\n",
    "\n",
    "**Come funziona:**\n",
    "\n",
    "1. Tutti i documenti vengono suddivisi in \"chunk\" (pezzi di testo) ‚Üí *text splitting*\n",
    "2. Ogni chunk viene trasformato in un **vettore embedding**\n",
    "3. Quando l‚Äôutente fa una domanda:\n",
    "\n",
    "   * La domanda viene **anch‚Äôessa convertita in embedding**\n",
    "   * Viene calcolata la **distanza vettoriale** tra l‚Äôembedding della domanda e quelli dei documenti\n",
    "   * I chunk pi√π ‚Äúvicini‚Äù vengono recuperati e forniti al modello generativo\n",
    "\n",
    "> Senza embedding, non potremmo confrontare in modo efficiente frasi in linguaggio naturale per similarit√† di significato.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Propriet√† di un buon embedding**\n",
    "\n",
    "Un buon modello di embedding deve:\n",
    "\n",
    "* Catturare **relazioni semantiche**, non solo sintattiche\n",
    "* Supportare **testi multilingua** se necessario\n",
    "* Essere **compatto ma informativo** (vettori densi, es. 384‚Äì1536 dimensioni)\n",
    "* Essere **veloce da calcolare** anche su grandi quantit√† di testo\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Tipologie di modelli di embedding**\n",
    "\n",
    "| Tipo                           | Esempi                                                    | Caratteristiche principali                                |\n",
    "| ------------------------------ | --------------------------------------------------------- | --------------------------------------------------------- |\n",
    "| **Pre-addestrati commerciali** | OpenAI `text-embedding-ada-002`, `text-embedding-3-large` | Alta qualit√†, multilingua, ma richiedono API a pagamento  |\n",
    "| **Modelli open-source**        | MiniLM, BGE, Instructor, GTE                              | Gratuiti, eseguibili in locale, meno accurati in generale |\n",
    "| **Custom**                     | Addestrati su dati aziendali                              | Ottimizzati per il dominio specifico                      |\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Distanza vettoriale e similarit√†**\n",
    "\n",
    "Gli embedding non servono da soli: vanno confrontati con una **metrica di similarit√†**. Le pi√π comuni sono:\n",
    "\n",
    "* **Cosine similarity** ‚Äì misura l‚Äôangolo tra vettori (indipendente dalla lunghezza)\n",
    "* **Distanza L2 (euclidea)** ‚Äì usata ad esempio da FAISS `IndexFlatL2`\n",
    "* **Dot product** ‚Äì usata spesso in modelli neurali\n",
    "\n",
    "Queste metriche permettono di recuperare i chunk ‚Äúpi√π simili‚Äù alla query in modo efficiente, anche su milioni di documenti.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Prestazioni: qualit√† vs velocit√†**\n",
    "\n",
    "Scegliere il giusto embedding dipende dal contesto:\n",
    "\n",
    "| Contesto                                       | Modello consigliato      |\n",
    "| ---------------------------------------------- | ------------------------ |\n",
    "| RAG aziendale di qualit√† su dati in pi√π lingue | `text-embedding-3-large` |\n",
    "| Ricerca veloce su documenti in inglese         | `text-embedding-ada-002` |\n",
    "| Sistema locale, senza costi o cloud            | `MiniLM`, `BGE`, `GTE`   |\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Visualizzazione (opzionale)**\n",
    "\n",
    "√à possibile proiettare gli embedding in 2D o 3D (es. con PCA o UMAP) per visualizzare la distribuzione dei significati nel testo.\n",
    "Questa analisi √® utile per verificare se frasi simili vengono effettivamente mappate vicino tra loro.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Rischi e best practice**\n",
    "\n",
    "* **Chunk troppo lunghi** portano a embedding poco precisi ‚Üí usa text splitter\n",
    "* **Contenuto rumoroso** o ripetitivo pu√≤ alterare i risultati ‚Üí filtra o pulisci prima\n",
    "* **Usa batching** per calcolare pi√π embedding in parallelo e risparmiare tempo\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "Gli embedding sono la base invisibile ma fondamentale di qualsiasi sistema di retrieval semantico.\n",
    "Che si tratti di un motore RAG, un motore di ricerca AI, o un sistema di classificazione testi, il **modo in cui trasformi il testo in numeri** determina tutta la qualit√† della tua pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d0f222-834b-4967-9b33-9870f11cdeb2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **3. Selezione del Vector Store**\n",
    "\n",
    "Il **vector store** √® il componente responsabile dell‚Äôindicizzazione, memorizzazione e ricerca dei **vettori embedding**.\n",
    "√à parte fondamentale in ogni sistema **RAG**, perch√© permette di **recuperare documenti simili semanticamente** a partire dalla query dell‚Äôutente.\n",
    "\n",
    "LangChain supporta numerosi vector store, tra cui:\n",
    "\n",
    "* **FAISS** ‚Äì locale, veloce, open-source\n",
    "* **Qdrant** ‚Äì scalabile, open-source, API-based o locale\n",
    "* **Pinecone** ‚Äì cloud-native, ottimizzato per applicazioni su larga scala\n",
    "\n",
    "---\n",
    "\n",
    "###  **Installazione base**\n",
    "\n",
    "```bash\n",
    "pip install -qU langchain-community\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Esempio 1 ‚Äì FAISS (locale, veloce, open-source)**\n",
    "\n",
    "**FAISS** √® una libreria sviluppata da Facebook AI Research. Ottima per test locali, prototipi e ambienti controllati.\n",
    "\n",
    "### Codice:\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Calcola la dimensione degli embedding\n",
    "embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
    "\n",
    "# Crea un indice FAISS con distanza L2\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Inizializza il vector store\n",
    "vs = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings\n",
    ")\n",
    "```\n",
    "\n",
    "### Quando usarlo:\n",
    "\n",
    "* Ambienti locali\n",
    "* Nessuna dipendenza da servizi esterni\n",
    "* Dataset medio-piccoli\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Esempio 2 ‚Äì Qdrant (cloud o locale, open-source)**\n",
    "\n",
    "**Qdrant** √® un motore vettoriale moderno scritto in Rust, molto efficiente, scalabile e con supporto per **payload metadata**, **filtri**, **search ibrida**.\n",
    "\n",
    "### Installazione:\n",
    "\n",
    "```bash\n",
    "pip install qdrant-client langchain-community\n",
    "```\n",
    "\n",
    "### Codice (con server Qdrant locale su `http://localhost:6333`):\n",
    "\n",
    "```python\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Connessione a Qdrant\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Inizializzazione dello store vettoriale\n",
    "vector_store = Qdrant.from_documents(\n",
    "    documents=[Document(page_content=\"Test\", metadata={\"source\": \"example\"})],\n",
    "    embedding=embeddings,\n",
    "    location=\"http://localhost:6333\",\n",
    "    collection_name=\"rag_example\",\n",
    "    client=client,\n",
    ")\n",
    "```\n",
    "\n",
    "### Quando usarlo:\n",
    "\n",
    "* Hai bisogno di metadati avanzati e filtri\n",
    "* Deployment on-premise o su cloud\n",
    "* Performance e scalabilit√† superiori a FAISS\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Esempio 3 ‚Äì Pinecone (cloud-native, altamente scalabile)**\n",
    "\n",
    "**Pinecone** √® un vector database professionale gestito, ottimo per applicazioni in produzione con milioni di documenti. Offre **alta disponibilit√†, replica, filtraggio, gestione di namespace**, ecc.\n",
    "\n",
    "### Installazione:\n",
    "\n",
    "```bash\n",
    "pip install pinecone-client langchain-community\n",
    "```\n",
    "\n",
    "### Codice:\n",
    "\n",
    "```python\n",
    "import pinecone\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Inizializza Pinecone con API key e ambiente\n",
    "pinecone.init(\n",
    "    api_key=\"YOUR_PINECONE_API_KEY\",\n",
    "    environment=\"us-east1-gcp\"  # esempio, dipende dal tuo account\n",
    ")\n",
    "\n",
    "# Crea l'indice se non esiste\n",
    "index_name = \"rag-demo\"\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(index_name, dimension=len(embeddings.embed_query(\"test\")))\n",
    "\n",
    "# Connessione all‚Äôindice\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# Crea il vector store\n",
    "vector_store = Pinecone.from_documents(\n",
    "    documents=[Document(page_content=\"Questo √® un documento\", metadata={\"source\": \"A\"})],\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name,\n",
    ")\n",
    "```\n",
    "\n",
    "### Quando usarlo:\n",
    "\n",
    "* Applicazioni enterprise\n",
    "* Dataset molto grandi\n",
    "* Hai bisogno di disponibilit√† e replicazione automatica\n",
    "\n",
    "---\n",
    "\n",
    "##  **Confronto tra FAISS, Qdrant e Pinecone**\n",
    "\n",
    "| Caratteristica    | FAISS             | Qdrant                  | Pinecone              |\n",
    "| ----------------- | ----------------- | ----------------------- | --------------------- |\n",
    "| Tipo              | Locale            | Cloud/Locale            | Solo Cloud            |\n",
    "| Performance       | Alta (locale)     | Alta (cloud & locale)   | Altissima (cloud)     |\n",
    "| Filtri avanzati   | ‚ùå                 | ‚úÖ                       | ‚úÖ                     |\n",
    "| Supporto metadati | ‚ùå (limitato)      | ‚úÖ                       | ‚úÖ                     |\n",
    "| Persistenza       | Manuale           | Integrata               | Integrata             |\n",
    "| Costo             | Gratuito (locale) | Gratuito / self-hosting | A pagamento           |\n",
    "| Casistica ideale  | Prototipi         | Produzione flessibile   | Produzione enterprise |\n",
    "\n",
    "---\n",
    "\n",
    "###  **Conclusione**\n",
    "\n",
    "La scelta del vector store dipende dal contesto d‚Äôuso:\n",
    "\n",
    "* **FAISS**: perfetto per test, prototipi, sviluppo locale\n",
    "* **Qdrant**: ideale se vuoi scalabilit√†, open-source e controllo fine\n",
    "* **Pinecone**: soluzione cloud altamente affidabile per produzione su larga scala\n",
    "\n",
    "Tutti i vector store funzionano allo stesso modo: ricevono i vettori dagli **embedding** e li rendono ricercabili per similarit√†.\n",
    "LangChain rende il passaggio da uno all‚Äôaltro molto semplice, mantenendo la stessa interfaccia di utilizzo per l‚Äôintero flusso RAG.\n",
    "\n",
    "## ATTENZIONE\n",
    "Faiss nella sua versione pi√π recente non ha bisogno dell'indice esterni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d0fab-9702-4f42-9160-511ba9d1642b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Approfondimento: utilizzo di FAISS come Vector Store in LangChain\n",
    "\n",
    "### Introduzione\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) √® una libreria open-source sviluppata da Meta per effettuare ricerche vettoriali efficienti su larga scala. Viene utilizzata per confrontare rappresentazioni numeriche (embedding) di documenti al fine di recuperarne quelli semanticamente pi√π simili a una query.\n",
    "\n",
    "In LangChain, FAISS √® uno dei vector store supportati per costruire pipeline di retrieval all'interno di applicazioni RAG (Retrieval-Augmented Generation). Di seguito viene mostrato un esempio pratico completo e una spiegazione di tutti i parametri rilevanti.\n",
    "\n",
    "---\n",
    "\n",
    "### Installazione\n",
    "\n",
    "Per utilizzare FAISS con LangChain e modelli Hugging Face:\n",
    "\n",
    "```bash\n",
    "pip install faiss-cpu langchain-community sentence-transformers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Inizializzazione: embedding Hugging Face e documenti di esempio\n",
    "\n",
    "```python\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Inizializza un modello di embedding gratuito e locale\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Documenti simulati in inglese\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain is a powerful framework for building LLM applications.\"),\n",
    "    Document(page_content=\"FAISS enables fast semantic search over text data.\"),\n",
    "    Document(page_content=\"Python is widely used in AI and machine learning projects.\"),\n",
    "    Document(page_content=\"Vector databases store dense representations of text.\"),\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Costruzione dell‚Äôindice FAISS\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# Calcola la dimensione dei vettori di embedding\n",
    "embedding_dim = len(embeddings.embed_query(\"test\"))\n",
    "\n",
    "def build_faiss_vectorstore(chunks: List[Document], embeddings: HuggingFaceEmbeddings, persist_dir: str) -> FAISS:\n",
    "    \"\"\"\n",
    "    Costruisce da zero un FAISS index (IndexFlatL2) e lo salva su disco.\n",
    "    \"\"\"\n",
    "    # Determina la dimensione dell'embedding\n",
    "    vs = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "\n",
    "    Path(persist_dir).mkdir(parents=True, exist_ok=True)\n",
    "    vs.save_local(persist_dir)\n",
    "    return vs\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Ricerca semantica\n",
    "\n",
    "√à possibile effettuare una ricerca semantica a partire da una query in linguaggio naturale:\n",
    "\n",
    "```python\n",
    "results = vector_store.similarity_search(\"How can I build AI applications?\", k=2)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "```\n",
    "\n",
    "LangChain calcola l'embedding della query, esegue la ricerca nel vector store e restituisce i documenti pi√π vicini semanticamente.\n",
    "\n",
    "---\n",
    "\n",
    "### Salvataggio dell‚Äôindice FAISS su disco\n",
    "\n",
    "Per evitare di ricostruire l‚Äôindice ogni volta, √® possibile salvarlo localmente:\n",
    "\n",
    "```python\n",
    "vector_store.save_local(\"faiss_index_example\")\n",
    "```\n",
    "\n",
    "Questo salver√† due file nella directory `faiss_index_example/`:\n",
    "\n",
    "* `index.faiss`: rappresentazione binaria dell‚Äôindice vettoriale\n",
    "* `index.pkl`: metadati, mappature e documenti\n",
    "\n",
    "---\n",
    "\n",
    "### Caricamento dell‚Äôindice FAISS gi√† costruito\n",
    "\n",
    "In un secondo momento, √® possibile ricaricare l‚Äôindice e riprendere l‚Äôuso del vector store:\n",
    "\n",
    "```python\n",
    "vector_store = FAISS.load_local(\"faiss_index_example\", embeddings)\n",
    "```\n",
    "\n",
    "In questo modo si evita il costo computazionale della rigenerazione degli embedding e dell‚Äôindice.\n",
    "\n",
    "---\n",
    "\n",
    "### Uso come retriever in una pipeline RAG\n",
    "\n",
    "LangChain consente di convertire il vector store in un retriever standard compatibile con le catene di domanda-risposta:\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "```\n",
    "\n",
    "Questo retriever pu√≤ essere usato come componente all‚Äôinterno di una catena `RetrievalQA` o `ConversationalRetrievalChain`.\n",
    "\n",
    "---\n",
    "\n",
    "### Tipologie di indice FAISS\n",
    "\n",
    "| Tipo FAISS      | Descrizione                                                              |\n",
    "| --------------- | ------------------------------------------------------------------------ |\n",
    "| `IndexFlatL2`   | Ricerca esatta con distanza euclidea                                     |\n",
    "| `IndexFlatIP`   | Ricerca esatta con prodotto scalare (dot product)                        |\n",
    "| `IndexIVFFlat`  | Ricerca approssimata basata su clustering (richiede addizionale `train`) |\n",
    "| `IndexHNSWFlat` | Ricerca approssimata tramite grafo navigabile                            |\n",
    "\n",
    "Per progetti semplici o prototipi, `IndexFlatL2` √® sufficiente e non richiede training. Per dataset molto grandi, √® possibile utilizzare indici approssimati come `IndexIVFFlat` o `HNSW`.\n",
    "\n",
    "---\n",
    "\n",
    "### Considerazioni finali\n",
    "\n",
    "FAISS √® una soluzione locale ad alte prestazioni per la ricerca semantica su documenti testuali. In combinazione con LangChain e modelli di embedding gratuiti come MiniLM, consente di costruire pipeline RAG complete senza dover dipendere da API esterne.\n",
    "\n",
    "Per applicazioni scalabili e in produzione, pu√≤ essere utile valutare alternative cloud come Qdrant o Pinecone, ma FAISS resta un‚Äôottima scelta per prototipi, ambienti controllati, e applicazioni offline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6620ca1b-0123-4620-8611-ece99a13bef7",
   "metadata": {},
   "source": [
    "## Embedding Dim\n",
    "```python\n",
    "embedding_dim = len(embeddings.embed_query(\"test\"))\n",
    "```\n",
    "\n",
    "Nel contesto di LangChain e degli embedding, questa istruzione ha una funzione fondamentale: **determina la dimensione (numero di componenti) del vettore embedding prodotto dal modello**. Vediamo perch√© √® importante e come funziona sotto il cofano.\n",
    "\n",
    "---\n",
    "\n",
    "### Cosa fa questa istruzione?\n",
    "\n",
    "* `embeddings.embed_query(\"test\")` restituisce un vettore embedding (una lista di numeri in virgola mobile) per la stringa `\"test\"`.\n",
    "* `len(...)` calcola la **lunghezza di quella lista**, ovvero il numero di dimensioni (componenti) del vettore embedding.\n",
    "\n",
    "Questa dimensione √® essenziale per inizializzare un indice FAISS correttamente, perch√© FAISS richiede di sapere quanti elementi conterr√† ogni vettore per costruire l‚Äôindice.\n",
    "\n",
    "---\n",
    "\n",
    "### Perch√© non esiste un metodo esplicito per ottenerla?\n",
    "\n",
    "LangChain non espone direttamente la dimensione degli embedding generati. In particolare, per l‚Äôimplementazione `HuggingFaceEmbeddings`, non c‚Äô√® un attributo come `embeddings.dimension`, motivo per cui molti sviluppatori utilizzano la soluzione pi√π pratica:\n",
    "\n",
    "> Chiedere direttamente al modello: generare un embedding su una query di prova e misurarne la lunghezza.\n",
    "> ([Stack Overflow][1])\n",
    "\n",
    "---\n",
    "\n",
    "### Dimensioni comuni di embedding Hugging‚ÄØFace\n",
    "\n",
    "Un esempio frequente √® il modello `all-MiniLM-L6-v2` di Sentence Transformers. Questo modello mappa ogni frase in uno spazio vettoriale di **384 dimensioni**.\n",
    "([Hugging Face][2])\n",
    "\n",
    "Ci√≤ significa che l‚Äôistruzione\n",
    "\n",
    "```python\n",
    "len(embeddings.embed_query(\"test\"))\n",
    "```\n",
    "\n",
    "restituir√† 384 quando `embeddings` √® basato su `all-MiniLM-L6-v2`.\n",
    "\n",
    "---\n",
    "\n",
    "### Scenario pratico con FAISS\n",
    "\n",
    "Ecco il flusso completo per utilizzare questa informazione nel creare un indice FAISS:\n",
    "\n",
    "```python\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 1. Embedding open-source gratuito\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Documenti di esempio\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain is great for building intelligent systems.\"),\n",
    "    Document(page_content=\"FAISS enables quick semantic search.\"),\n",
    "]\n",
    "\n",
    "# 3. Calcola la dimensione del vettore embedding\n",
    "embedding_dim = len(embeddings.embed_query(\"test\"))\n",
    "\n",
    "# 4. Costruisci un indice FAISS basato su distanza euclidea (L2)\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# 5. Costruisci il vector store\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "```\n",
    "\n",
    "Cos√¨ il vettore fornito da `embed_query(\"test\")` garantisce che `IndexFlatL2(embedding_dim)` sia inizializzato con la dimensione corretta, evitando errori e garantendo integrit√† nel salvataggio e ricerca dell‚Äôindice.\n",
    "\n",
    "---\n",
    "\n",
    "### In sintesi\n",
    "\n",
    "* **Scopo**: Determinare il numero di dimensioni del vettore embedding generato dal modello.\n",
    "* **Motivo**: LangChain attualmente non esprime questa informazione direttamente, quindi si utilizza un embedding di prova come workaround. \n",
    "* **Esempio pratico**: `all-MiniLM-L6-v2` restituisce embedding da 384 componenti. \n",
    "* **Applicazione**: Questa dimensione √® fondamentale per inizializzare correttamente un indice Faiss, garantendo coerenza nel confronto vettoriale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ceaf70-06b4-4e6c-af59-68a5400e01b5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Similarity Metrics: Cosine, Dot Product, L2\n",
    "\n",
    "Quando si confrontano vettori di embedding, √® necessario misurare **quanto sono simili due vettori nello spazio vettoriale**. Le metriche di similarit√† sono funzioni matematiche che restituiscono una misura numerica di quanto due vettori siano ‚Äúvicini‚Äù tra loro.\n",
    "\n",
    "Le metriche pi√π usate nei sistemi di ricerca semantica sono:\n",
    "\n",
    "* **Cosine Similarity**\n",
    "* **Dot Product (Inner Product)**\n",
    "* **L2 Distance (Euclidean Distance)**\n",
    "\n",
    "Ognuna ha caratteristiche, vantaggi e casi d'uso specifici.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Cosine Similarity\n",
    "\n",
    "#### Definizione\n",
    "\n",
    "La **cosine similarity** misura il **coseno dell‚Äôangolo** tra due vettori. Pi√π piccolo √® l‚Äôangolo, pi√π simili sono i vettori.\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\cdot \\|B\\|}\n",
    "$$\n",
    "\n",
    "Dove:\n",
    "\n",
    "* $A \\cdot B$ √® il prodotto scalare (dot product)\n",
    "* $\\|A\\|$ e $\\|B\\|$ sono le norme (lunghezze) dei vettori\n",
    "\n",
    "#### Valori\n",
    "\n",
    "* $1$: perfettamente simili (stesso verso)\n",
    "* $0$: ortogonali (nessuna similarit√†)\n",
    "* $-1$: direzioni opposte\n",
    "\n",
    "#### Pro e contro\n",
    "\n",
    "| Vantaggi                           | Svantaggi                |\n",
    "| ---------------------------------- | ------------------------ |\n",
    "| Invariante rispetto alla lunghezza | Richiede normalizzazione |\n",
    "| Ottimo per misurare orientamento   | Non cattura magnitudine  |\n",
    "\n",
    "#### Quando usarla\n",
    "\n",
    "* Quando ti interessa **la direzione del significato** piuttosto che la sua intensit√†\n",
    "* Per testi di **lunghezza variabile** e **embedding normalizzati**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Dot Product (Inner Product)\n",
    "\n",
    "#### Definizione\n",
    "\n",
    "Il **dot product** √® una versione semplificata del cosine similarity **senza normalizzazione**. Misura l‚Äôallineamento e la magnitudine dei vettori:\n",
    "\n",
    "$$\n",
    "\\text{dot\\_product}(A, B) = \\sum_i A_i \\cdot B_i\n",
    "$$\n",
    "\n",
    "#### Valori\n",
    "\n",
    "* Pi√π alto √® il valore, pi√π simili sono i vettori\n",
    "\n",
    "#### Pro e contro\n",
    "\n",
    "| Vantaggi                                        | Svantaggi                            |\n",
    "| ----------------------------------------------- | ------------------------------------ |\n",
    "| Veloce da calcolare                             | Sensibile alla lunghezza dei vettori |\n",
    "| Supportato nativamente da FAISS (`IndexFlatIP`) | Pu√≤ penalizzare vettori corti        |\n",
    "\n",
    "#### Quando usarla\n",
    "\n",
    "* Quando gli **embedding non sono normalizzati**\n",
    "* Quando usi modelli dove la magnitudine del vettore ha significato (es. classificazione o ranking)\n",
    "* In contesti **dove vuoi priorit√† su \"importanza\" oltre alla direzione**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. L2 Distance (Euclidean Distance)\n",
    "\n",
    "#### Definizione\n",
    "\n",
    "La **L2 distance** √® la distanza euclidea standard tra due vettori:\n",
    "\n",
    "$$\n",
    "\\text{L2}(A, B) = \\sqrt{\\sum_i (A_i - B_i)^2}\n",
    "$$\n",
    "\n",
    "Pi√π piccolo √® il valore, **pi√π simili** sono i vettori.\n",
    "\n",
    "#### Pro e contro\n",
    "\n",
    "| Vantaggi                                        | Svantaggi                    |\n",
    "| ----------------------------------------------- | ---------------------------- |\n",
    "| Intuitiva e diretta                             | Sensibile alla scala         |\n",
    "| Supportata nativamente in FAISS (`IndexFlatL2`) | Necessita embedding uniformi |\n",
    "\n",
    "#### Quando usarla\n",
    "\n",
    "* Quando vuoi una metrica geometrica diretta\n",
    "* In applicazioni con embedding generati in ambienti controllati e normalizzati\n",
    "* Quando **non normalizzi** i vettori e vuoi valutare distanza ‚Äúfisica‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## Riepilogo comparativo\n",
    "\n",
    "| Metrica           | Range tipico          | Richiede normalizzazione | Tipico uso in FAISS      | Adatta per                |\n",
    "| ----------------- | --------------------- | ------------------------ | ------------------------ | ------------------------- |\n",
    "| Cosine Similarity | -1 a 1                | S√¨                       | Con `IndexFlatIP + norm` | Similarit√† semantica pura |\n",
    "| Dot Product       | $-\\infty$ a $+\\infty$ | No                       | `IndexFlatIP`            | Sistemi di ranking        |\n",
    "| L2 Distance       | $[0, +\\infty)$        | No                       | `IndexFlatL2`            | Misura geometrica         |\n",
    "\n",
    "---\n",
    "\n",
    "## Quale metrica scegliere?\n",
    "\n",
    "### Usa Cosine Similarity se:\n",
    "\n",
    "* Gli embedding sono normalizzati\n",
    "* Vuoi solo misurare **orientamento semantico**\n",
    "* Usi modelli come `sentence-transformers` con `cosine similarity` come obiettivo\n",
    "\n",
    "### Usa Dot Product se:\n",
    "\n",
    "* L‚Äôampiezza del vettore embedding **ha significato**\n",
    "* Usi FAISS con `IndexFlatIP` e non vuoi normalizzare\n",
    "\n",
    "### Usa L2 Distance se:\n",
    "\n",
    "* Vuoi una **metrica geometrica diretta**\n",
    "* Usi `IndexFlatL2` in FAISS\n",
    "* Gli embedding non sono normalizzati ma comparabili\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio pratico con FAISS in LangChain\n",
    "\n",
    "### Cosine similarity con vettori normalizzati\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Normalizza i vettori\n",
    "def normalize(vectors):\n",
    "    return vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "\n",
    "# Usiamo IndexFlatIP ma normalizzando prima\n",
    "vectors = np.array([[1.0, 2.0], [2.0, 3.0]], dtype=\"float32\")\n",
    "vectors = normalize(vectors)\n",
    "\n",
    "index = faiss.IndexFlatIP(2)\n",
    "index.add(vectors)\n",
    "\n",
    "# Anche la query va normalizzata\n",
    "query = normalize(np.array([[1.0, 1.5]], dtype=\"float32\"))\n",
    "D, I = index.search(query, k=1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### L2 distance (ricerca esatta, non normalizzata)\n",
    "\n",
    "```python\n",
    "index = faiss.IndexFlatL2(2)\n",
    "index.add(np.array([[1.0, 2.0], [2.0, 3.0]], dtype=\"float32\"))\n",
    "query = np.array([[1.0, 1.5]], dtype=\"float32\")\n",
    "D, I = index.search(query, k=1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusione\n",
    "\n",
    "La metrica di similarit√† √® un **elemento critico in un sistema RAG o di ricerca semantica**, perch√© influenza direttamente i risultati restituiti. Scegliere la metrica giusta dipende:\n",
    "\n",
    "* dal tipo di modello di embedding utilizzato\n",
    "* da come gli embedding sono normalizzati o strutturati\n",
    "* dal comportamento desiderato (priorit√† alla direzione semantica, magnitudine, o distanza)\n",
    "\n",
    "LangChain e FAISS forniscono gli strumenti per lavorare con tutte queste metriche in modo flessibile e integrato.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a4d36-1d3e-4528-a682-feee0920381b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Hybrid Search: BM25 + Vector Re-ranking**\n",
    "\n",
    "### **Cos'√® la Hybrid Search?**\n",
    "\n",
    "La **Hybrid Search** (ricerca ibrida) √® una tecnica che **combina due approcci diversi** per migliorare la qualit√† del recupero dei documenti in risposta a una query:\n",
    "\n",
    "1. **BM25** (ricerca testuale classica, keyword-based)\n",
    "2. **Vector Search** (ricerca semantica tramite embedding)\n",
    "\n",
    "L‚Äôobiettivo √® **sfruttare i punti di forza di entrambi**:\n",
    "\n",
    "| Approccio         | Vantaggi principali                           | Limiti principali                                             |\n",
    "| ----------------- | --------------------------------------------- | ------------------------------------------------------------- |\n",
    "| **BM25**          | Ottimo per match di parole esatte, efficiente | Non comprende il significato semantico                        |\n",
    "| **Vector Search** | Capisce sinonimi e concetti simili            | Pu√≤ restituire risultati semanticamente vicini ma irrilevanti |\n",
    "\n",
    "La ricerca ibrida consente di **aumentare precisione e recall**, specialmente in contesti dove √® importante sia la **pertinenza semantica**, sia il **match testuale**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Come funziona una pipeline BM25 + Vector Re-ranking**\n",
    "\n",
    "1. **BM25** recupera un set iniziale di documenti **basato su corrispondenze di parole chiave**.\n",
    "2. Questi documenti vengono **embeddingizzati**.\n",
    "3. La query viene trasformata in un embedding vettoriale.\n",
    "4. Si calcola la **similarit√† vettoriale** tra la query e ciascun documento del set iniziale.\n",
    "5. I risultati vengono **riordinati (re-ranked)** in base alla similarit√† semantica.\n",
    "\n",
    "In alternativa:\n",
    "\n",
    "* si pu√≤ fare **merge score-based**: combinando punteggi BM25 e vettoriali con pesi ($\\alpha \\cdot \\text{bm25\\_score} + (1 - \\alpha) \\cdot \\text{vector\\_similarity}$)\n",
    "\n",
    "---\n",
    "\n",
    "## **Cos'√® Qdrant e perch√© √® adatto alla ricerca ibrida**\n",
    "\n",
    "**Qdrant** √® un **motore di ricerca vettoriale open-source**, moderno, scritto in Rust, pensato per la ricerca scalabile e semantica.\n",
    "A differenza di FAISS, Qdrant supporta **filtri**, **payload JSON**, **metadati** e **ricerca ibrida** nativamente.\n",
    "\n",
    "### Caratteristiche principali di Qdrant:\n",
    "\n",
    "* Supporto nativo per **BM25**\n",
    "* Supporto nativo per **Vector search**\n",
    "* Supporto per **filtri combinati**, **payload personalizzati**, **tag**\n",
    "* API REST o gRPC + supporto per client Python (`qdrant-client`)\n",
    "* Persistenza automatica e clustering\n",
    "\n",
    "---\n",
    "\n",
    "## **Installazione**\n",
    "\n",
    "### Server Qdrant (opzioni):\n",
    "\n",
    "* Via Docker:\n",
    "\n",
    "```bash\n",
    "docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant\n",
    "```\n",
    "\n",
    "### Python client:\n",
    "\n",
    "```bash\n",
    "pip install qdrant-client langchain-community\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Esempio pratico: Hybrid Search con Qdrant**\n",
    "\n",
    "### 1. Inizializzazione embedding e client\n",
    "\n",
    "```python\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Embedding model open-source\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Connessione al server Qdrant locale\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Creazione della collection (con supporto hybrid)\n",
    "\n",
    "```python\n",
    "# Dimensione dell'embedding\n",
    "dim = len(embedding_model.embed_query(\"example\"))\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=\"hybrid_demo\",\n",
    "    vectors_config={\"default\": {\"size\": dim, \"distance\": \"Cosine\"}},\n",
    "    optimizers_config={\"default_segment_number\": 1},\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Inserimento dei documenti\n",
    "\n",
    "```python\n",
    "from langchain.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain is a powerful framework for building LLM applications.\", metadata={\"id\": \"doc1\"}),\n",
    "    Document(page_content=\"FAISS enables fast semantic search over text data.\", metadata={\"id\": \"doc2\"}),\n",
    "    Document(page_content=\"Python is widely used in AI projects.\", metadata={\"id\": \"doc3\"}),\n",
    "]\n",
    "\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "vector_store = Qdrant.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"hybrid_demo\",\n",
    "    client=client\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Esecuzione di una Hybrid Search\n",
    "\n",
    "```python\n",
    "query = \"How can I build AI applications?\"\n",
    "\n",
    "# Metodo 1: solo vector search\n",
    "results_vector = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "# Metodo 2: hybrid search con BM25 + vector\n",
    "results_hybrid = client.search(\n",
    "    collection_name=\"hybrid_demo\",\n",
    "    query_vector=embedding_model.embed_query(query),\n",
    "    with_payload=True,\n",
    "    limit=3,\n",
    "    with_vectors=False,\n",
    "    score_threshold=None,\n",
    "    params={\n",
    "        \"exact\": False,\n",
    "        \"hybrid\": {\n",
    "            \"alpha\": 0.5,       # bilanciamento: 0 = solo BM25, 1 = solo vettore\n",
    "            \"vector\": True,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "for hit in results_hybrid:\n",
    "    print(hit.payload)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Vantaggi della ricerca ibrida con Qdrant**\n",
    "\n",
    "1. **Precisione migliorata** ‚Äì Recupera documenti con match testuale *e* significato simile.\n",
    "2. **Supporto nativo** ‚Äì Nessuna logica custom: BM25 + embedding sono integrati.\n",
    "3. **Filtri avanzati** ‚Äì Puoi filtrare per metadati (es. categoria, data, autore).\n",
    "4. **Persistenza automatica** ‚Äì Non c'√® bisogno di gestire manualmente salvataggi.\n",
    "\n",
    "---\n",
    "\n",
    "## **Quando usare la Hybrid Search**\n",
    "\n",
    "* Quando i documenti contengono **termini tecnici precisi** ma anche sinonimi e varianti linguistiche.\n",
    "* Quando vuoi **evitare risultati semanticamente simili ma fuori tema**.\n",
    "* In domini come **ambiti legali, medici, documentazione tecnica**, dove sia il significato che la terminologia contano.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusione**\n",
    "\n",
    "La Hybrid Search combina il meglio dei due mondi: la precisione delle keyword (BM25) e la flessibilit√† semantica degli embedding.\n",
    "**Qdrant** √® uno dei pochi vector store a supportare questa modalit√† **nativamente**, rendendolo una scelta eccellente per applicazioni RAG moderne, scalabili e sensibili al contesto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211d4a1-efa4-4af2-a8c4-0721114fe57f",
   "metadata": {},
   "source": [
    "## progetto RAG completo\n",
    "\n",
    "* **Documenti simulati** (in inglese)\n",
    "* **FAISS** come vector store (senza hybrid search)\n",
    "* **Embedding open-source gratuito** di Hugging Face (`all-MiniLM-L6-v2`)\n",
    "* **LM Studio** come LLM per rispondere alle domande (server OpenAI-compatible)\n",
    "\n",
    "Il codice include le ottimizzazioni: text splitting accurato, FAISS persistente (save/load per evitare rebuild), retriever con **MMR** per diversificazione dei risultati, prompt con **citazioni** alle fonti e struttura modulare ‚Äúproduction-ready‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "## Requisiti\n",
    "\n",
    "```bash\n",
    "pip install -qU faiss-cpu langchain langchain-community sentence-transformers\n",
    "```\n",
    "\n",
    "> Assicurati che **LM Studio** sia in esecuzione e stia esponendo un endpoint OpenAI-compatible (default: `http://localhost:1234/v1`).\n",
    "> Carica un modello in LM Studio (es. Mistral, Llama) e annotane il nome visualizzato.\n",
    "\n",
    "---\n",
    "\n",
    "## Variabili d‚Äôambiente (LM Studio)\n",
    "\n",
    "Nel tuo terminale (oppure impostale a runtime nel codice):\n",
    "\n",
    "```bash\n",
    "# LM Studio espone un endpoint OpenAI-compatible\n",
    "export OPENAI_BASE_URL=\"http://localhost:1234/v1\"\n",
    "export OPENAI_API_KEY=\"not-needed\"   # richiesto dall'SDK ma non usato\n",
    "export LMSTUDIO_MODEL=\"mistral\"      # usa il nome del modello caricato in LM Studio\n",
    "```\n",
    "\n",
    "Su Windows (PowerShell):\n",
    "\n",
    "```powershell\n",
    "$env:OPENAI_BASE_URL=\"http://localhost:1234/v1\"\n",
    "$env:OPENAI_API_KEY=\"not-needed\"\n",
    "$env:LMSTUDIO_MODEL=\"mistral\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Codice completo\n",
    "\n",
    "> Salvalo come `rag_faiss_lmstudio.py` e avvialo.\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import faiss\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LangChain Core (prompt/chain)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Chat model init (provider-agnostic, qui puntiamo a LM Studio via OpenAI-compatible)\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Configurazione\n",
    "# =========================\n",
    "\n",
    "@dataclass\n",
    "class Settings:\n",
    "    # Persistenza FAISS\n",
    "    persist_dir: str = \"faiss_index_example\"\n",
    "    # Text splitting\n",
    "    chunk_size: int = 700\n",
    "    chunk_overlap: int = 100\n",
    "    # Retriever (MMR)\n",
    "    search_type: str = \"mmr\"        # \"mmr\" o \"similarity\"\n",
    "    k: int = 4                      # risultati finali\n",
    "    fetch_k: int = 20               # candidati iniziali (per MMR)\n",
    "    mmr_lambda: float = 0.3         # 0 = diversificazione massima, 1 = pertinenza massima\n",
    "    # Embedding\n",
    "    hf_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    # LM Studio (OpenAI-compatible)\n",
    "    lmstudio_model_env: str = \"LMSTUDIO_MODEL\"  # nome del modello in LM Studio, via env var\n",
    "\n",
    "\n",
    "SETTINGS = Settings()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Componenti di base\n",
    "# =========================\n",
    "\n",
    "def get_embeddings(settings: Settings) -> HuggingFaceEmbeddings:\n",
    "    \"\"\"\n",
    "    Restituisce un modello di embedding locale e gratuito (Hugging Face).\n",
    "    \"\"\"\n",
    "    return HuggingFaceEmbeddings(model_name=settings.hf_model_name)\n",
    "\n",
    "\n",
    "def get_llm_from_lmstudio(settings: Settings):\n",
    "    \"\"\"\n",
    "    Inizializza un ChatModel puntando a LM Studio (OpenAI-compatible).\n",
    "    Richiede:\n",
    "      - OPENAI_BASE_URL (es. http://localhost:1234/v1)\n",
    "      - OPENAI_API_KEY (placeholder qualsiasi, es. \"not-needed\")\n",
    "      - LMSTUDIO_MODEL (nome del modello caricato in LM Studio)\n",
    "    \"\"\"\n",
    "    base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    model_name = os.getenv(settings.lmstudio_model_env)\n",
    "\n",
    "    if not base_url or not api_key:\n",
    "        raise RuntimeError(\n",
    "            \"OPENAI_BASE_URL e OPENAI_API_KEY devono essere impostate per LM Studio.\"\n",
    "        )\n",
    "    if not model_name:\n",
    "        raise RuntimeError(\n",
    "            f\"Imposta la variabile {settings.lmstudio_model_env} con il nome del modello caricato in LM Studio.\"\n",
    "        )\n",
    "\n",
    "    # model_provider=\"openai\" perch√© l'endpoint √® OpenAI-compatible\n",
    "    return init_chat_model(model_name, model_provider=\"openai\")\n",
    "\n",
    "\n",
    "def simulate_corpus() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Crea un piccolo corpus di documenti in inglese con metadati e 'source' per citazioni.\n",
    "    \"\"\"\n",
    "    docs = [\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"LangChain is a framework that helps developers build applications \"\n",
    "                \"powered by Large Language Models (LLMs). It provides chains, agents, \"\n",
    "                \"prompt templates, memory, and integrations with vector stores.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc1\", \"source\": \"intro-langchain.md\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"FAISS is a library for efficient similarity search and clustering of dense vectors. \"\n",
    "                \"It supports exact and approximate nearest neighbor search and scales to millions of vectors.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc2\", \"source\": \"faiss-overview.md\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"Sentence-transformers like all-MiniLM-L6-v2 produce sentence embeddings suitable \"\n",
    "                \"for semantic search, clustering, and information retrieval. The embedding size is 384.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc3\", \"source\": \"embeddings-minilm.md\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"A typical RAG pipeline includes indexing (load, split, embed, store) and \"\n",
    "                \"retrieval+generation. Retrieval selects the most relevant chunks, and the LLM produces \"\n",
    "                \"an answer grounded in those chunks.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc4\", \"source\": \"rag-pipeline.md\"}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=(\n",
    "                \"Maximal Marginal Relevance (MMR) balances relevance and diversity during retrieval. \"\n",
    "                \"It helps avoid redundant chunks and improves coverage of different aspects.\"\n",
    "            ),\n",
    "            metadata={\"id\": \"doc5\", \"source\": \"retrieval-mmr.md\"}\n",
    "        ),\n",
    "    ]\n",
    "    return docs\n",
    "\n",
    "\n",
    "def split_documents(docs: List[Document], settings: Settings) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Applica uno splitting robusto ai documenti per ottimizzare il retrieval.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=settings.chunk_size,\n",
    "        chunk_overlap=settings.chunk_overlap,\n",
    "        separators=[\n",
    "            \"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \"; \", \": \",\n",
    "            \", \", \" \", \"\"  # fallback aggressivo\n",
    "        ],\n",
    "    )\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "def build_faiss_vectorstore(chunks: List[Document], embeddings: HuggingFaceEmbeddings, persist_dir: str) -> FAISS:\n",
    "    \"\"\"\n",
    "    Costruisce da zero un FAISS index (IndexFlatL2) e lo salva su disco.\n",
    "    \"\"\"\n",
    "    # Determina la dimensione dell'embedding\n",
    "    vs = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "\n",
    "    Path(persist_dir).mkdir(parents=True, exist_ok=True)\n",
    "    vs.save_local(persist_dir)\n",
    "    return vs\n",
    "\n",
    "\n",
    "def load_or_build_vectorstore(settings: Settings, embeddings: HuggingFaceEmbeddings, docs: List[Document]) -> FAISS:\n",
    "    \"\"\"\n",
    "    Tenta il load di un indice FAISS persistente; se non esiste, lo costruisce e lo salva.\n",
    "    \"\"\"\n",
    "    persist_path = Path(settings.persist_dir)\n",
    "    index_file = persist_path / \"index.faiss\"\n",
    "    meta_file = persist_path / \"index.pkl\"\n",
    "\n",
    "    if index_file.exists() and meta_file.exists():\n",
    "        # Dal 2024/2025 molte build richiedono il flag 'allow_dangerous_deserialization' per caricare pkl locali\n",
    "        return FAISS.load_local(\n",
    "            settings.persist_dir,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "    chunks = split_documents(docs, settings)\n",
    "    return build_faiss_vectorstore(chunks, embeddings, settings.persist_dir)\n",
    "\n",
    "\n",
    "def make_retriever(vector_store: FAISS, settings: Settings):\n",
    "    \"\"\"\n",
    "    Configura il retriever. Con 'mmr' otteniamo risultati meno ridondanti e pi√π coprenti.\n",
    "    \"\"\"\n",
    "    if settings.search_type == \"mmr\":\n",
    "        return vector_store.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": settings.k, \"fetch_k\": settings.fetch_k, \"lambda_mult\": settings.mmr_lambda},\n",
    "        )\n",
    "    else:\n",
    "        return vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": settings.k},\n",
    "        )\n",
    "\n",
    "\n",
    "def format_docs_for_prompt(docs: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Prepara il contesto per il prompt, includendo citazioni [source].\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        src = d.metadata.get(\"source\", f\"doc{i}\")\n",
    "        lines.append(f\"[source:{src}] {d.page_content}\")\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "\n",
    "def build_rag_chain(llm, retriever):\n",
    "    \"\"\"\n",
    "    Costruisce la catena RAG (retrieval -> prompt -> LLM) con citazioni e regole anti-hallucination.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"Sei un assistente esperto. Rispondi in italiano. \"\n",
    "        \"Usa esclusivamente il CONTENUTO fornito nel contesto. \"\n",
    "        \"Se l'informazione non √® presente, dichiara che non √® disponibile. \"\n",
    "        \"Includi citazioni tra parentesi quadre nel formato [source:...]. \"\n",
    "        \"Sii conciso, accurato e tecnicamente corretto.\"\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\",\n",
    "         \"Domanda:\\n{question}\\n\\n\"\n",
    "         \"Contesto (estratti selezionati):\\n{context}\\n\\n\"\n",
    "         \"Istruzioni:\\n\"\n",
    "         \"1) Rispondi solo con informazioni contenute nel contesto.\\n\"\n",
    "         \"2) Cita sempre le fonti pertinenti nel formato [source:FILE].\\n\"\n",
    "         \"3) Se la risposta non √® nel contesto, scrivi: 'Non √® presente nel contesto fornito.'\")\n",
    "    ])\n",
    "\n",
    "    # LCEL: dict -> prompt -> llm -> parser\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs_for_prompt,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "def rag_answer(question: str, chain) -> str:\n",
    "    \"\"\"\n",
    "    Esegue la catena RAG per una singola domanda.\n",
    "    \"\"\"\n",
    "    return chain.invoke(question)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Esecuzione dimostrativa\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    settings = SETTINGS\n",
    "\n",
    "    # 1) Componenti\n",
    "    embeddings = get_embeddings(settings)\n",
    "    llm = get_llm_from_lmstudio(settings)\n",
    "\n",
    "    # 2) Dati simulati e indicizzazione (load or build)\n",
    "    docs = simulate_corpus()\n",
    "    vector_store = load_or_build_vectorstore(settings, embeddings, docs)\n",
    "\n",
    "    # 3) Retriever ottimizzato\n",
    "    retriever = make_retriever(vector_store, settings)\n",
    "\n",
    "    # 4) Catena RAG\n",
    "    chain = build_rag_chain(llm, retriever)\n",
    "\n",
    "    # 5) Esempi di domande\n",
    "    questions = [\n",
    "        \"Che cos'√® una pipeline RAG e quali sono le sue fasi principali?\",\n",
    "        \"A cosa serve FAISS e quali capacit√† offre?\",\n",
    "        \"Cos'√® MMR e perch√© √® utile durante il retrieval?\",\n",
    "        \"Quale dimensione hanno gli embedding prodotti da all-MiniLM-L6-v2?\"\n",
    "    ]\n",
    "\n",
    "    for q in questions:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Q:\", q)\n",
    "        print(\"-\" * 80)\n",
    "        ans = rag_answer(q, chain)\n",
    "        print(ans)\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Note progettuali e scelte tecniche\n",
    "\n",
    "1. **Embedding locale e gratuito**\n",
    "   Usa `sentence-transformers/all-MiniLM-L6-v2` (dimensione 384). √à veloce, senza costi e adatto a prototipi e piccoli sistemi RAG.\n",
    "\n",
    "2. **Text splitting**\n",
    "   `RecursiveCharacterTextSplitter` con `chunk_size=700` e `chunk_overlap=100` bilancia copertura e coerenza. Puoi regolare questi parametri se i tuoi documenti sono molto densi o molto eterogenei.\n",
    "\n",
    "3. **FAISS persistente**\n",
    "   `save_local()` e `load_local()` evitano di ricalcolare gli embedding e ricostruire l‚Äôindice ad ogni esecuzione. √à importante per tempi di avvio rapidi in ambienti reali.\n",
    "\n",
    "4. **Retriever MMR**\n",
    "   `search_type=\"mmr\"` con `fetch_k` e `lambda_mult` aiuta a ridurre la ridondanza e a coprire aspetti diversi dei documenti. Se preferisci i ‚Äútop-k‚Äù pi√π densi, passa a `search_type=\"similarity\"`.\n",
    "\n",
    "5. **LM Studio via endpoint OpenAI-compatible**\n",
    "   `init_chat_model(..., model_provider=\"openai\")` punta a `OPENAI_BASE_URL` con `OPENAI_API_KEY` di comodo. Imposta `LMSTUDIO_MODEL` al nome del modello caricato in LM Studio.\n",
    "\n",
    "6. **Citations**\n",
    "   Le fonti sono propagate nei metadati `source` dei `Document`. La funzione `format_docs_for_prompt` costruisce un contesto con citazioni `[source:<file>]`.\n",
    "\n",
    "7. **Estensioni possibili**\n",
    "\n",
    "   * Contextual compression (LLM chain extractor) per ridurre i chunk prima del prompt.\n",
    "   * Filtri per metadati in fase di retrieval.\n",
    "   * Valutazioni offline con LangSmith (tracing, debugging) se sposti il backend a un provider remoto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7adfe-ce30-4473-95ad-1abf4a1274e1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1. **Search Type** (tipi di ricerca nel retriever)\n",
    "\n",
    "Quando usi un retriever (ad esempio con FAISS o Qdrant) hai diversi modi di recuperare i documenti in risposta a una query. I pi√π comuni in LangChain sono:\n",
    "\n",
    "### a) **Similarity Search (default)**\n",
    "\n",
    "* Recupera i documenti **pi√π vicini alla query** nello spazio vettoriale.\n",
    "* Funziona ordinando i risultati per distanza (o similarit√†) e prendendo i primi *k*.\n",
    "* √à semplice, veloce e di solito sufficiente.\n",
    "* Problema: pu√≤ restituire chunk molto simili tra loro (ridondanza).\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # prendi i 4 pi√π simili\n",
    ")\n",
    "```\n",
    "\n",
    "### b) **MMR (Maximal Marginal Relevance)**\n",
    "\n",
    "* Non prende solo i *pi√π simili*, ma **bilancia somiglianza e diversit√†**.\n",
    "* Funziona cos√¨:\n",
    "\n",
    "  1. Trova un insieme pi√π ampio di candidati (*fetch\\_k*, es. 20)\n",
    "  2. Seleziona i primi risultati bilanciando:\n",
    "\n",
    "     * **Pertinenza** rispetto alla query\n",
    "     * **Diversit√†** rispetto ai documenti gi√† scelti\n",
    "* Parametro chiave: **Œª (lambda\\_mult)**\n",
    "\n",
    "  * 0 = privilegia solo la diversit√†\n",
    "  * 1 = privilegia solo la pertinenza\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 4, \"fetch_k\": 20, \"lambda_mult\": 0.3}\n",
    ")\n",
    "```\n",
    "\n",
    "* Utile quando:\n",
    "\n",
    "  * Vuoi **evitare ridondanza**\n",
    "  * Hai documenti che trattano aspetti diversi della query\n",
    "  * Vuoi aumentare **copertura informativa**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Chunking e Overlap**\n",
    "\n",
    "Quando hai documenti grandi (manuali, articoli, report), non puoi mandarli interamente all‚ÄôLLM (per limiti di contesto). Si usano i **Text Splitter** per dividerli in porzioni pi√π piccole (**chunk**).\n",
    "\n",
    "### a) **Chunk Size**\n",
    "\n",
    "* √à la lunghezza massima di un pezzo di documento (in caratteri o token).\n",
    "* Scelta critica:\n",
    "\n",
    "  * Troppo piccolo ‚Üí perdita di contesto, pezzi non informativi\n",
    "  * Troppo grande ‚Üí rischi di superare il limite del modello e di avere embedding poco precisi\n",
    "\n",
    "Esempio:\n",
    "\n",
    "```python\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,   # ~700 caratteri per chunk\n",
    "    chunk_overlap=100\n",
    ")\n",
    "```\n",
    "\n",
    "Qui ogni chunk avr√† circa **700 caratteri** (con punteggiatura come separatori preferiti).\n",
    "\n",
    "### b) **Chunk Overlap**\n",
    "\n",
    "* √à la **sovrapposizione** tra un chunk e il successivo (in caratteri).\n",
    "* Serve per **non perdere il contesto alle frontiere dei chunk**.\n",
    "* Esempio: se un paragrafo √® lungo 710 caratteri e il chunk size √® 700 senza overlap, rischi di ‚Äútagliare‚Äù frasi a met√†.\n",
    "* Con overlap=100:\n",
    "\n",
    "  * Il chunk1 contiene 0‚Äì700\n",
    "  * Il chunk2 contiene 600‚Äì1300\n",
    "  * Le frasi tra 600‚Äì700 appaiono in entrambi, preservando il contesto.\n",
    "\n",
    "### c) Linee guida pratiche\n",
    "\n",
    "* **Chunk size**:\n",
    "\n",
    "  * 500‚Äì1000 caratteri se usi modelli piccoli (es. MiniLM)\n",
    "  * 1000‚Äì2000 caratteri per GPT-4/Claude con contesti ampi\n",
    "\n",
    "* **Overlap**:\n",
    "\n",
    "  * 10‚Äì20% della lunghezza del chunk\n",
    "  * Tipicamente 100‚Äì200 caratteri\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio pratico combinato\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Splitting robusto con chunking e overlap\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \"; \", \": \", \", \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Retriever con MMR per ridurre ridondanza\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.3}\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusione\n",
    "\n",
    "* **Search Type**:\n",
    "\n",
    "  * `similarity`: prende i pi√π simili ‚Üí veloce, ma rischia ridondanza\n",
    "  * `mmr`: bilancia pertinenza e diversit√† ‚Üí pi√π copertura, meno ridondanza\n",
    "\n",
    "* **Chunking & Overlap**:\n",
    "\n",
    "  * Chunk size determina la granularit√† delle unit√† di ricerca\n",
    "  * Overlap evita di perdere il contesto tra due chunk adiacenti\n",
    "\n",
    "Entrambi sono strumenti cruciali per garantire che un sistema RAG sia **accurato, completo e robusto**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd0b17-c88d-46e8-a206-2f84e40e3f4a",
   "metadata": {},
   "source": [
    "## Documenti reali\n",
    "\n",
    "Per utilizzare **documenti reali** invece di testi simulati nella tua pipeline RAG, devi costruire una funzione che **carichi file da disco**, li legga, e li converta in oggetti `Document` compatibili con LangChain.\n",
    "\n",
    "Vediamo come fare in modo semplice, ordinato e professionale.\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivo della funzione\n",
    "\n",
    "Sostituire questa funzione:\n",
    "\n",
    "```python\n",
    "def simulate_corpus() -> List[Document]:\n",
    "    ...\n",
    "```\n",
    "\n",
    "con una versione reale che carichi i contenuti da una cartella locale contenente file `.txt`, `.md`, `.pdf`, o altri formati supportati da LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "## Passaggi da seguire\n",
    "\n",
    "1. Leggere i file reali da una directory locale.\n",
    "2. Usare i **Document Loaders** di LangChain per convertirli in `Document`.\n",
    "3. Impostare metadati utili, come il nome del file (per le citazioni).\n",
    "4. Restituire una `List[Document]` identica come formato a quella della funzione simulata.\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio: caricamento da file `.txt` e `.md`\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def load_real_documents_from_folder(folder_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Carica documenti reali da file di testo (es. .txt, .md) all'interno di una cartella.\n",
    "    Ogni file viene letto e convertito in un oggetto Document con metadato 'source'.\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    documents: List[Document] = []\n",
    "\n",
    "    if not folder.exists() or not folder.is_dir():\n",
    "        raise ValueError(f\"La cartella '{folder_path}' non esiste o non √® una directory.\")\n",
    "\n",
    "    for file_path in folder.glob(\"**/*\"):\n",
    "        if file_path.suffix.lower() not in [\".txt\", \".md\"]:\n",
    "            continue  # ignora file non supportati\n",
    "\n",
    "        loader = TextLoader(str(file_path), encoding=\"utf-8\")\n",
    "        docs = loader.load()\n",
    "\n",
    "        # Aggiunge il metadato 'source' per citazioni (es. nome del file)\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = file_path.name\n",
    "\n",
    "        documents.extend(docs)\n",
    "\n",
    "    return documents\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Utilizzo\n",
    "\n",
    "Nel tuo codice principale, sostituisci:\n",
    "\n",
    "```python\n",
    "docs = simulate_corpus()\n",
    "```\n",
    "\n",
    "con:\n",
    "\n",
    "```python\n",
    "docs = load_real_documents_from_folder(\"path/alla/cartella/documenti\")\n",
    "```\n",
    "\n",
    "Assicurati che la cartella contenga file `.txt` o `.md` leggibili, come:\n",
    "\n",
    "```\n",
    "/documenti/\n",
    "‚îú‚îÄ‚îÄ langchain_overview.md\n",
    "‚îú‚îÄ‚îÄ faiss_notes.txt\n",
    "‚îú‚îÄ‚îÄ embeddings_info.md\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Supporto per altri formati (PDF, HTML, CSV, ecc.)\n",
    "\n",
    "LangChain supporta altri loader:\n",
    "\n",
    "* `PyPDFLoader` per `.pdf`\n",
    "* `UnstructuredFileLoader` per `.docx`, `.pptx`, `.eml`\n",
    "* `BSHTMLLoader` per file HTML\n",
    "* `CSVLoader` per `.csv`\n",
    "\n",
    "Puoi combinarli usando controlli come:\n",
    "\n",
    "```python\n",
    "if file_path.suffix == \".pdf\":\n",
    "    loader = PyPDFLoader(str(file_path))\n",
    "elif file_path.suffix == \".html\":\n",
    "    loader = BSHTMLLoader(str(file_path))\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Considerazioni aggiuntive\n",
    "\n",
    "* I file troppo lunghi verranno poi **spezzati in chunk** tramite lo `TextSplitter`, come gi√† previsto nel tuo codice.\n",
    "* Il metadato `\"source\"` √® importante per le **citazioni automatiche** nei prompt RAG.\n",
    "* √à buona norma assicurarsi che tutti i documenti siano in UTF-8 ed evitare file binari o malformati.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adff0f4-ce0b-4216-bb08-1290b1ea9238",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Come funziona la `build_rag_chain`\n",
    "\n",
    "```python\n",
    "def build_rag_chain(llm, retriever):\n",
    "```\n",
    "\n",
    "Questa funzione crea una **catena LCEL (LangChain Expression Language)** composta da 4 step:\n",
    "\n",
    "1. **Retriever + formatter** ‚Üí recupera i documenti e li trasforma in stringa.\n",
    "2. **PromptTemplate** ‚Üí costruisce il prompt per l‚ÄôLLM con una struttura coerente.\n",
    "3. **LLM** ‚Üí genera la risposta.\n",
    "4. **OutputParser** ‚Üí estrae solo il testo generato.\n",
    "\n",
    "---\n",
    "\n",
    "##  Dettaglio: `system_prompt` e `ChatPromptTemplate`\n",
    "\n",
    "### 1. `system_prompt`\n",
    "\n",
    "Il system prompt serve a dare istruzioni **costanti** al modello, come se fosse la sua \"personalit√†\":\n",
    "\n",
    "```python\n",
    "system_prompt = (\n",
    "    \"Sei un assistente esperto. Rispondi in italiano. \"\n",
    "    \"Usa esclusivamente il CONTENUTO fornito nel contesto. \"\n",
    "    \"Se l'informazione non √® presente, dichiara che non √® disponibile. \"\n",
    "    \"Includi citazioni tra parentesi quadre nel formato [source:...]. \"\n",
    "    \"Sii conciso, accurato e tecnicamente corretto.\"\n",
    ")\n",
    "```\n",
    "\n",
    "Questo √® **statico**, non cambia a ogni input.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `ChatPromptTemplate` con variabili\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\",\n",
    "     \"Domanda:\\n{question}\\n\\n\"\n",
    "     \"Contesto (estratti selezionati):\\n{context}\\n\\n\"\n",
    "     \"Istruzioni:\\n\"\n",
    "     \"1) Rispondi solo con informazioni contenute nel contesto.\\n\"\n",
    "     \"2) Cita sempre le fonti pertinenti nel formato [source:FILE].\\n\"\n",
    "     \"3) Se la risposta non √® nel contesto, scrivi: 'Non √® presente nel contesto fornito.'\")\n",
    "])\n",
    "```\n",
    "\n",
    "Qui usi **due messaggi**:\n",
    "\n",
    "* `(\"system\", ...)`: per il comportamento fisso del modello.\n",
    "* `(\"human\", ...)`: per l‚Äôinput dell‚Äôutente, con **variabili** `{question}` e `{context}` che verranno riempite a runtime.\n",
    "\n",
    "---\n",
    "\n",
    "##  Cos‚Äô√® la `chain` e il passaggio dei dati\n",
    "\n",
    "```python\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs_for_prompt,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "```\n",
    "\n",
    "### Step-by-step:\n",
    "\n",
    "1. `question`: passa direttamente il testo con `RunnablePassthrough()`.\n",
    "2. `context`: passa per il retriever e una funzione che formatta i documenti.\n",
    "3. `prompt`: usa `ChatPromptTemplate` per combinare domanda e contesto.\n",
    "4. `llm`: genera la risposta.\n",
    "5. `StrOutputParser`: prende solo il testo, escludendo metadati.\n",
    "\n",
    "---\n",
    "\n",
    "##  Come si esegue con `.invoke()`\n",
    "\n",
    "```python\n",
    "def rag_answer(question: str, chain) -> str:\n",
    "    return chain.invoke(question)\n",
    "```\n",
    "\n",
    "### `chain.invoke(...)`:\n",
    "\n",
    "* Prende in input una stringa (`question`).\n",
    "* La passa lungo la catena.\n",
    "* Recupera il contesto, costruisce il prompt, genera risposta.\n",
    "* Restituisce solo la stringa finale (grazie a `StrOutputParser()`).\n",
    "\n",
    "---\n",
    "\n",
    "##  Riassunto visivo\n",
    "\n",
    "```\n",
    "[User Question] \n",
    "   ‚Üì\n",
    "[Retriever]\n",
    "   ‚Üì\n",
    "[Contesto] + [Question]\n",
    "   ‚Üì\n",
    "[Prompt Template]\n",
    "   ‚Üì\n",
    "[LLM]\n",
    "   ‚Üì\n",
    "[Output Parser]\n",
    "   ‚Üì\n",
    "Risposta con citazioni accurate \n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
