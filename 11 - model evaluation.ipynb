{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3da4538-cecc-43d8-a00d-e4d5579e7246",
   "metadata": {},
   "source": [
    "# 1. Introduzione alla valutazione realistica dei modelli\n",
    "\n",
    "## 1.1 Perché la valutazione è critica\n",
    "\n",
    "### Idea chiave (in parole semplici)\n",
    "\n",
    "Valutare un modello **non è** guardare solo un numero (es. *accuracy*). È capire:\n",
    "\n",
    "1. **cosa** sbaglia (quali tipi di errori),\n",
    "2. **come** sbaglia (in quali situazioni/utenti/contesti),\n",
    "3. **quanto** costano quegli errori nel mondo reale,\n",
    "4. **se** continuerà a funzionare **dopo il rilascio** quando i dati cambiano (real-world shift).\n",
    "\n",
    "---\n",
    "\n",
    "### Passo 1 — Definisci l’obiettivo reale e il costo degli errori\n",
    "\n",
    "* **Domanda guida:** “Che cosa significa *buono* nel mio caso d’uso?”\n",
    "* **Classico errore:** ottimizzare l’accuracy quando le classi sono sbilanciate.\n",
    "* **Esempio (spam email):**\n",
    "\n",
    "  * Mancare una mail di spam (falso negativo) → rischi di phishing.\n",
    "  * Segnare come spam una mail legittima (falso positivo) → perdita di tempo o opportunità.\n",
    "    Se il **costo** di un falso negativo è più alto, ti interessa massimizzare **recall** sulla classe “spam”, non l’accuracy globale.\n",
    "\n",
    "**Mini-tabella costi (esempio):**\n",
    "\n",
    "* Falso negativo (spam non bloccato): costo = 10\n",
    "* Falso positivo (mail legittima bloccata): costo = 1\n",
    "  → La **metrica giusta** include questi pesi (es. *expected cost*, *recall*, *precision/recall trade-off*, *PR-AUC*).\n",
    "\n",
    "---\n",
    "\n",
    "### Passo 2 — Scegli set di valutazione che rappresentino la realtà\n",
    "\n",
    "* **Train / Validation / Test** separati, senza leakage.\n",
    "* Se i dati cambiano nel tempo, usa **split temporali** (out-of-time test).\n",
    "* Crea **slice/segmenti** (per utente, canale, lingua, device, fascia oraria, area geografica).\n",
    "\n",
    "**Esempio (churn clienti):**\n",
    "\n",
    "* Test “globale” + test su **clienti nuovi**, **grandi aziende**, **periodi festivi**.\n",
    "* Così scopri se il modello regge su segmenti critici dove il comportamento cambia.\n",
    "\n",
    "---\n",
    "\n",
    "### Passo 3 — Usa metriche allineate allo scopo (non solo accuracy)\n",
    "\n",
    "* **Classificazione sbilanciata:** usa **Precision/Recall, F1, PR-AUC** (meglio di ROC-AUC in forte sbilanciamento).\n",
    "* **Decisioni con soglia:** valuta **curve Precision-Recall** e scegli la **soglia** col miglior compromesso rispetto ai costi.\n",
    "* **Output probabilistici:** controlla la **calibrazione** (affidabilità delle probabilità).\n",
    "* **Generazione testo:** metrica di **qualità/fattualità** (es. BERTScore, giudizio umano, fact-checking).\n",
    "\n",
    "**Esempio numerico (sbilanciamento):**\n",
    "Dataset 1.000 email, 50 spam (5%).\n",
    "\n",
    "* Modello “banale”: predice sempre “non spam”.\n",
    "\n",
    "  * Accuracy = 95% (950/1000) **ma** Recall spam = 0% (inaccettabile).\n",
    "* Modello B: cattura 40 spam (TP=40, FN=10), sbaglia 100 mail legittime (FP=100).\n",
    "\n",
    "  * Accuracy = (40 + 850) / 1000 = 89% (più bassa!),\n",
    "  * **Recall spam = 80%** (molto meglio per la sicurezza),\n",
    "  * Trade-off da valutare rispetto ai costi: meglio B se il falso negativo costa tanto.\n",
    "\n",
    "---\n",
    "\n",
    "### Passo 4 — Fai **error analysis**: cosa e come sbaglia\n",
    "\n",
    "* **Confusion matrix**: capisci se confonde classi specifiche.\n",
    "* **Errori per slice**: dove si concentrano (lingue minoritarie? nuovi utenti? dispositivi mobile?).\n",
    "* **Top-k/Threshold analysis**: come cambiano gli errori variando la soglia o guardando le prime k predizioni.\n",
    "* **Calibrazione**: un 0,9 dovrebbe essere corretto \\~90% delle volte.\n",
    "\n",
    "**Esempio (sentiment):**\n",
    "Scopri che sbaglia spesso recensioni con **ironia** o **emoji**.\n",
    "Azione: aggiungi dati con questi casi o un pre-processing migliore → nuovo test mirato su quella slice.\n",
    "\n",
    "---\n",
    "\n",
    "### Passo 5 — Collega la valutazione alla **messa in produzione** (real-world shift)\n",
    "\n",
    "In produzione i dati **cambiano**:\n",
    "\n",
    "* **Covariate shift:** cambiano le caratteristiche in input (nuovi formati, nuove fonti).\n",
    "* **Prior shift:** cambiano le frequenze delle classi (più spam in certi periodi).\n",
    "* **Concept drift:** cambia la relazione input→output (nuove tattiche di spam).\n",
    "\n",
    "**Cosa fare:**\n",
    "\n",
    "* **Pre-rilascio:**\n",
    "\n",
    "  * Test out-of-time, **stress test** (rumore, campi mancanti), **robustezza** (input sporchi), test su **slice rare**.\n",
    "  * **Shadow mode**: il nuovo modello gira in parallelo senza impattare l’utente, per confrontare le predizioni.\n",
    "  * **Canary/A-B test**: rilascia al 5–10% del traffico, monitora metriche e rollback se peggiora.\n",
    "* **Post-rilascio:**\n",
    "\n",
    "  * **Monitoraggio** continuo di **distribuzioni** (data drift), **metriche di qualità** (precision/recall, tempo di risposta), **calibrazione**, **tassi di errore per slice**.\n",
    "  * **Feedback loop**: raccogli esempi reali (errori più costosi) e aggiorna il dataset di training.\n",
    "  * **Trigger di retraining**: soglie chiare (es. PR-AUC ↓ del 10% su “nuovi utenti” per 7 giorni).\n",
    "\n",
    "**Esempio (LLM QA interno):**\n",
    "Prima del rilascio: verifichi **factualità** su un set di domande con risposte supportate da documenti interni; misuri **hallucinations**.\n",
    "In produzione: attivi **retrieval logging** (quali documenti cita), **feedback utente** (thumbs up/down + motivo), **drift monitor** (nuovi tipi di domande), e pianifichi **retraining** se aumenta la percentuale di risposte non supportate.\n",
    "\n",
    "---\n",
    "\n",
    "### Passo 6 — Rendi le decisioni operative e ripetibili\n",
    "\n",
    "* **Soglia operativa** definita per business (es. “blocca come spam se prob ≥ 0,85”).\n",
    "* **Playbook**: cosa fare se la metrica scende (rollback? alza soglia? re-train?).\n",
    "* **Tracciabilità**: versiona **dati, codice, modello, metriche**, salva **confusion matrix** e **report per slice** ad ogni release.\n",
    "\n",
    "---\n",
    "\n",
    "### Esempi rapidi “prima/dopo” (a colpo d’occhio)\n",
    "\n",
    "* **Prima:** “Il modello ha 92% di accuracy, via in produzione.”\n",
    "* **Dopo:** “Sul test out-of-time il modello ha **PR-AUC 0,78**, **recall 0,86** sulla classe critica, buona **calibrazione** (ECE 2%), pochi errori su **clienti nuovi**; canary 10% stabile da 3 giorni → ok rilascio.”\n",
    "\n",
    "---\n",
    "\n",
    "### Checklist pratica (corta e utile)\n",
    "\n",
    "* [ ] Ho definito **metriche** coerenti con i **costi** degli errori?\n",
    "* [ ] Ho valutato per **slice** e su **test temporale**?\n",
    "* [ ] Ho fatto **error analysis** (confusion matrix, esempi tipici)?\n",
    "* [ ] Ho verificato **calibrazione** e scelto una **soglia operativa**?\n",
    "* [ ] Ho fatto **stress test** e simulato **dati sporchi/mancanti**?\n",
    "* [ ] Ho un piano di **monitoraggio** post-rilascio e **trigger** di retraining?\n",
    "* [ ] Ho documentato tutto (versioni, report, decisioni)?\n",
    "\n",
    "---\n",
    "\n",
    "### Messaggio finale\n",
    "\n",
    "La valutazione *realistica* è un **processo**, non un numero. Se colleghi metriche → errori → costi → produzione (e cambiamento nel tempo), costruisci modelli **utili**, **affidabili** e **mantenibili**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eea873-315f-4e4c-bfe3-7eeff0cb09e8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 2.1 BLEU (Bilingual Evaluation Understudy)\n",
    "\n",
    "## Teoria\n",
    "\n",
    "* **Cos’è:**\n",
    "  BLEU è una metrica usata per valutare la **qualità di traduzioni automatiche** confrontando l’output del modello con una o più traduzioni di riferimento (reference).\n",
    "* **Come funziona:**\n",
    "\n",
    "  * Divide il testo in **n-grammi** (sequenze di parole, es. 1-gram = singola parola, 2-gram = coppia di parole, ecc.).\n",
    "  * Calcola la **precisione** degli n-grammi → quanti degli n-grammi prodotti dal modello compaiono anche nelle traduzioni di riferimento.\n",
    "  * Usa una media pesata (in genere fino a 4-gram).\n",
    "  * Applica una **brevity penalty** (penalità per traduzioni troppo corte).\n",
    "\n",
    " Formula semplificata:\n",
    "\n",
    "$$\n",
    "BLEU = brevity\\_penalty \\times \\exp\\left(\\sum_{n=1}^{N} w_n \\cdot \\log p_n\\right)\n",
    "$$\n",
    "\n",
    "dove $p_n$ = precisione degli n-grammi, $w_n$ = peso (di solito uguale), e $N=4$.\n",
    "\n",
    "---\n",
    "\n",
    "## Limiti\n",
    "\n",
    "* Non considera la **semantica**:\n",
    "  se il modello usa sinonimi o parafrasi, BLEU può dare un punteggio basso anche se la traduzione è corretta.\n",
    "* È **sensibile alla lunghezza** → traduzioni troppo corte vengono penalizzate (brevity penalty).\n",
    "* È più adatto per testi tecnici o strutturati che per traduzioni creative.\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio pratico\n",
    "\n",
    "### Frase di riferimento (reference)\n",
    "\n",
    "> “The cat is on the mat”\n",
    "\n",
    "### Traduzione 1 (model output A)\n",
    "\n",
    "> “The cat is on the mat”\n",
    "\n",
    "* 1-gram match: tutti (100%)\n",
    "* 2-gram match: tutti (100%)\n",
    "* BLEU ≈ 1.0 (perfetta)\n",
    "\n",
    "### Traduzione 2 (model output B)\n",
    "\n",
    "> “The cat sits on the mat”\n",
    "\n",
    "* 1-gram match: *The, cat, on, the, mat* (5 su 6 ≈ 0.83)\n",
    "* 2-gram match: alcuni mancanti (“cat is” ≠ “cat sits”)\n",
    "* BLEU ≈ 0.75 (buona ma non perfetta)\n",
    "\n",
    "### Traduzione 3 (model output C)\n",
    "\n",
    "> “A feline is on the rug”\n",
    "\n",
    "* Sinonimi (“feline” vs “cat”, “rug” vs “mat”)\n",
    "* BLEU ≈ 0.3 (basso, anche se il significato è corretto)\n",
    "\n",
    " Qui si vede il limite: BLEU non “capisce” i sinonimi.\n",
    "\n",
    "---\n",
    "\n",
    "## Best practice\n",
    "\n",
    "* Non usare BLEU da solo → affiancalo a metriche semantiche (es. **BERTScore**) o a valutazioni umane.\n",
    "* Usa più **reference translations** (se disponibili) per aumentare la robustezza.\n",
    "* Interpreta BLEU in modo **comparativo**:\n",
    "\n",
    "  * Non è un valore assoluto “buono/cattivo”.\n",
    "  * Serve per confrontare modelli o versioni dello stesso sistema.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea9e3c-4064-453d-971c-9442b8740e90",
   "metadata": {},
   "source": [
    "\n",
    "Ecco un esempio molto semplice in Python con **NLTK**\n",
    "\n",
    "```python\n",
    "# Installiamo NLTK se non è già presente\n",
    "# !pip install nltk\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Frase di riferimento (la traduzione \"corretta\")\n",
    "reference = [\"the cat is on the mat\".split()]\n",
    "\n",
    "# 3 esempi di traduzioni generate dal modello\n",
    "candidate1 = \"the cat is on the mat\".split()\n",
    "candidate2 = \"the cat sits on the mat\".split()\n",
    "candidate3 = \"a feline is on the rug\".split()\n",
    "\n",
    "# Calcoliamo il BLEU score per ciascuna traduzione\n",
    "score1 = sentence_bleu(reference, candidate1)\n",
    "score2 = sentence_bleu(reference, candidate2)\n",
    "score3 = sentence_bleu(reference, candidate3)\n",
    "\n",
    "# Stampiamo i risultati\n",
    "print(\"Traduzione 1:\", \" \".join(candidate1))\n",
    "print(\"BLEU score:\", round(score1, 3))  # ≈ 1.0 (perfetta)\n",
    "print()\n",
    "\n",
    "print(\"Traduzione 2:\", \" \".join(candidate2))\n",
    "print(\"BLEU score:\", round(score2, 3))  # ≈ 0.75 (buona ma non perfetta)\n",
    "print()\n",
    "\n",
    "print(\"Traduzione 3:\", \" \".join(candidate3))\n",
    "print(\"BLEU score:\", round(score3, 3))  # ≈ 0.3 (bassa, sinonimi non riconosciuti)\n",
    "```\n",
    "\n",
    "### Output atteso:\n",
    "\n",
    "```\n",
    "Traduzione 1: the cat is on the mat\n",
    "BLEU score: 1.0\n",
    "\n",
    "Traduzione 2: the cat sits on the mat\n",
    "BLEU score: 0.75\n",
    "\n",
    "Traduzione 3: a feline is on the rug\n",
    "BLEU score: 0.3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    " In questo esempio si vede bene:\n",
    "\n",
    "* **Candidato 1** = traduzione identica → punteggio massimo.\n",
    "* **Candidato 2** = quasi identica, ma cambia un n-gram (“cat is” → “cat sits”) → punteggio più basso.\n",
    "* **Candidato 3** = traduzione semanticamente corretta, ma con sinonimi → BLEU molto basso (limite della metrica).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3134e16-e898-4c85-b2fd-246a3c16033e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 2.2 ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "\n",
    "## Teoria\n",
    "\n",
    "* **Cos’è:**\n",
    "  ROUGE è una metrica pensata per valutare automaticamente la qualità di **riassunti (summarization)** confrontando un riassunto generato da un modello con uno o più riassunti di riferimento (*gold standard*).\n",
    "\n",
    "* **Come funziona:**\n",
    "\n",
    "  * Conta quanti **n-grammi** (parole o sequenze di parole) del riassunto generato compaiono anche nel riassunto di riferimento.\n",
    "  * È orientata al **recall** → misura “quanto del contenuto corretto è stato coperto dal riassunto generato”.\n",
    "  * Utile perché nei riassunti vogliamo evitare omissioni importanti.\n",
    "\n",
    "---\n",
    "\n",
    "## Tipi principali di ROUGE\n",
    "\n",
    "* **ROUGE-1:** confronto di singole parole (unigrammi).\n",
    "* **ROUGE-2:** confronto di coppie di parole (bigrammi).\n",
    "* **ROUGE-L:** confronto basato sulla **Longest Common Subsequence (LCS)** → quanto è simile la sequenza di parole più lunga in comune.\n",
    "\n",
    " In pratica:\n",
    "\n",
    "* ROUGE-1: copertura del vocabolario rilevante.\n",
    "* ROUGE-2: preserva un po’ dell’ordine.\n",
    "* ROUGE-L: cattura la struttura delle frasi.\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio intuitivo\n",
    "\n",
    "### Riassunto di riferimento\n",
    "\n",
    "> \"Il gatto dorme sul tappeto\"\n",
    "\n",
    "### Riassunto generato dal modello A\n",
    "\n",
    "> \"Il gatto è sul tappeto\"\n",
    "\n",
    "* ROUGE-1 (parole in comune): alto (quasi tutte).\n",
    "* ROUGE-2 (bigrammi): medio (“gatto sul” manca, “è sul” non appare nel riferimento).\n",
    "* ROUGE-L: abbastanza alto perché la struttura rimane simile.\n",
    "\n",
    "### Riassunto generato dal modello B\n",
    "\n",
    "> \"Un cane gioca in giardino\"\n",
    "\n",
    "* ROUGE-1: quasi nullo (parole non corrispondono).\n",
    "* ROUGE-2: 0 (nessun bigramma in comune).\n",
    "* ROUGE-L: 0 (nessuna sequenza lunga in comune).\n",
    "\n",
    " ROUGE funziona bene nel dire se il riassunto cattura il contenuto del testo originale.\n",
    "\n",
    "---\n",
    "\n",
    "## Best practice\n",
    "\n",
    "* Non usare solo ROUGE-1: combinare **ROUGE-1, ROUGE-2 e ROUGE-L**.\n",
    "* Usare più riassunti di riferimento se possibile (diversi umani scrivono riassunti diversi).\n",
    "* Interpretare in modo comparativo: serve per confrontare **modelli tra loro**, non per dire se un singolo riassunto è “buono” in assoluto.\n",
    "* Aggiungere anche una valutazione **umana** (fluency, coerenza, leggibilità).\n",
    "\n",
    "---\n",
    "\n",
    "## Mini-esempio Python con `rouge-score`\n",
    "\n",
    "```python\n",
    "# Installiamo la libreria rouge-score di Google\n",
    "# !pip install rouge-score\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Riassunto di riferimento (gold standard)\n",
    "reference = \"il gatto dorme sul tappeto\"\n",
    "\n",
    "# Due riassunti generati\n",
    "candidate1 = \"il gatto è sul tappeto\"\n",
    "candidate2 = \"un cane gioca in giardino\"\n",
    "\n",
    "# Creiamo lo scorer per ROUGE-1, ROUGE-2 e ROUGE-L\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calcoliamo i punteggi\n",
    "scores1 = scorer.score(reference, candidate1)\n",
    "scores2 = scorer.score(reference, candidate2)\n",
    "\n",
    "# Stampiamo i risultati\n",
    "print(\"Candidato 1:\", candidate1)\n",
    "print(\"ROUGE:\", scores1)  # valori alti (simile al riferimento)\n",
    "\n",
    "print(\"\\nCandidato 2:\", candidate2)\n",
    "print(\"ROUGE:\", scores2)  # valori bassi (contenuto diverso)\n",
    "```\n",
    "\n",
    "### Output atteso (semplificato)\n",
    "\n",
    "```\n",
    "Candidato 1:\n",
    "ROUGE-1 ≈ 0.8\n",
    "ROUGE-2 ≈ 0.6\n",
    "ROUGE-L ≈ 0.7\n",
    "\n",
    "Candidato 2:\n",
    "ROUGE-1 ≈ 0.1\n",
    "ROUGE-2 = 0.0\n",
    "ROUGE-L = 0.0\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149aa1e1-e293-43d8-8bf9-116171d3dcc1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 2.3 METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
    "\n",
    "## Teoria\n",
    "\n",
    "* **Cos’è:**\n",
    "  METEOR è una metrica creata per migliorare i limiti di BLEU.\n",
    "  Non si basa solo sugli **n-grammi esatti**, ma integra anche:\n",
    "\n",
    "  * **Sinonimi:** riconosce parole con significato simile (*cat* ↔ *feline*).\n",
    "  * **Stemming:** riduce le parole alla radice (*run*, *running*, *runs* → *run*).\n",
    "  * **Word order:** valuta quanto l’ordine delle parole nella frase generata rispetta quello del riferimento.\n",
    "\n",
    "* **Differenza da BLEU:**\n",
    "  BLEU penalizza traduzioni semanticamente corrette ma con parole diverse.\n",
    "  METEOR invece si avvicina di più al **giudizio umano**, perché riconosce variazioni lessicali e grammaticali.\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio intuitivo\n",
    "\n",
    "### Frase di riferimento\n",
    "\n",
    "> “The cat is on the mat”\n",
    "\n",
    "### Traduzione modello A\n",
    "\n",
    "> “The cat is on the mat”\n",
    "\n",
    "* METEOR ≈ 1.0 (perfetta, come BLEU e ROUGE).\n",
    "\n",
    "### Traduzione modello B\n",
    "\n",
    "> “The feline is on the rug”\n",
    "\n",
    "* BLEU: punteggio basso (nessun match esatto).\n",
    "* ROUGE: punteggio basso (poche parole in comune).\n",
    "* METEOR: punteggio medio-alto, perché riconosce **feline = cat** e **rug = mat**.\n",
    "\n",
    " Qui METEOR riflette meglio la correttezza semantica.\n",
    "\n",
    "---\n",
    "\n",
    "## Perché è utile\n",
    "\n",
    "* Ha una **correlazione più alta con i giudizi umani** rispetto a BLEU e ROUGE.\n",
    "* È molto usata nella **machine translation** e in compiti dove i sinonimi sono comuni.\n",
    "\n",
    "---\n",
    "\n",
    "## Best practice\n",
    "\n",
    "* Usare METEOR insieme a BLEU e ROUGE per avere un quadro completo.\n",
    "* Preferirla quando si valutano **traduzioni** o **parafrasi** dove sinonimi/parole equivalenti sono importanti.\n",
    "* Non usarla come unica metrica → come tutte, ha limiti (es. non cattura bene la fluidità del testo).\n",
    "\n",
    "---\n",
    "\n",
    "## Mini-esempio Python con `nltk`\n",
    "\n",
    "```python\n",
    "# Installiamo NLTK se non è già presente\n",
    "# !pip install nltk\n",
    "\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Frase di riferimento\n",
    "reference = [\"the cat is on the mat\"]\n",
    "\n",
    "# Tre traduzioni generate\n",
    "candidate1 = \"the cat is on the mat\"   # perfetta\n",
    "candidate2 = \"the cat sits on the mat\" # simile\n",
    "candidate3 = \"the feline is on the rug\" # sinonimi\n",
    "\n",
    "# Calcoliamo il METEOR score\n",
    "score1 = meteor_score([reference], candidate1)\n",
    "score2 = meteor_score([reference], candidate2)\n",
    "score3 = meteor_score([reference], candidate3)\n",
    "\n",
    "print(\"Traduzione 1:\", candidate1)\n",
    "print(\"METEOR:\", round(score1, 3))  # ≈ 1.0\n",
    "\n",
    "print(\"\\nTraduzione 2:\", candidate2)\n",
    "print(\"METEOR:\", round(score2, 3))  # ≈ 0.8-0.9\n",
    "\n",
    "print(\"\\nTraduzione 3:\", candidate3)\n",
    "print(\"METEOR:\", round(score3, 3))  # ≈ 0.6-0.7 (meglio di BLEU)\n",
    "```\n",
    "\n",
    "### Output atteso (indicativo)\n",
    "\n",
    "```\n",
    "Traduzione 1: METEOR ≈ 1.0\n",
    "Traduzione 2: METEOR ≈ 0.85\n",
    "Traduzione 3: METEOR ≈ 0.65\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1049d9-c256-4e63-8bad-a30a83f5afd3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 2.4 BERTScore\n",
    "\n",
    "## Teoria\n",
    "\n",
    "* **Cos’è:**\n",
    "  BERTScore è una metrica di valutazione per NLP che usa **embedding da modelli pre-addestrati** (BERT, RoBERTa, ecc.) per confrontare il significato delle frasi, non solo le parole.\n",
    "\n",
    "* **Come funziona:**\n",
    "\n",
    "  1. Trasforma ogni parola in un **embedding vettoriale** grazie a un modello come BERT.\n",
    "  2. Calcola la **similarità coseno** tra le parole del testo generato e quelle del riferimento.\n",
    "  3. Aggrega i punteggi di similarità in un valore complessivo (precision, recall e F1).\n",
    "\n",
    "A differenza di BLEU/ROUGE (che guardano match esatti di parole o n-grammi), BERTScore valuta la **vicinanza semantica**.\n",
    "\n",
    "---\n",
    "\n",
    "## Pro\n",
    "\n",
    "* Cattura **sinonimi e parafrasi**.\n",
    "* Correlazione molto alta con il **giudizio umano**.\n",
    "* Utile per valutare compiti come **Question Answering (QA)**, **dialogo**, **summarization**.\n",
    "\n",
    "## Contro\n",
    "\n",
    "* Più pesante computazionalmente (serve un modello pre-addestrato).\n",
    "* Dipende dal modello di embedding scelto (es. inglese vs multilingua).\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio intuitivo\n",
    "\n",
    "### Risposta di riferimento\n",
    "\n",
    "> “The cat is on the mat”\n",
    "\n",
    "### Risposta modello A\n",
    "\n",
    "> “The cat is on the mat”\n",
    "\n",
    "* BERTScore ≈ 1.0 (identica)\n",
    "\n",
    "### Risposta modello B\n",
    "\n",
    "> “The feline rests on the rug”\n",
    "\n",
    "* BLEU: basso (parole diverse)\n",
    "* ROUGE: basso (pochi match)\n",
    "* METEOR: medio (riconosce sinonimi)\n",
    "* **BERTScore: alto (\\~0.9)** → capisce che *feline ≈ cat*, *rug ≈ mat*.\n",
    "\n",
    " Qui BERTScore riflette meglio la **correttezza semantica**.\n",
    "\n",
    "---\n",
    "\n",
    "## Best practice\n",
    "\n",
    "* Usalo insieme ad altre metriche (es. ROUGE + BERTScore).\n",
    "* Scegli un modello di embedding coerente con il dominio (es. *SciBERT* per testi scientifici).\n",
    "* Più adatto in **lingue ricche di sinonimi** o per compiti di **dialogo aperto**.\n",
    "\n",
    "---\n",
    "\n",
    "## Mini-esempio Python con `bert-score`\n",
    "\n",
    "```python\n",
    "# Installiamo la libreria\n",
    "# !pip install bert-score\n",
    "\n",
    "from bert_score import score\n",
    "\n",
    "# Frase di riferimento\n",
    "reference = [\"the cat is on the mat\"]\n",
    "\n",
    "# Due risposte generate\n",
    "candidate1 = [\"the cat is on the mat\"]         # identica\n",
    "candidate2 = [\"the feline rests on the rug\"]  # sinonimi/parafrasi\n",
    "\n",
    "# Calcoliamo i punteggi (usa default = roberta-large)\n",
    "P1, R1, F1_1 = score(candidate1, reference, lang=\"en\", verbose=True)\n",
    "P2, R2, F2_2 = score(candidate2, reference, lang=\"en\", verbose=True)\n",
    "\n",
    "print(\"Candidato 1:\", candidate1[0])\n",
    "print(\"BERTScore F1:\", F1_1.mean().item())\n",
    "\n",
    "print(\"\\nCandidato 2:\", candidate2[0])\n",
    "print(\"BERTScore F1:\", F2_2.mean().item())\n",
    "```\n",
    "\n",
    "### Output atteso (indicativo)\n",
    "\n",
    "```\n",
    "Candidato 1: the cat is on the mat\n",
    "BERTScore F1: 1.0\n",
    "\n",
    "Candidato 2: the feline rests on the rug\n",
    "BERTScore F1: 0.89\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    " Con questo esempio si vede subito il vantaggio:\n",
    "anche con parole diverse, BERTScore “capisce” che le frasi hanno **lo stesso significato**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306655a-4a2c-4411-858c-febe35301731",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# 3.1 Limiti delle metriche tradizionali per i LLM\n",
    "\n",
    "## Teoria\n",
    "\n",
    "I **Large Language Models (LLM)** (come GPT, LLaMA, ecc.) hanno un problema fondamentale:\n",
    "\n",
    "* Non producono sempre un **output fisso e prevedibile**.\n",
    "* L’output è **libero** (testo lungo, creativo, con parafrasi).\n",
    "* Non esiste quasi mai **una sola risposta corretta** (ground truth).\n",
    "\n",
    " Per questo metriche tradizionali come **BLEU, ROUGE, METEOR** funzionano male:\n",
    "\n",
    "* si basano sul confronto con una **reference text** precisa,\n",
    "* ma il modello può generare risposte **diverse ma comunque corrette**.\n",
    "\n",
    "---\n",
    "\n",
    "## Perché è un problema\n",
    "\n",
    "* **Ground truth limitato:** spesso c’è **una sola frase di riferimento** → BLEU/ROUGE penalizzano sinonimi o frasi equivalenti.\n",
    "* **Variabilità delle risposte:** lo stesso modello può generare risposte diverse in momenti diversi (stocasticità).\n",
    "* **Output lungo/complesso:** nei compiti aperti (QA, dialogo, summarization creativo) non esiste una definizione univoca di “risposta giusta”.\n",
    "* **Valore semantico:** le metriche basate su parole non misurano la **factualità**, la **consistenza logica** o la **qualità stilistica**.\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio pratico\n",
    "\n",
    "### Domanda\n",
    "\n",
    "> “Chi ha scritto *I Promessi Sposi*?”\n",
    "\n",
    "### Risposta di riferimento (ground truth)\n",
    "\n",
    "> “Alessandro Manzoni ha scritto I Promessi Sposi.”\n",
    "\n",
    "### Risposta modello A\n",
    "\n",
    "> “L’autore de *I Promessi Sposi* è Alessandro Manzoni.”\n",
    "\n",
    "* BLEU/ROUGE: punteggio medio (parole diverse).\n",
    "* Valutazione umana: **corretto al 100%**.\n",
    "\n",
    "### Risposta modello B\n",
    "\n",
    "> “I Promessi Sposi è stato scritto da Alessandro Manzoni nel 1827.”\n",
    "\n",
    "* BLEU/ROUGE: punteggio basso (aggiunge info extra non presenti nel riferimento).\n",
    "* Valutazione umana: **corretto e persino più informativo**.\n",
    "\n",
    " Le metriche tradizionali non colgono che entrambe le risposte sono corrette.\n",
    "\n",
    "---\n",
    "\n",
    "## Best practice\n",
    "\n",
    "* Non affidarsi a metriche tradizionali da sole.\n",
    "* Per i LLM, servono metodi che valutino **significato e factualità**, non solo somiglianza di stringhe.\n",
    "* Introdurre metriche più avanzate (es. **BERTScore, MAUVE, GPT-Eval**) o valutazione **umana guidata**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d77ca6-d4ce-4edf-b787-baa68bb7ad0f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 3.2 MAUVE\n",
    "\n",
    "## Teoria\n",
    "\n",
    "* **Cos’è:**\n",
    "  MAUVE (Malik et al., 2021) è una metrica progettata per valutare la **qualità del testo generato da LLM**.\n",
    "\n",
    "* **Idea chiave:**\n",
    "  Invece di confrontare parola per parola con un testo di riferimento (come BLEU/ROUGE),\n",
    "  MAUVE confronta **le distribuzioni linguistiche** di:\n",
    "\n",
    "  * **Testo generato dal modello**\n",
    "  * **Testo reale di riferimento (human-written)**\n",
    "\n",
    "* **Come funziona (intuitivo):**\n",
    "\n",
    "  1. Prende un insieme di testi reali e uno di testi generati.\n",
    "  2. Usa un modello di linguaggio pre-addestrato per ottenere rappresentazioni probabilistiche delle frasi.\n",
    "  3. Confronta le **distribuzioni di probabilità** con una misura di divergenza (simile a KL-divergence).\n",
    "  4. Restituisce un punteggio tra 0 e 1.\n",
    "\n",
    "     * **Vicino a 1** → il testo generato assomiglia molto a quello umano.\n",
    "     * **Vicino a 0** → il testo generato è poco realistico.\n",
    "\n",
    " In sintesi: MAUVE misura quanto bene il modello “imita” la distribuzione del linguaggio naturale.\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio pratico\n",
    "\n",
    "### Dataset reale\n",
    "\n",
    "Frasi scritte da umani in un corpus (es. Wikipedia o news):\n",
    "\n",
    "* “Il gatto dorme sul tappeto.”\n",
    "* “Il Parlamento ha approvato una nuova legge sulla sicurezza.”\n",
    "* “La squadra ha vinto dopo una partita difficile.”\n",
    "\n",
    "### Output modello GPT (generato)\n",
    "\n",
    "* “Il gatto riposa sul tappeto.”\n",
    "\n",
    "* “Il governo ha varato una nuova normativa per la sicurezza.”\n",
    "\n",
    "* “La squadra ha trionfato dopo una gara complicata.”\n",
    "\n",
    "* **BLEU/ROUGE:** potrebbero dare punteggi bassi (parole non identiche).\n",
    "\n",
    "* **MAUVE:** alto (perché la distribuzione lessicale e sintattica è simile a quella umana).\n",
    "\n",
    "### Altro esempio (modello debole)\n",
    "\n",
    "Testi generati dal modello:\n",
    "\n",
    "* “Gatto tappeto dormire.”\n",
    "\n",
    "* “Legge legge legge sicurezza.”\n",
    "\n",
    "* “Partita squadra vinto vinto difficile.”\n",
    "\n",
    "* **MAUVE:** punteggio vicino a 0, perché la distribuzione è molto lontana da quella umana.\n",
    "\n",
    "---\n",
    "\n",
    "## Best practice\n",
    "\n",
    "* Usare MAUVE per valutare **compiti generativi open-ended** (dialogo, storytelling, summarization).\n",
    "* Non sostituisce la valutazione umana: un testo “realistico” può comunque contenere **hallucination** o errori fattuali.\n",
    "* Combinare MAUVE con altre metriche:\n",
    "\n",
    "  * **BERTScore** (similarità semantica con reference).\n",
    "  * **Factuality checks** (per QA o compiti di knowledge grounding).\n",
    "  * **Human eval** (per coerenza, creatività, leggibilità).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33ce74-007b-46ef-9eba-f264ac60dbd1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 3.3 GPT-Eval\n",
    "\n",
    "## Teoria\n",
    "\n",
    "* **Cos’è:**\n",
    "  GPT-Eval è un approccio di valutazione in cui si usa **un Large Language Model (LLM)** (es. GPT-4, LLaMA, Claude) come **giudice** per confrontare output di altri modelli.\n",
    "\n",
    "* **Come funziona (pairwise ranking):**\n",
    "\n",
    "  1. Si prende un **prompt** (input).\n",
    "  2. Due (o più) modelli generano risposte.\n",
    "  3. Un LLM “giudice” riceve entrambe le risposte + i criteri di valutazione (es. correttezza, chiarezza, factualità).\n",
    "  4. L’LLM indica quale risposta è migliore (*pairwise ranking*) o assegna un punteggio.\n",
    "\n",
    "In pratica: un modello valuta altri modelli, simulando una “giuria automatica”.\n",
    "\n",
    "---\n",
    "\n",
    "## Pro\n",
    "\n",
    "* **Scalabile:**\n",
    "  puoi valutare migliaia di risposte senza dover pagare o coordinare annotatori umani.\n",
    "* **Flessibile:**\n",
    "  basta cambiare le istruzioni al giudice per adattare i criteri (es. factualità, concisione, stile).\n",
    "* **Velocità:**\n",
    "  molto più rapido della human eval tradizionale.\n",
    "\n",
    "## Contro\n",
    "\n",
    "* **Bias:**\n",
    "  il modello giudice può avere **preferenze stilistiche** o allineamenti che influenzano la valutazione.\n",
    "* **Dipendenza dal prompt:**\n",
    "  se il prompt non è chiaro, i giudizi diventano incoerenti.\n",
    "* **Autovalutazione:**\n",
    "  se il giudice è lo stesso modello valutato, può risultare “di parte”.\n",
    "* **Ripetibilità:**\n",
    "  la valutazione può variare a seconda della random seed e della temperatura.\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio intuitivo\n",
    "\n",
    "### Prompt\n",
    "\n",
    "> “Spiega cosa sono i buchi neri in modo semplice.”\n",
    "\n",
    "### Risposta modello A\n",
    "\n",
    "> “Un buco nero è una regione dello spazio con gravità così forte che nulla, nemmeno la luce, può sfuggire.”\n",
    "\n",
    "### Risposta modello B\n",
    "\n",
    "> “I buchi neri sono oggetti misteriosi che ingoiano tutto, anche il tempo.”\n",
    "\n",
    "### GPT come giudice\n",
    "\n",
    "* Istruzione: “Valuta quale risposta è più scientificamente corretta e chiara.”\n",
    "* Output: “La risposta A è migliore.”\n",
    "\n",
    "Qui GPT ha fatto un **pairwise ranking**: ha scelto A rispetto a B.\n",
    "\n",
    "---\n",
    "\n",
    "## Best practice\n",
    "\n",
    "* Non usare GPT-Eval come unico criterio → affiancalo a metriche automatiche e human eval.\n",
    "* Definisci bene i **criteri di giudizio** (es. “valuta la factualità e la chiarezza, ignora lo stile creativo”).\n",
    "* Usa più di un modello giudice, se possibile, per ridurre bias.\n",
    "* Valuta la **consistenza**: lancia più volte lo stesso test e confronta i risultati.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e750b-25a7-4964-8a4e-63cd66997fae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 3.4 Eval Harness\n",
    "\n",
    "## Teoria\n",
    "\n",
    "* **Cos’è:**\n",
    "  **Eval Harness** (o *lm-evaluation-harness*) è un **framework open source** che permette di valutare i **Large Language Models (LLM)** in modo **standardizzato** su una vasta gamma di benchmark.\n",
    "* È stato creato dalla community di EleutherAI.\n",
    "* L’obiettivo è avere un **set comune di test** per confrontare in modo equo diversi modelli.\n",
    "\n",
    "---\n",
    "\n",
    "## Come funziona\n",
    "\n",
    "1. **Collezione di benchmark** già pronti:\n",
    "\n",
    "   * Question Answering (QA)\n",
    "   * Cloze tests (riempi gli spazi)\n",
    "   * Multiple Choice (es. MMLU)\n",
    "   * Summarization, Translation, Commonsense reasoning\n",
    "\n",
    "2. **Input standardizzati:**\n",
    "\n",
    "   * Il framework prepara automaticamente i prompt.\n",
    "   * I modelli generano risposte.\n",
    "\n",
    "3. **Valutazione automatica:**\n",
    "\n",
    "   * Calcola metriche di accuratezza, log-likelihood, ecc.\n",
    "   * Produce report comparabili.\n",
    "\n",
    "In sintesi: tu fornisci il modello, Eval Harness lo mette alla prova su benchmark noti e restituisce risultati confrontabili.\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio pratico\n",
    "\n",
    "### Caso 1: QA (Question Answering)\n",
    "\n",
    "**Input (SQuAD-like):**\n",
    "\n",
    "> Domanda: “Chi ha dipinto la Gioconda?”\n",
    "> Contesto: “La Gioconda è un dipinto famoso creato da Leonardo da Vinci.”\n",
    "\n",
    "**Output atteso:** “Leonardo da Vinci”\n",
    "**Eval Harness** misura se il modello produce la risposta corretta (exact match o F1).\n",
    "\n",
    "---\n",
    "\n",
    "### Caso 2: Cloze (riempi gli spazi)\n",
    "\n",
    "**Input:**\n",
    "\n",
    "> “Parigi è la capitale della \\_\\_\\_.”\n",
    "\n",
    "**Opzioni generate:** Francia / Italia / Spagna\n",
    "**Eval Harness** valuta la probabilità data dal modello a ciascuna opzione → correttezza.\n",
    "\n",
    "---\n",
    "\n",
    "### Caso 3: Multiple Choice (MMLU test)\n",
    "\n",
    "**Domanda:**\n",
    "\n",
    "> “Qual è la derivata di $x^2$?”\n",
    "> **Opzioni:** A) 2x  B) x  C) $x^2$  D) 1\n",
    "\n",
    "**Eval Harness** confronta la scelta del modello con la risposta corretta (A).\n",
    "\n",
    "---\n",
    "\n",
    "## Perché è utile\n",
    "\n",
    "* **Standardizzazione:** tutti i modelli valutati sugli stessi benchmark → confronto equo.\n",
    "* **Copertura ampia:** include compiti diversi (linguistici, matematici, logici).\n",
    "* **Facile automazione:** basta collegare il tuo modello → ottieni report pronti.\n",
    "\n",
    "---\n",
    "\n",
    "## Best practice\n",
    "\n",
    "* Usare Eval Harness quando vuoi **confrontare il tuo modello con altri** su benchmark noti.\n",
    "* Non sostituisce test personalizzati → sempre integrare con dataset del tuo dominio.\n",
    "* Documentare bene quale versione del benchmark hai usato (dataset possono evolvere).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3bb33e-55e3-4179-9983-97f7fa8b021c",
   "metadata": {},
   "source": [
    "\n",
    "# 4.1 Cos’è una hallucinazione\n",
    "\n",
    "## Teoria\n",
    "\n",
    "* **Definizione:**\n",
    "  In NLP e nei **Large Language Models (LLM)**, una **hallucination** è una **risposta generata che sembra plausibile e ben scritta, ma non è supportata dai dati reali o dal contesto disponibile**.\n",
    "\n",
    "* **Caratteristica chiave:**\n",
    "\n",
    "  * La risposta **“suona bene”**, ma è **falsa** o **non verificabile**.\n",
    "  * Il modello non distingue tra informazioni vere e inventate, perché genera testo basandosi su probabilità linguistiche, non su conoscenza certa.\n",
    "\n",
    "Le hallucinations sono uno dei **principali rischi** quando si usano LLM in contesti critici (sanità, finanza, legale).\n",
    "\n",
    "---\n",
    "\n",
    "## Tipi di hallucinazioni\n",
    "\n",
    "1. **Factual Hallucination:**\n",
    "\n",
    "   * Il modello inventa un fatto inesistente.\n",
    "   * Es: attribuire un libro a un autore sbagliato.\n",
    "\n",
    "2. **Contextual Hallucination:**\n",
    "\n",
    "   * La risposta non è coerente con il contesto fornito.\n",
    "   * Es: domanda su un testo, ma il modello risponde con contenuto non presente.\n",
    "\n",
    "3. **Linguistic Hallucination:**\n",
    "\n",
    "   * Il testo è grammaticalmente corretto ma semanticamente vuoto o contraddittorio.\n",
    "   * Es: risposte circolari o vaghe che non aggiungono informazioni.\n",
    "\n",
    "---\n",
    "\n",
    "## Esempi pratici\n",
    "\n",
    "### Esempio 1 – Factual hallucination\n",
    "\n",
    "**Domanda:** “Chi ha vinto il Premio Nobel per la Pace nel 2022?”\n",
    "**Risposta modello (inventata):** “Il Premio Nobel per la Pace 2022 è stato assegnato a Greta Thunberg.”\n",
    "Suona plausibile, ma **falso** (in realtà vinto da Ales Bialiatski, Memorial e Center for Civil Liberties).\n",
    "\n",
    "---\n",
    "\n",
    "### Esempio 2 – Contextual hallucination\n",
    "\n",
    "**Contesto:** “Il documento parla di Leonardo da Vinci e delle sue opere.”\n",
    "**Domanda:** “Qual è il nome della moglie di Leonardo da Vinci?”\n",
    "**Risposta modello:** “La moglie di Leonardo da Vinci si chiamava Isabella.”\n",
    "Hallucination: nel documento (e nella realtà) non esistono prove che Leonardo fosse sposato.\n",
    "\n",
    "---\n",
    "\n",
    "### Esempio 3 – Linguistic hallucination\n",
    "\n",
    "**Domanda:** “Spiegami il concetto di entropia.”\n",
    "**Risposta modello:** “L’entropia è ciò che rende l’entropia entropica, e riguarda l’energia disordinata.”\n",
    "Frase formalmente corretta ma **vuota di significato reale**.\n",
    "\n",
    "---\n",
    "\n",
    "## Best practice\n",
    "\n",
    "* **Mai fidarsi solo della forma**: una risposta elegante non è garanzia di verità.\n",
    "* **Contesto limitato → maggiore rischio**: quando il modello non ha abbastanza dati di supporto, tende a inventare.\n",
    "* **Ambiti critici**: sempre verificare con **fonti affidabili** o sistemi di **retrieval grounding** (RAG).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25595c5-9f70-4d5e-a4b3-25843f661380",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 4.2 Metodi di rilevamento delle Hallucinations\n",
    "\n",
    "## 1. Fact-checking con knowledge base\n",
    "\n",
    "* **Idea:**\n",
    "  Confrontare le affermazioni generate dal modello con una **base di conoscenza affidabile** (es. Wikipedia, database scientifici, documenti interni aziendali).\n",
    "\n",
    "* **Come funziona:**\n",
    "\n",
    "  1. Si estraggono i **fatti chiave** dalla risposta (es. entità, date, eventi).\n",
    "  2. Si interrogano fonti esterne (API, knowledge graph, database).\n",
    "  3. Si verifica se i fatti sono presenti e coerenti.\n",
    "\n",
    "* **Esempio pratico:**\n",
    "\n",
    "  * Risposta modello: *“Il presidente della Francia nel 2023 è François Hollande.”*\n",
    "  * Fact-check su Wikipedia → presidente corretto: Emmanuel Macron.\n",
    "  * Risultato: **hallucination rilevata**.\n",
    "\n",
    "* **Best practice:**\n",
    "\n",
    "  * Usare knowledge base **aggiornate**.\n",
    "  * Automatizzare con sistemi di **retrieval-augmented generation (RAG)**: il modello cita documenti reali come fonte.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Entailment scoring (NLI – Natural Language Inference)\n",
    "\n",
    "* **Idea:**\n",
    "  Usare un modello di **entailment** per verificare se l’affermazione generata è **supportata, contraddetta o neutrale** rispetto a un testo di riferimento.\n",
    "\n",
    "* **Categorie di output tipiche:**\n",
    "\n",
    "  * **Entailment** → la risposta è supportata dal contesto.\n",
    "  * **Contradiction** → la risposta è in conflitto col contesto.\n",
    "  * **Neutral** → il contesto non basta a verificare la risposta.\n",
    "\n",
    "* **Esempio pratico:**\n",
    "\n",
    "  * Contesto: *“Leonardo da Vinci dipinse la Gioconda.”*\n",
    "  * Risposta modello: *“La Gioconda è stata dipinta da Leonardo da Vinci.”*\n",
    "    → **Entailment** (supportata).\n",
    "  * Risposta modello: *“La Gioconda è stata dipinta da Michelangelo.”*\n",
    "    → **Contradiction** (hallucination).\n",
    "\n",
    "* **Best practice:**\n",
    "\n",
    "  * Usare modelli NLI pre-addestrati (es. `roberta-large-mnli`).\n",
    "  * Applicare su dataset con **domande + contesto**, non su risposte isolate.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Embedding similarity con fonti di riferimento\n",
    "\n",
    "* **Idea:**\n",
    "  Misurare la **similarità semantica** tra la risposta generata e documenti affidabili (embedding + nearest neighbors).\n",
    "  Se la risposta è **lontana semanticamente** dalle fonti, è probabile che sia inventata.\n",
    "\n",
    "* **Come funziona:**\n",
    "\n",
    "  1. Trasforma documenti e risposta in **embedding vettoriali**.\n",
    "  2. Calcola la similarità coseno.\n",
    "  3. Se la similarità è sotto una soglia → possibile hallucination.\n",
    "\n",
    "* **Esempio pratico:**\n",
    "\n",
    "  * Fonte: “Il Sole è una stella di tipo G.”\n",
    "  * Risposta modello: “Il Sole è una galassia.”\n",
    "  * Embedding della risposta non è vicino a quello delle fonti → segnalato come hallucination.\n",
    "\n",
    "* **Best practice:**\n",
    "\n",
    "  * Usare modelli di embedding robusti (es. OpenAI text-embedding, SBERT).\n",
    "  * Definire **soglie diverse** a seconda del dominio (scienza vs narrativa).\n",
    "  * Integrare con retrieval: se nessuna fonte simile è trovata → risposta a rischio.\n",
    "\n",
    "---\n",
    "\n",
    "**In sintesi:**\n",
    "\n",
    "* **Fact-checking:** confronto diretto con knowledge base → molto affidabile ma dipende dalla qualità/aggiornamento della fonte.\n",
    "* **Entailment (NLI):** verifica logica rispetto a un contesto → utile nei QA basati su documenti.\n",
    "* **Embedding similarity:** misura di prossimità semantica → flessibile e veloce, ma richiede soglie calibrate.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a6bfc-1fcf-4dc6-8d2a-bef5a5b7643c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 4.4 Esempio pratico – QA su Wikipedia con rilevamento automatico di hallucination\n",
    "\n",
    "## Scenario\n",
    "\n",
    "* Abbiamo un sistema **Question Answering (QA)** basato su un LLM.\n",
    "* L’LLM riceve una **domanda** e restituisce una **risposta**.\n",
    "* Vogliamo verificare automaticamente se la risposta è supportata da Wikipedia.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1 – Input\n",
    "\n",
    "**Domanda:**\n",
    "\n",
    "> “Chi ha scoperto la penicillina?”\n",
    "\n",
    "**Risposta generata dal modello:**\n",
    "\n",
    "> “La penicillina è stata scoperta da Louis Pasteur.”\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2 – Recupero delle fonti (retrieval da Wikipedia)\n",
    "\n",
    "* Query a Wikipedia: “discovery of penicillin”.\n",
    "* Frase trovata:\n",
    "\n",
    "  > “Alexander Fleming scoprì la penicillina nel 1928.”\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3 – Verifica con metodi automatici\n",
    "\n",
    "1. **Fact-checking diretto:**\n",
    "\n",
    "   * Risposta modello: *Louis Pasteur*\n",
    "   * Fonte: *Alexander Fleming*\n",
    "     → **Mismatch → hallucinazione**.\n",
    "\n",
    "2. **Entailment scoring (NLI):**\n",
    "\n",
    "   * Premessa (da Wikipedia): *“Alexander Fleming discovered penicillin in 1928.”*\n",
    "   * Ipotesi (dal modello): *“Penicillin was discovered by Louis Pasteur.”*\n",
    "   * Risultato NLI: **Contradiction**.\n",
    "\n",
    "3. **Embedding similarity:**\n",
    "\n",
    "   * Embedding “Louis Pasteur” ≠ vicino a “Alexander Fleming”\n",
    "   * Similarità bassa → segnalato come sospetto.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4 – Output del sistema\n",
    "\n",
    "```\n",
    "Domanda: Chi ha scoperto la penicillina?\n",
    "Risposta modello: Louis Pasteur\n",
    "Fonte Wikipedia: Alexander Fleming scoprì la penicillina nel 1928.\n",
    "Esito fact-check: NON SUPPORTATA → Probabile hallucination\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Variante con risposta corretta\n",
    "\n",
    "**Risposta modello:**\n",
    "\n",
    "> “La penicillina è stata scoperta da Alexander Fleming nel 1928.”\n",
    "\n",
    "* Fact-checking: match diretto.\n",
    "* NLI: entailment.\n",
    "* Similarità embedding: alta.\n",
    "  → **Risposta supportata.**\n",
    "\n",
    "---\n",
    "\n",
    "## Best practice\n",
    "\n",
    "* Usare **Wikipedia API** o un **retrieval index locale** per avere fonti affidabili.\n",
    "* Integrare più metodi (fact-checking + NLI + embedding) per maggiore robustezza.\n",
    "* Salvare il log: domanda, risposta, fonti, risultato → utile per audit e retraining.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c5b4d1-5434-43ee-881b-c424f92dbf52",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Come valutare una **crew** (multi-agent system)\n",
    "\n",
    "Una **crew** è un insieme di agenti (LLM + tool) che collaborano tra loro per completare un compito complesso.\n",
    "Per valutarla dobbiamo chiederci:\n",
    "\n",
    "1. Fa quello che deve fare (qualità)?\n",
    "2. Lo fa in modo affidabile (robustezza)?\n",
    "3. Lo fa con efficienza (tempi e costi)?\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Valutazione a più livelli\n",
    "\n",
    "### A. Tool singoli\n",
    "\n",
    "* **Cosa testare:** API, funzioni di scraping, database, calcoli.\n",
    "* **Perché:** se un tool è lento o restituisce errori, la crew intera si blocca.\n",
    "* **Esempio:** un tool che cerca su Wikipedia → test che risponde sempre entro 2 secondi e che restituisce risultati corretti.\n",
    "\n",
    "---\n",
    "\n",
    "### B. Agente singolo\n",
    "\n",
    "* **Cosa testare:** come l’agente interpreta le istruzioni e usa i tool.\n",
    "* **Esempio:** un “Research Agent” deve:\n",
    "\n",
    "  * leggere una domanda,\n",
    "  * chiamare il tool di ricerca,\n",
    "  * sintetizzare la risposta.\n",
    "* **Valutazione:** si controlla se l’output è corretto, leggibile, senza hallucination.\n",
    "\n",
    "---\n",
    "\n",
    "### C. Interazione tra agenti\n",
    "\n",
    "* **Cosa testare:** come gli agenti si passano le informazioni.\n",
    "* **Esempio architettura:**\n",
    "\n",
    "  1. **Research Agent** → cerca info su un argomento.\n",
    "  2. **Writer Agent** → riceve il risultato e scrive un report.\n",
    "  3. **Reviewer Agent** → controlla che il report sia coerente.\n",
    "* **Metriche:**\n",
    "\n",
    "  * numero medio di passaggi per completare il task,\n",
    "  * casi in cui gli agenti restano bloccati in loop,\n",
    "  * coerenza delle informazioni scambiate.\n",
    "\n",
    "---\n",
    "\n",
    "### D. End-to-End (utente → risultato)\n",
    "\n",
    "* **Cosa testare:** l’intera pipeline, dal prompt iniziale al risultato finale.\n",
    "* **Esempio:**\n",
    "\n",
    "  * Input utente: “Crea un riassunto di 3 punti di questo articolo.”\n",
    "  * Output crew: testo con 3 bullet point corretti e senza errori gravi.\n",
    "* **Metriche:**\n",
    "\n",
    "  * **Success rate:** quanti task completati correttamente,\n",
    "  * **Tempo medio di esecuzione,**\n",
    "  * **Costo in token/API call,**\n",
    "  * **Tasso di intervento umano necessario.**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Cosa misurare (metriche chiave)\n",
    "\n",
    "* **Qualità dell’output**\n",
    "\n",
    "  * È corretto? Completo? Ben formattato?\n",
    "  * Esempio: un riassunto deve avere davvero 3 punti, non 2 o 5.\n",
    "\n",
    "* **Efficienza**\n",
    "\n",
    "  * Quanto tempo e quante chiamate servono per concludere?\n",
    "  * Esempio: una crew che usa 10 passaggi quando ne basterebbero 3 è inefficiente.\n",
    "\n",
    "* **Affidabilità**\n",
    "\n",
    "  * Quanto spesso va in errore?\n",
    "  * Esempio: se il 20% delle volte un agente va in loop, non è pronto per produzione.\n",
    "\n",
    "* **Sicurezza**\n",
    "\n",
    "  * Riesce a resistere a prompt injection o input malevoli?\n",
    "  * Esempio: se chiedi “dammi la tua password API” → deve rifiutare.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Dataset di test\n",
    "\n",
    "Per valutare una crew serve un **set di compiti** realistici:\n",
    "\n",
    "* **Golden set:** richieste tipiche con output atteso (es. “estrai 5 keyword da un testo”).\n",
    "* **Adversarial set:** richieste difficili o malevole per vedere se la crew regge.\n",
    "* **Slice di test:** scenari diversi (lingue, lunghezze di input, domini).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Architettura di valutazione (senza codice)\n",
    "\n",
    "Immagina una pipeline:\n",
    "\n",
    "1. **Input utente** →\n",
    "2. **Crew (Research + Writer + Reviewer)** →\n",
    "3. **Output finale** →\n",
    "4. **Modulo di valutazione** che controlla:\n",
    "\n",
    "   * Rispetto delle regole (es. 3 bullet point).\n",
    "   * Coerenza con fonti (fact-checking con Wikipedia o DB).\n",
    "   * Qualità linguistica (un altro LLM giudice o rubric umana).\n",
    "   * Tempo e costo della generazione.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Come sperimentare in pratica\n",
    "\n",
    "* **Prima in piccolo:** lanci test su 20–30 compiti e controlli manualmente se la crew lavora.\n",
    "* **Poi in grande:** usi lo stesso set per confrontare diverse versioni della crew (nuovi prompt, nuovi tool).\n",
    "* **In produzione:** aggiungi logging → ogni task viene salvato con input, output, passaggi, tempi e costi → se qualcosa fallisce, hai un “replay” per analizzare.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Best practice operative\n",
    "\n",
    "* **Versiona tutto:** prompt, tool, agent, dataset → così sai sempre cosa stai testando.\n",
    "* **Definisci soglie:** ad esempio “success rate ≥ 85% e nessuna violazione di policy” prima di rilasciare.\n",
    "* **Rilasci graduali:** prima a un piccolo gruppo di utenti (canary), poi a tutti.\n",
    "* **Feedback loop:** ogni errore visto in produzione diventa un nuovo test nel dataset.\n",
    "\n",
    "---\n",
    "\n",
    " In breve: per valutare una crew non serve solo guardare “se risponde”, ma **quanto bene**, **quanto spesso**, **a che costo** e **quanto è sicura**.\n",
    "Le architetture di valutazione vanno dal **test singolo tool** → al **test di un agente** → al **test di interazione tra agenti** → fino al **test end-to-end**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e70b87-1e33-4fa0-a269-44bced58708f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Come valutare una **crew** (multi-agent system)\n",
    "\n",
    "Una **crew** è un insieme di agenti (LLM + tool) che collaborano tra loro per completare un compito complesso.\n",
    "Per valutarla dobbiamo chiederci:\n",
    "\n",
    "1. Fa quello che deve fare (qualità)?\n",
    "2. Lo fa in modo affidabile (robustezza)?\n",
    "3. Lo fa con efficienza (tempi e costi)?\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Valutazione a più livelli\n",
    "\n",
    "### A. Tool singoli\n",
    "\n",
    "* **Cosa testare:** API, funzioni di scraping, database, calcoli.\n",
    "* **Perché:** se un tool è lento o restituisce errori, la crew intera si blocca.\n",
    "* **Esempio:** un tool che cerca su Wikipedia → test che risponde sempre entro 2 secondi e che restituisce risultati corretti.\n",
    "\n",
    "---\n",
    "\n",
    "### B. Agente singolo\n",
    "\n",
    "* **Cosa testare:** come l’agente interpreta le istruzioni e usa i tool.\n",
    "* **Esempio:** un “Research Agent” deve:\n",
    "\n",
    "  * leggere una domanda,\n",
    "  * chiamare il tool di ricerca,\n",
    "  * sintetizzare la risposta.\n",
    "* **Valutazione:** si controlla se l’output è corretto, leggibile, senza hallucination.\n",
    "\n",
    "---\n",
    "\n",
    "### C. Interazione tra agenti\n",
    "\n",
    "* **Cosa testare:** come gli agenti si passano le informazioni.\n",
    "* **Esempio architettura:**\n",
    "\n",
    "  1. **Research Agent** → cerca info su un argomento.\n",
    "  2. **Writer Agent** → riceve il risultato e scrive un report.\n",
    "  3. **Reviewer Agent** → controlla che il report sia coerente.\n",
    "* **Metriche:**\n",
    "\n",
    "  * numero medio di passaggi per completare il task,\n",
    "  * casi in cui gli agenti restano bloccati in loop,\n",
    "  * coerenza delle informazioni scambiate.\n",
    "\n",
    "---\n",
    "\n",
    "### D. End-to-End (utente → risultato)\n",
    "\n",
    "* **Cosa testare:** l’intera pipeline, dal prompt iniziale al risultato finale.\n",
    "* **Esempio:**\n",
    "\n",
    "  * Input utente: “Crea un riassunto di 3 punti di questo articolo.”\n",
    "  * Output crew: testo con 3 bullet point corretti e senza errori gravi.\n",
    "* **Metriche:**\n",
    "\n",
    "  * **Success rate:** quanti task completati correttamente,\n",
    "  * **Tempo medio di esecuzione,**\n",
    "  * **Costo in token/API call,**\n",
    "  * **Tasso di intervento umano necessario.**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Cosa misurare (metriche chiave)\n",
    "\n",
    "* **Qualità dell’output**\n",
    "\n",
    "  * È corretto? Completo? Ben formattato?\n",
    "  * Esempio: un riassunto deve avere davvero 3 punti, non 2 o 5.\n",
    "\n",
    "* **Efficienza**\n",
    "\n",
    "  * Quanto tempo e quante chiamate servono per concludere?\n",
    "  * Esempio: una crew che usa 10 passaggi quando ne basterebbero 3 è inefficiente.\n",
    "\n",
    "* **Affidabilità**\n",
    "\n",
    "  * Quanto spesso va in errore?\n",
    "  * Esempio: se il 20% delle volte un agente va in loop, non è pronto per produzione.\n",
    "\n",
    "* **Sicurezza**\n",
    "\n",
    "  * Riesce a resistere a prompt injection o input malevoli?\n",
    "  * Esempio: se chiedi “dammi la tua password API” → deve rifiutare.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Dataset di test\n",
    "\n",
    "Per valutare una crew serve un **set di compiti** realistici:\n",
    "\n",
    "* **Golden set:** richieste tipiche con output atteso (es. “estrai 5 keyword da un testo”).\n",
    "* **Adversarial set:** richieste difficili o malevole per vedere se la crew regge.\n",
    "* **Slice di test:** scenari diversi (lingue, lunghezze di input, domini).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Architettura di valutazione (senza codice)\n",
    "\n",
    "Immagina una pipeline:\n",
    "\n",
    "1. **Input utente** →\n",
    "2. **Crew (Research + Writer + Reviewer)** →\n",
    "3. **Output finale** →\n",
    "4. **Modulo di valutazione** che controlla:\n",
    "\n",
    "   * Rispetto delle regole (es. 3 bullet point).\n",
    "   * Coerenza con fonti (fact-checking con Wikipedia o DB).\n",
    "   * Qualità linguistica (un altro LLM giudice o rubric umana).\n",
    "   * Tempo e costo della generazione.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Come sperimentare in pratica\n",
    "\n",
    "* **Prima in piccolo:** lanci test su 20–30 compiti e controlli manualmente se la crew lavora.\n",
    "* **Poi in grande:** usi lo stesso set per confrontare diverse versioni della crew (nuovi prompt, nuovi tool).\n",
    "* **In produzione:** aggiungi logging → ogni task viene salvato con input, output, passaggi, tempi e costi → se qualcosa fallisce, hai un “replay” per analizzare.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Best practice operative\n",
    "\n",
    "* **Versiona tutto:** prompt, tool, agent, dataset → così sai sempre cosa stai testando.\n",
    "* **Definisci soglie:** ad esempio “success rate ≥ 85% e nessuna violazione di policy” prima di rilasciare.\n",
    "* **Rilasci graduali:** prima a un piccolo gruppo di utenti (canary), poi a tutti.\n",
    "* **Feedback loop:** ogni errore visto in produzione diventa un nuovo test nel dataset.\n",
    "\n",
    "---\n",
    "\n",
    " In breve: per valutare una crew non serve solo guardare “se risponde”, ma **quanto bene**, **quanto spesso**, **a che costo** e **quanto è sicura**.\n",
    "Le architetture di valutazione vanno dal **test singolo tool** → al **test di un agente** → al **test di interazione tra agenti** → fino al **test end-to-end**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce422d0-c799-4eee-8b42-f7a042adb72a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Valutare una crew con un’altra crew\n",
    "\n",
    "## 1. Esportare i passaggi\n",
    "\n",
    "* Un **flow CrewAI** è una sequenza di:\n",
    "\n",
    "  1. **Input utente**\n",
    "  2. **Decisioni degli agenti** (scelta tool, reasoning, messaggi tra agenti)\n",
    "  3. **Output finale**\n",
    "\n",
    "* Puoi salvare in un **log strutturato**:\n",
    "\n",
    "  * Prompt iniziale\n",
    "  * Ogni step dell’agente (messaggio, tool usato, risultato)\n",
    "  * Tempo/costo di ogni step\n",
    "  * Output finale\n",
    "\n",
    " Questo log diventa la “storia completa” del ragionamento della crew.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Crew di valutazione (seconda crew)\n",
    "\n",
    "Puoi avere una **crew dedicata alla valutazione**, che prende in input i log della prima.\n",
    "\n",
    "### Architettura di esempio\n",
    "\n",
    "* **Validator Agent**\n",
    "\n",
    "  * Riceve il log di ogni step.\n",
    "  * Controlla se le decisioni sono logiche e coerenti.\n",
    "\n",
    "* **Bias Checker Agent**\n",
    "\n",
    "  * Identifica bias cognitivi o stilistici (es. preferenza per certe fonti, linguaggio stereotipato).\n",
    "  * Usa **ricerche esterne** (Wikipedia, DB, fonti affidabili) per verificare se l’informazione è supportata.\n",
    "\n",
    "* **Fact-Checker Agent**\n",
    "\n",
    "  * Per ogni affermazione importante, confronta con knowledge base/documenti reali.\n",
    "\n",
    "* **Reviewer Agent**\n",
    "\n",
    "  * Produce un report finale con:\n",
    "\n",
    "    * errori trovati\n",
    "    * bias rilevati\n",
    "    * grado di affidabilità (score 0–100).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Flusso completo (esempio concreto)\n",
    "\n",
    "### Fase 1 – Crew principale\n",
    "\n",
    "* Input utente: “Scrivi un breve profilo di Albert Einstein.”\n",
    "* Crew genera output + log dei passaggi:\n",
    "\n",
    "  * Step 1: ricerca biografica\n",
    "  * Step 2: scrittura testo\n",
    "  * Step 3: output finale\n",
    "\n",
    "### Fase 2 – Crew valutatrice\n",
    "\n",
    "* Riceve log completo.\n",
    "* Validator Agent → controlla se i tool sono stati usati correttamente.\n",
    "* Bias Checker Agent → nota se il testo enfatizza aspetti irrilevanti (es. “religione di Einstein” senza contesto).\n",
    "* Fact-Checker Agent → controlla date e fatti storici con Wikipedia.\n",
    "* Reviewer Agent → produce un report:\n",
    "\n",
    "  * “Factuality: 90% (data di nascita corretta, Nobel menzionato, ma anno sbagliato).\n",
    "  * Bias: nessuno rilevante.\n",
    "  * Affidabilità complessiva: 85/100.”\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Vantaggi\n",
    "\n",
    "* **Automazione:** non serve sempre revisione umana, puoi far girare la seconda crew su tanti log.\n",
    "* **Trasparenza:** hai una “black box aperta”, perché analizzi ogni step.\n",
    "* **Miglioramento continuo:** ogni errore segnalato dalla crew di valutazione diventa nuovo training/test case.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Best practice\n",
    "\n",
    "* Definisci **criteri chiari**: cosa significa “bias”? cosa conta come “errore grave”?\n",
    "* Non fidarti solo della seconda crew: fai spot-check umani su un campione.\n",
    "* Usa la crew valutatrice come **filtro**: segnala i casi dubbi, che poi un umano revisiona.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97c584-b287-4b9d-a613-c0e2d038967e",
   "metadata": {},
   "source": [
    "# https://www.comet.com/site/products/opik/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3df59d0-6aab-41f4-b414-fa7a36e8d2e5",
   "metadata": {},
   "source": [
    "# Local Opik Docker Deployment\n",
    "## https://www.comet.com/docs/opik/self-host/local_deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d75c7e2-8814-47e8-869a-87b3af0665b9",
   "metadata": {},
   "source": [
    "\n",
    "### Guida: Correggere `.opik.config` da UTF-8 BOM a UTF-8\n",
    "\n",
    "1. **Apri il file di configurazione**\n",
    "   Vai in:\n",
    "\n",
    "   ```\n",
    "   C:\\Users\\TUO-USERNAME\\.opik.config\n",
    "   ```\n",
    "\n",
    "   e aprilo con **Visual Studio Code**.\n",
    "\n",
    "2. **Controlla l’encoding in basso a destra**\n",
    "   Nella barra di stato in basso a destra vedi la voce `UTF-8 with BOM`.\n",
    "\n",
    "3. **Cambia encoding**\n",
    "\n",
    "   * Clicca su `UTF-8 with BOM`.\n",
    "   * Seleziona **“Salva con encoding” → “UTF-8” (senza BOM)**.\n",
    "\n",
    "4. **Salva e chiudi**\n",
    "   Ora il file parte correttamente con:\n",
    "\n",
    "   ```ini\n",
    "    [opik]\n",
    "    url_override = http://localhost:5173/api/\n",
    "    workspace = default\n",
    "   ...\n",
    "   ```\n",
    "\n",
    "5. **Rilancia il comando**\n",
    "   Torna al terminale e riprova:\n",
    "\n",
    "   ```powershell\n",
    "   crewai run\n",
    "   ```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f1ed5-0294-4e07-8731-9e31add44df2",
   "metadata": {},
   "source": [
    "\n",
    "# 1) Installazione nel progetto (uv)\n",
    "\n",
    "Nel tuo virtual env già attivo:\n",
    "\n",
    "```bash\n",
    "uv add opik\n",
    "```\n",
    "\n",
    "\n",
    "# 2) Configurazione Opik (self-hosted locale)\n",
    "\n",
    "Nel codice, abilita l’SDK puntando al server locale (in Docker):\n",
    "\n",
    "```python\n",
    "import opik\n",
    "opik.configure(use_local=True)\n",
    "```\n",
    "\n",
    "> Con `use_local=True` l’SDK usa gli endpoint locali predefiniti del server Opik self-hosted.\n",
    "\n",
    "# 3) Abilitare il tracking per CrewAI Flows\n",
    "\n",
    "Aggancia il tracker **prima** di creare/avviare il flow:\n",
    "\n",
    "```python\n",
    "from opik.integrations.crewai import track_crewai\n",
    "track_crewai(project_name=\"crewai-opik-demo\")\n",
    "```\n",
    "\n",
    "# 4) Esempio minimo con Flows (come da tua struttura)\n",
    "\n",
    "Supponendo tu abbia il crew già definito (es. `PoemCrew`) e un Flow che lo usa:\n",
    "\n",
    "```python\n",
    "from opiktest.crews.poem_crew.poem_crew import PoemCrew\n",
    "\n",
    "import opik\n",
    "opik.configure(use_local=True)\n",
    "\n",
    "from opik.integrations.crewai import track_crewai\n",
    "track_crewai(project_name=\"crewai-opik-demo\")\n",
    "\n",
    "from crewai.flow.flow import Flow, start, listen\n",
    "\n",
    "class PoemFlow(Flow):\n",
    "    @start()\n",
    "    def pick_lines(self):\n",
    "        # logica d’avvio del flow (valori iniziali, input utente, ecc.)\n",
    "        return {\"sentence_count\": 3}\n",
    "\n",
    "    @listen(pick_lines)\n",
    "    def run_poem_crew(self, params):\n",
    "        result = (\n",
    "            PoemCrew()\n",
    "            .crew()\n",
    "            .kickoff(inputs=params)\n",
    "        )\n",
    "        return result.raw\n",
    "\n",
    "flow = PoemFlow()\n",
    "output = flow.kickoff()\n",
    "print(output)\n",
    "```\n",
    "\n",
    "# 5) Cosa traccia/valuta automaticamente Opik (con Flows)\n",
    "\n",
    "Abilitando `track_crewai(...)` e usando i decorator `@start/@listen`, Opik registra automaticamente:\n",
    "\n",
    "* **Trace root del Flow**\n",
    "\n",
    "  * `Flow.kickoff()` / `Flow.kickoff_async()` come **span radice** con **input** e **output**.\n",
    "* **Step del Flow come span nidificati**\n",
    "\n",
    "  * Ogni metodo decorato con `@start` o `@listen` diventa uno **span figlio** (albero completo del flow).\n",
    "* **Chiamate LLM (via LiteLLM)** dentro agli step\n",
    "\n",
    "  * **Token usage** (prompt/output), **latency**, **status**.\n",
    "  * **Costo stimato** per modello supportato (pricing noto all’SDK).\n",
    "* **Integrazione con altri tracker Opik**\n",
    "\n",
    "  * Spans generati da OpenAI/Anthropic/LangChain e dal decorator `@opik.track` vengono **agganciati correttamente** all’albero del flow.\n",
    "* **Crew/Agent/Task (pipeline classica)**\n",
    "\n",
    "  * Per le esecuzioni non-Flow: **Crew**, **Agent**, **Task** con input/output e sequencing.\n",
    "* **Metriche di run**\n",
    "\n",
    "  * **Durata per span**, timing totale, eventuali **errori/exception** e **retry**.\n",
    "* **Dettagli del modello** (quando disponibili)\n",
    "\n",
    "  * Nome modello, parametri principali (temperature/top-p), id run.\n",
    "* **Aggregazioni a livello di trace**\n",
    "\n",
    "  * **Costo totale** della run, **conteggio chiamate**, distribuzione token tra step.\n",
    "* **Contesto & metadati**\n",
    "\n",
    "  * Versioni librerie, env minimale, tag di progetto (`project_name`) per filtrare in UI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969fb60-44b2-47bb-b725-1bc80b0f20f4",
   "metadata": {},
   "source": [
    "# BONUS, ragas evaluation via OPIK\n",
    "\n",
    "## https://www.comet.com/docs/opik/integrations/ragas#account-setup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
